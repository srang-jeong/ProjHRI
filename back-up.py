import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import seaborn as sns
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
import pytz
import time
import io
import json
import os
from dotenv import load_dotenv
from supabase import create_client
from scipy.stats import chi2_contingency, ttest_ind, f_oneway, spearmanr, pearsonr
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import networkx as nx

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ë¡œë´‡ MBTI ì§„ë‹¨ ì‹œìŠ¤í…œ",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# MBTI ìƒ‰ìƒ ë§¤í•‘ - íŒŒë€ìƒ‰ ê³„ì—´
MBTI_COLORS = {
    'ENFJ': '#2196F3', 'ENTJ': '#1976D2', 'ENTP': '#42A5F5', 'ENFP': '#64B5F6',
    'ESFJ': '#81C784', 'ESFP': '#4FC3F7', 'ESTJ': '#29B6F6', 'ESTP': '#26C6DA',
    'INFJ': '#5C6BC0', 'INFP': '#7986CB', 'INTJ': '#3F51B5', 'INTP': '#9575CD',
    'ISFJ': '#26A69A', 'ISFP': '#66BB6A', 'ISTJ': '#42A5F5', 'ISTP': '#78909C'
}

# Supabase ì„¤ì •
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

# ë””ë²„ê¹…: í™˜ê²½ ë³€ìˆ˜ í™•ì¸
if not SUPABASE_URL:
    print("âš ï¸ SUPABASE_URLì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
if not SUPABASE_KEY:
    print("âš ï¸ SUPABASE_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

try:
    supabase = create_client(SUPABASE_URL, SUPABASE_KEY) if SUPABASE_URL and SUPABASE_KEY else None
    if supabase:
        print("âœ… Supabase í´ë¼ì´ì–¸íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ë° ìƒì„±
        try:
            # responses í…Œì´ë¸” í™•ì¸
            supabase.table("responses").select("id").limit(1).execute()
            print("âœ… responses í…Œì´ë¸”ì´ ì¡´ì¬í•©ë‹ˆë‹¤.")
            
            # location ì»¬ëŸ¼ í™•ì¸ ë° ìë™ ì¶”ê°€
            try:
                supabase.table("responses").select("location").limit(1).execute()
                print("âœ… location ì»¬ëŸ¼ì´ ì¡´ì¬í•©ë‹ˆë‹¤.")
            except Exception as location_error:
                if "location" in str(location_error).lower():
                    print("âš ï¸ location ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ìë™ìœ¼ë¡œ ì¶”ê°€ë¥¼ ì‹œë„í•©ë‹ˆë‹¤...")
                    try:
                        # RPCë¥¼ í†µí•´ ì»¬ëŸ¼ ì¶”ê°€ ì‹œë„
                        supabase.rpc('add_location_column').execute()
                        print("âœ… location ì»¬ëŸ¼ì´ ì„±ê³µì ìœ¼ë¡œ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    except Exception as add_error:
                        print(f"âŒ ìë™ ì»¬ëŸ¼ ì¶”ê°€ ì‹¤íŒ¨: {add_error}")
                        print("ìˆ˜ë™ìœ¼ë¡œ ë‹¤ìŒ SQLì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”:")
                        print("ALTER TABLE public.responses ADD COLUMN location TEXT DEFAULT 'ì¼ë°˜';")
                else:
                    print(f"âš ï¸ location ì»¬ëŸ¼ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {location_error}")
                    
        except Exception as e:
            print(f"âš ï¸ responses í…Œì´ë¸” í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
            
        try:
            # user_robots í…Œì´ë¸” í™•ì¸
            supabase.table("user_robots").select("id").limit(1).execute()
            print("âœ… user_robots í…Œì´ë¸”ì´ ì¡´ì¬í•©ë‹ˆë‹¤.")
        except Exception as e:
            print(f"âš ï¸ user_robots í…Œì´ë¸” í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
    else:
        print("âŒ Supabase í´ë¼ì´ì–¸íŠ¸ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
except Exception as e:
    print(f"âŒ Supabase í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
    supabase = None

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
def init_session_state():
    """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”"""
    defaults = {
        'user_id': None,
        'robot_id': "ë¡œë´‡A",
        'robot_list': ["ë¡œë´‡A"],
        'admin_logged_in': False,
        'selected_location': None,
        'page': 1,
        'saved_result': False,
        'current_diagnosis_id': None,  # í˜„ì¬ ì§„ë‹¨ ì„¸ì…˜ ID ì¶”ê°€
        'force_new_diagnosis': False,  # ìƒˆ ì§„ë‹¨ ê°•ì œ í”Œë˜ê·¸
        'user_profile': {"gender": "ë‚¨", "age_group": "20ëŒ€", "job": "í•™ìƒ"},
        'registered_users': {
            "admin": "admin123",
            "manager": "manager123"
        }
    }
    
    for key, default_value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = default_value

def reset_diagnosis_session():
    """ì§„ë‹¨ ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”"""
    st.session_state.saved_result = False
    st.session_state.current_diagnosis_id = None
    st.session_state.responses = {}
    st.session_state.force_new_diagnosis = False

# CSS ìŠ¤íƒ€ì¼ ì„¤ì •
def setup_styles():
    """CSS ìŠ¤íƒ€ì¼ ì„¤ì • - íŒŒë€ìƒ‰ ê³„ì—´ ë””ìì¸"""
    st.markdown("""
    <style>
    /* ë©”ì¸ ë°°ê²½ - íŒŒë€ìƒ‰ ê·¸ë¼ë°ì´ì…˜ */
    .main {
        background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
        padding: 2rem;
    }
    
    /* ë‹¤í¬ëª¨ë“œ ì§€ì› */
    @media (prefers-color-scheme: dark) {
        .main {
            background: linear-gradient(135deg, #0f1419 0%, #1a2332 100%);
        }
    }
    
    /* í—¤ë” ìŠ¤íƒ€ì¼ - íŒŒë€ìƒ‰ ê·¸ë¼ë°ì´ì…˜ */
    .main .block-container h1 {
        background: linear-gradient(45deg, #2196F3, #00BCD4);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        font-size: 3rem;
        font-weight: bold;
        text-align: center;
        margin-bottom: 2rem;
        animation: fadeIn 1s ease-in;
    }
    
    /* ì¹´ë“œ ìŠ¤íƒ€ì¼ */
    .stCard {
        background: rgba(255, 255, 255, 0.95);
        border-radius: 15px;
        box-shadow: 0 8px 32px rgba(33, 150, 243, 0.1);
        backdrop-filter: blur(10px);
        border: 1px solid rgba(33, 150, 243, 0.2);
        transition: transform 0.3s ease;
    }
    
    /* ë²„íŠ¼ ìŠ¤íƒ€ì¼ - íŒŒë€ìƒ‰ ê·¸ë¼ë°ì´ì…˜ */
    .stButton > button {
        background: linear-gradient(135deg, #2196F3 0%, #1976D2 100%);
        color: white !important;
        border: none;
        border-radius: 12px;
        padding: 12px 24px;
        font-weight: 600;
        transition: all 0.3s ease;
        box-shadow: 0 4px 15px rgba(33, 150, 243, 0.3);
    }
    
    /* ë²„íŠ¼ í˜¸ë²„ íš¨ê³¼ */
    .stButton > button:hover {
        transform: translateY(-2px) scale(1.02);
        box-shadow: 0 8px 25px rgba(33, 150, 243, 0.4);
        background: linear-gradient(135deg, #1976D2 0%, #1565C0 100%);
    }
    
    /* ì…ë ¥ í•„ë“œ ìŠ¤íƒ€ì¼ - íŒŒë€ìƒ‰ í…Œë§ˆ */
    .stTextInput > div > div > input {
        background: rgba(255, 255, 255, 0.95) !important;
        border: 2px solid rgba(33, 150, 243, 0.3);
        border-radius: 12px;
        padding: 12px 16px;
        font-size: 14px;
        color: #1565C0 !important;
        transition: all 0.3s ease;
        box-shadow: 0 2px 8px rgba(33, 150, 243, 0.1);
    }
    
    .stTextInput > div > div > input:focus {
        border-color: #2196F3;
        box-shadow: 0 4px 15px rgba(33, 150, 243, 0.3);
        transform: translateY(-1px);
    }
    
    /* ì„ íƒ ë°•ìŠ¤ ìŠ¤íƒ€ì¼ - íŒŒë€ìƒ‰ í…Œë§ˆ */
    .stSelectbox > div > div > div {
        background: rgba(255, 255, 255, 0.95) !important;
        border: 2px solid rgba(33, 150, 243, 0.3);
        border-radius: 12px;
        transition: all 0.3s ease;
        box-shadow: 0 2px 8px rgba(33, 150, 243, 0.1);
        color: #1565C0 !important;
    }
    
    .stSelectbox > div > div > div:hover {
        border-color: #2196F3;
        box-shadow: 0 4px 15px rgba(33, 150, 243, 0.3);
    }
    
    /* ì‚¬ì´ë“œë°” ìŠ¤íƒ€ì¼ - íŒŒë€ìƒ‰ í…Œë§ˆ */
    .css-1d391kg {
        background: linear-gradient(180deg, #1e3c72 0%, #2a5298 100%);
    }
    
    @media (prefers-color-scheme: dark) {
        .css-1d391kg {
            background: linear-gradient(180deg, #0f1419 0%, #1a2332 100%);
        }
        
        .stTextInput > div > div > input {
            background: rgba(30, 60, 114, 0.95) !important;
            border: 2px solid rgba(33, 150, 243, 0.4);
            color: white !important;
        }
        
        .stSelectbox > div > div > div {
            background: rgba(30, 60, 114, 0.95) !important;
            border: 2px solid rgba(33, 150, 243, 0.4);
            color: white !important;
        }
    }
    
    /* ë©”íŠ¸ë¦­ ì¹´ë“œ - íŒŒë€ìƒ‰ í…Œë§ˆ */
    .metric-card {
        background: linear-gradient(135deg, #2196F3 0%, #1976D2 100%);
        color: white;
        padding: 1rem;
        border-radius: 10px;
        text-align: center;
        margin: 0.5rem 0;
        box-shadow: 0 4px 15px rgba(33, 150, 243, 0.3);
    }
    
    /* ë¡œê·¸ì¸ ì¹´ë“œ ìŠ¤íƒ€ì¼ - íŒŒë€ìƒ‰ í…Œë§ˆ */
    .login-card {
        background: rgba(255, 255, 255, 0.95);
        border-radius: 20px;
        box-shadow: 0 15px 35px rgba(33, 150, 243, 0.2);
        backdrop-filter: blur(10px);
        border: 1px solid rgba(33, 150, 243, 0.3);
        padding: 2rem;
        margin: 2rem auto;
        max-width: 400px;
    }
    
    @media (prefers-color-scheme: dark) {
        .login-card {
            background: rgba(30, 60, 114, 0.95);
            border: 1px solid rgba(33, 150, 243, 0.3);
            color: white;
        }
    }
    
    /* íƒ­ ìŠ¤íƒ€ì¼ - íŒŒë€ìƒ‰ í…Œë§ˆ */
    .stTabs [data-baseweb="tab-list"] {
        gap: 8px;
    }
    
    .stTabs [data-baseweb="tab"] {
        background-color: rgba(33, 150, 243, 0.1);
        border-radius: 8px;
        color: #1976D2;
        font-weight: 600;
    }
    
    .stTabs [aria-selected="true"] {
        background-color: #2196F3 !important;
        color: white !important;
    }
    
    /* í”„ë¡œê·¸ë ˆìŠ¤ ë°” - íŒŒë€ìƒ‰ í…Œë§ˆ */
    .stProgress > div > div > div > div {
        background-color: #2196F3;
    }
    
    /* ì„±ê³µ/ì •ë³´ ë©”ì‹œì§€ - íŒŒë€ìƒ‰ í…Œë§ˆ */
    .stSuccess {
        background-color: rgba(33, 150, 243, 0.1);
        border-left: 4px solid #2196F3;
        border-radius: 8px;
        animation: slideIn 0.3s ease-out;
    }
    
    .stInfo {
        background-color: rgba(33, 150, 243, 0.1);
        border-left: 4px solid #2196F3;
        border-radius: 8px;
        animation: slideIn 0.3s ease-out;
    }
    
    /* ì‹œê°ì  ì§‘ì¤‘ë„ë¥¼ ë†’ì´ëŠ” ì¶”ê°€ ìŠ¤íƒ€ì¼ */
    .stExpander {
        border: 1px solid rgba(33, 150, 243, 0.2);
        border-radius: 12px;
        box-shadow: 0 2px 8px rgba(33, 150, 243, 0.1);
        transition: all 0.3s ease;
    }
    
    .stExpander:hover {
        box-shadow: 0 4px 16px rgba(33, 150, 243, 0.2);
        transform: translateY(-1px);
    }
    
    /* ë°ì´í„°í”„ë ˆì„ ìŠ¤íƒ€ì¼ ê°œì„  */
    .stDataFrame {
        border-radius: 12px;
        overflow: hidden;
        box-shadow: 0 4px 12px rgba(33, 150, 243, 0.1);
    }
    
    /* ë©”íŠ¸ë¦­ ì¹´ë“œ í˜¸ë²„ íš¨ê³¼ */
    .metric-card:hover {
        transform: translateY(-2px) scale(1.02);
        box-shadow: 0 8px 25px rgba(33, 150, 243, 0.4);
    }
    
    /* ì°¨íŠ¸ ì»¨í…Œì´ë„ˆ ìŠ¤íƒ€ì¼ */
    .js-plotly-plot {
        border-radius: 12px;
        box-shadow: 0 4px 12px rgba(33, 150, 243, 0.1);
        transition: all 0.3s ease;
    }
    
    .js-plotly-plot:hover {
        box-shadow: 0 8px 20px rgba(33, 150, 243, 0.2);
    }
    
    /* ì• ë‹ˆë©”ì´ì…˜ ê°œì„  */
    @keyframes slideIn {
        from { 
            opacity: 0; 
            transform: translateX(-20px); 
        }
        to { 
            opacity: 1; 
            transform: translateX(0); 
        }
    }
    
    @keyframes pulse {
        0% { transform: scale(1); }
        50% { transform: scale(1.05); }
        100% { transform: scale(1); }
    }
    
    /* ì¤‘ìš”í•œ ë²„íŠ¼ì— í„ìŠ¤ íš¨ê³¼ */
    .stButton > button[data-testid="baseButton-primary"] {
        animation: pulse 2s infinite;
    }
    
    /* ìŠ¤í¬ë¡¤ë°” ìŠ¤íƒ€ì¼ ê°œì„  */
    ::-webkit-scrollbar {
        width: 8px;
        height: 8px;
    }
    
    ::-webkit-scrollbar-track {
        background: rgba(33, 150, 243, 0.1);
        border-radius: 4px;
    }
    
    ::-webkit-scrollbar-thumb {
        background: linear-gradient(135deg, #2196F3, #1976D2);
        border-radius: 4px;
    }
    
    ::-webkit-scrollbar-thumb:hover {
        background: linear-gradient(135deg, #1976D2, #1565C0);
    }
    
    @keyframes fadeIn {
        from { opacity: 0; transform: translateY(20px); }
        to { opacity: 1; transform: translateY(0); }
    }
    </style>
    """, unsafe_allow_html=True)

# ì‚¬ìš©ì ID ê²€ì¦ í•¨ìˆ˜ë“¤
def validate_user_id(user_id):
    """ì‚¬ìš©ì ID ìœ íš¨ì„± ê²€ì¦"""
    if not user_id or not user_id.strip():
        return False, "ì‚¬ìš©ì IDë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."
    
    if len(user_id.strip()) < 2:
        return False, "ì‚¬ìš©ì IDëŠ” 2ì ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤."
    
    if len(user_id.strip()) > 20:
        return False, "ì‚¬ìš©ì IDëŠ” 20ì ì´í•˜ì—¬ì•¼ í•©ë‹ˆë‹¤."
    
    # íŠ¹ìˆ˜ë¬¸ì ì œí•œ (ë³´ì•ˆì„± í–¥ìƒ)
    import re
    if not re.match(r'^[a-zA-Z0-9ê°€-í£\s_-]+$', user_id.strip()):
        return False, "ì‚¬ìš©ì IDëŠ” ì˜ë¬¸, ìˆ«ì, í•œê¸€, ê³µë°±, ì–¸ë”ìŠ¤ì½”ì–´(_), í•˜ì´í”ˆ(-)ë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤."
    
    return True, "ìœ íš¨í•œ ì‚¬ìš©ì IDì…ë‹ˆë‹¤."

def validate_robot_id(robot_id):
    """ë¡œë´‡ ID ìœ íš¨ì„± ê²€ì¦"""
    if not robot_id or not robot_id.strip():
        return False, "ë¡œë´‡ IDë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."
    
    if len(robot_id.strip()) < 2:
        return False, "ë¡œë´‡ IDëŠ” 2ì ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤."
    
    if len(robot_id.strip()) > 20:
        return False, "ë¡œë´‡ IDëŠ” 20ì ì´í•˜ì—¬ì•¼ í•©ë‹ˆë‹¤."
    
    # íŠ¹ìˆ˜ë¬¸ì ì œí•œ
    import re
    if not re.match(r'^[a-zA-Z0-9ê°€-í£\s_-]+$', robot_id.strip()):
        return False, "ë¡œë´‡ IDëŠ” ì˜ë¬¸, ìˆ«ì, í•œê¸€, ê³µë°±, ì–¸ë”ìŠ¤ì½”ì–´(_), í•˜ì´í”ˆ(-)ë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤."
    
    return True, "ìœ íš¨í•œ ë¡œë´‡ IDì…ë‹ˆë‹¤."

def sanitize_input(text):
    """ì…ë ¥ê°’ ì •ì œ (XSS ë°©ì§€)"""
    if not text:
        return ""
    
    # HTML íƒœê·¸ ì œê±°
    import re
    text = re.sub(r'<[^>]+>', '', text)
    
    # ìœ„í—˜í•œ ë¬¸ì ì œê±°
    text = text.replace('"', '').replace("'", '').replace(';', '').replace('--', '')
    
    return text.strip()

def check_admin_login(username, password):
    """ê´€ë¦¬ì ë¡œê·¸ì¸ í™•ì¸"""
    admin_credentials = {
        "admin": "admin123",
        "manager": "manager123"
    }
    return username in admin_credentials and admin_credentials[username] == password

# ë°ì´í„° ê´€ë¦¬ í•¨ìˆ˜ë“¤
def save_response(user_id, responses, mbti, scores, profile, robot_id):
    """ì‘ë‹µ ë°ì´í„° ì €ì¥ (ë³´ì•ˆ ê°•í™”)"""
    try:
        if not supabase:
            st.error("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ì—†ìŠµë‹ˆë‹¤.")
            return False
            
        # ì…ë ¥ê°’ ì •ì œ
        user_id = sanitize_input(user_id)
        robot_id = sanitize_input(robot_id)
        
        # ìœ íš¨ì„± ê²€ì¦
        user_valid, user_msg = validate_user_id(user_id)
        robot_valid, robot_msg = validate_robot_id(robot_id)
        
        if not user_valid:
            st.error(f"ì‚¬ìš©ì ID ì˜¤ë¥˜: {user_msg}")
            return False
        
        if not robot_valid:
            st.error(f"ë¡œë´‡ ID ì˜¤ë¥˜: {robot_msg}")
            return False
        
        # ê¸°ë³¸ ë ˆì½”ë“œ êµ¬ì„±
        record = {
            "user_id": user_id,
            "gender": profile["gender"],
            "age_group": profile["age_group"],
            "job": profile["job"],
            "robot_id": robot_id,
            "responses": responses,
            "mbti": mbti,
            "scores": scores,
            "timestamp": datetime.now(pytz.timezone("Asia/Seoul")).isoformat()
        }
        
        # location ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì¶”ê°€
        try:
            # ë¨¼ì € location ì»¬ëŸ¼ ì—†ì´ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
            test_record = record.copy()
            test_record["location"] = st.session_state.get('selected_location', 'ì¼ë°˜')
            supabase.table("responses").insert(test_record).execute()
            return True
        except Exception as location_error:
            # location ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš°, location ì—†ì´ ì €ì¥ ì‹œë„
            if "location" in str(location_error).lower():
                # location ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš° ì¡°ìš©íˆ ì²˜ë¦¬ (ê²½ê³  ë©”ì‹œì§€ ì œê±°)
                supabase.table("responses").insert(record).execute()
                return True
            else:
                raise location_error
    except Exception as e:
        st.error(f"ì‘ë‹µ ì €ì¥ ì‹¤íŒ¨: {e}")
        return False

def load_responses():
    """ëª¨ë“  ì‘ë‹µ ë°ì´í„° ë¡œë“œ"""
    try:
        if not supabase:
            st.error("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()
            
        res = supabase.table("responses").select("*").execute()
        return pd.DataFrame(res.data) if res.data else pd.DataFrame()
    except Exception as e:
        st.error(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return pd.DataFrame()

def reset_all_data():
    """ì „ì²´ ë°ì´í„° ë¦¬ì…‹"""
    try:
        if not supabase:
            return False, "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ì—†ìŠµë‹ˆë‹¤."
        
        # ëª¨ë“  í…Œì´ë¸”ì˜ ë°ì´í„° ì‚­ì œ
        deleted_responses = 0
        deleted_robots = 0
        
        # responses í…Œì´ë¸” ë°ì´í„° ì‚­ì œ
        try:
            responses_result = supabase.table("responses").delete().neq("id", 0).execute()
            if hasattr(responses_result, 'data') and responses_result.data:
                deleted_responses = len(responses_result.data)
        except Exception as e:
            st.warning(f"responses í…Œì´ë¸” ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}")
        
        # user_robots í…Œì´ë¸” ë°ì´í„° ì‚­ì œ
        try:
            robots_result = supabase.table("user_robots").delete().neq("id", 0).execute()
            if hasattr(robots_result, 'data') and robots_result.data:
                deleted_robots = len(robots_result.data)
        except Exception as e:
            st.warning(f"user_robots í…Œì´ë¸” ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return True, f"ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤. (ì§„ë‹¨ ë°ì´í„°: {deleted_responses}ê±´, ë¡œë´‡ ë°ì´í„°: {deleted_robots}ê±´)"
        
    except Exception as e:
        return False, f"ë°ì´í„° ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

def get_user_data_summary():
    """ì‚¬ìš©ì ë°ì´í„° ìš”ì•½"""
    try:
        if not supabase:
            return {"error": "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ì—†ìŠµë‹ˆë‹¤."}
        
        # responses í…Œì´ë¸”ì—ì„œ ë°ì´í„° ì¡°íšŒ
        users = supabase.table("responses").select("user_id").execute()
        robots = supabase.table("user_robots").select("*").execute()
        
        return {
            "total_users": len(set([u['user_id'] for u in users.data])) if users.data else 0,
            "total_responses": len(users.data) if users.data else 0,
            "total_robots": len(robots.data) if robots.data else 0
        }
    except Exception as e:
        return {"error": str(e)}

def delete_user_data(user_id):
    """íŠ¹ì • ì‚¬ìš©ì ë°ì´í„° ì‚­ì œ"""
    try:
        if not supabase:
            return False, "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ì—†ìŠµë‹ˆë‹¤."
        
        # ì‚¬ìš©ìì˜ ëª¨ë“  ë°ì´í„° ì‚­ì œ
        supabase.table("responses").delete().eq("user_id", user_id).execute()
        supabase.table("user_robots").delete().eq("user_id", user_id).execute()
        
        return True, f"ì‚¬ìš©ì {user_id}ì˜ ë°ì´í„°ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."
    except Exception as e:
        return False, f"ì‚¬ìš©ì ë°ì´í„° ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}"

def load_user_robots(user_id):
    """ì‚¬ìš©ìì˜ ë¡œë´‡ ëª©ë¡ ë¡œë“œ"""
    try:
        if not supabase:
            return []
            
        res = supabase.table("user_robots").select("*").eq("user_id", user_id).execute()
        return [robot['robot_name'] for robot in res.data] if res.data else []
    except Exception as e:
        return []

def save_robot(user_id, robot_name, robot_description=""):
    """ë¡œë´‡ ì •ë³´ ì €ì¥ (ë³´ì•ˆ ê°•í™”)"""
    try:
        # ì…ë ¥ê°’ ì •ì œ
        user_id = sanitize_input(user_id)
        robot_name = sanitize_input(robot_name)
        robot_description = sanitize_input(robot_description)
        
        # ìœ íš¨ì„± ê²€ì¦
        user_valid, user_msg = validate_user_id(user_id)
        robot_valid, robot_msg = validate_robot_id(robot_name)
        
        if not user_valid:
            st.error(f"ì‚¬ìš©ì ID ì˜¤ë¥˜: {user_msg}")
            return False
        
        if not robot_valid:
            st.error(f"ë¡œë´‡ ID ì˜¤ë¥˜: {robot_msg}")
            return False
        
        # ì¤‘ë³µ ë¡œë´‡ í™•ì¸
        try:
            if supabase:
                existing = supabase.table("user_robots").select("robot_name").eq("user_id", user_id).eq("robot_name", robot_name).execute()
                if existing.data:
                    st.warning(f"ì´ë¯¸ ë“±ë¡ëœ ë¡œë´‡ì…ë‹ˆë‹¤: {robot_name}")
                    return False
        except Exception as e:
            st.info(f"ì¤‘ë³µ í™•ì¸ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")
        
        if not supabase:
            st.error("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ì—†ìŠµë‹ˆë‹¤.")
            return False
            
        record = {
            "user_id": user_id,
            "robot_name": robot_name,
            "robot_description": robot_description,
            "created_at": datetime.now(pytz.timezone("Asia/Seoul")).isoformat()
        }
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
        result = supabase.table("user_robots").insert(record).execute()
        
        if result.data:
            return True
        else:
            st.error("ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: ì‘ë‹µ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
            
    except Exception as e:
        st.error(f"ë¡œë´‡ ë“±ë¡ ì‹¤íŒ¨: {str(e)}")
        st.info("ê°€ëŠ¥í•œ ì›ì¸: ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë¬¸ì œ, í…Œì´ë¸” êµ¬ì¡° ì˜¤ë¥˜, ê¶Œí•œ ë¬¸ì œ")
        return False

# MBTI ê³„ì‚° í•¨ìˆ˜ë“¤
def load_questions(location="ì¼ë°˜"):
    """ì¥ì†Œë³„ íŠ¹í™”ëœ ì§ˆë¬¸ ë°ì´í„° ë¡œë“œ"""
    base_questions = [
        {"id":"Q1","text":"ë¡œë´‡ì´ ë¨¼ì € ì¸ì‚¬í•  ë•Œ ë‹¹ì‹ ì˜ ë°˜ì‘ì€?",
         "choices":["ì¦‰ì‹œ ëŒ€í™”ì— ì°¸ì—¬í•œë‹¤","ì ì‹œ ìƒí™©ì„ ê´€ì°°í•œë‹¤"],"axes":("E","I")},
        {"id":"Q2","text":"ì—¬ëŸ¬ ì‚¬ëŒê³¼ ë¡œë´‡ì„ ì‚¬ìš©í•  ë•Œ ì„ í˜¸í•˜ëŠ” ë°©ì‹ì€?",
         "choices":["ëª¨ë‘ê°€ í•¨ê»˜ ì°¸ì—¬í•œë‹¤","ê°œì¸ì ìœ¼ë¡œ 1:1 ìƒí˜¸ì‘ìš©í•œë‹¤"],"axes":("E","I")},
        {"id":"Q3","text":"ë¡œë´‡ê³¼ ëŒ€í™”í•  ë•Œ ë‹¹ì‹ ì˜ ìŠ¤íƒ€ì¼ì€?",
         "choices":["ì ê·¹ì ìœ¼ë¡œ ì§ˆë¬¸í•˜ê³  ì˜ê²¬ì„ í‘œí˜„í•œë‹¤","ë¡œë´‡ì˜ ì„¤ëª…ì„ ë“£ê³  ìƒê°í•œ í›„ ë°˜ì‘í•œë‹¤"],"axes":("E","I")},
        {"id":"Q4","text":"ë¡œë´‡ì˜ ì•ˆë‚´ ë°©ì‹ì„ ì„ í˜¸í•˜ëŠ” ìŠ¤íƒ€ì¼ì€?",
         "choices":["ë‹¨ê³„ë³„ë¡œ êµ¬ì²´ì ì¸ ì„¸ë¶€ì‚¬í•­ì„ ì œê³µí•œë‹¤","ì „ì²´ì ì¸ ë§¥ë½ê³¼ ì˜ë¯¸ë¥¼ ë¨¼ì € ì„¤ëª…í•œë‹¤"],"axes":("S","N")},
        {"id":"Q5","text":"ìƒˆë¡œìš´ ë¡œë´‡ ê¸°ëŠ¥ì„ ë°°ìš¸ ë•Œ ì„ í˜¸í•˜ëŠ” ë°©ë²•ì€?",
         "choices":["ì‹¤ì œë¡œ ì§ì ‘ ì¡°ì‘í•´ë³´ë©° í•™ìŠµí•œë‹¤","ê°œë…ê³¼ ì›ë¦¬ë¥¼ ë¨¼ì € ì´í•´í•œ í›„ ì‹œë„í•œë‹¤"],"axes":("S","N")},
        {"id":"Q6","text":"ë¡œë´‡ì—ê²Œ ì‘ì—…ì„ ìš”ì²­í•  ë•Œ ì„ í˜¸í•˜ëŠ” ë°©ì‹ì€?",
         "choices":["êµ¬ì²´ì ì´ê³  ëª…í™•í•œ ì§€ì‹œì‚¬í•­ì„ ì¤€ë‹¤","ì¼ë°˜ì ì¸ ëª©í‘œì™€ ë°©í–¥ì„±ë§Œ ì œì‹œí•œë‹¤"],"axes":("S","N")},
        {"id":"Q7","text":"ë¡œë´‡ê³¼ ì˜ì‚¬ê²°ì •ì„ í•  ë•Œ ì¤‘ì‹œí•˜ëŠ” ê²ƒì€?",
         "choices":["ë…¼ë¦¬ì  ë¶„ì„ê³¼ ê°ê´€ì  ë°ì´í„°","ê°ì •ì  ê³µê°ê³¼ ì£¼ê´€ì  ê²½í—˜"],"axes":("T","F")},
        {"id":"Q8","text":"ë¡œë´‡ì´ ì‹¤ìˆ˜ë¥¼ í–ˆì„ ë•Œ ë‹¹ì‹ ì˜ ë°˜ì‘ì€?",
         "choices":["ë¬¸ì œë¥¼ ë¶„ì„í•˜ê³  í•´ê²°ì±…ì„ ì°¾ëŠ”ë‹¤","ë¡œë´‡ì˜ ê°ì •ì„ ê³ ë ¤í•˜ì—¬ ëŒ€í™”í•œë‹¤"],"axes":("T","F")},
        {"id":"Q9","text":"ë¡œë´‡ì—ê²Œ í”¼ë“œë°±ì„ ì¤„ ë•Œ ì„ í˜¸í•˜ëŠ” ìŠ¤íƒ€ì¼ì€?",
         "choices":["ì •í™•í•˜ê³  êµ¬ì²´ì ì¸ ê°œì„ ì ì„ ì œì‹œí•œë‹¤","ê¸ì •ì  ê²©ë ¤ì™€ í•¨ê»˜ ì¡°ì–¸í•œë‹¤"],"axes":("T","F")},
        {"id":"Q10","text":"ë¡œë´‡ê³¼ì˜ ì¼ì • ê´€ë¦¬ì—ì„œ ì„ í˜¸í•˜ëŠ” ë°©ì‹ì€?",
         "choices":["ë¯¸ë¦¬ ê³„íší•˜ê³  ì²´ê³„ì ìœ¼ë¡œ ì§„í–‰í•œë‹¤","ìƒí™©ì— ë”°ë¼ ìœ ì—°í•˜ê²Œ ì¡°ì •í•œë‹¤"],"axes":("J","P")},
        {"id":"Q11","text":"ë¡œë´‡ê³¼ ìƒˆë¡œìš´ í™œë™ì„ í•  ë•Œì˜ ì ‘ê·¼ë²•ì€?",
         "choices":["ì •í•´ì§„ ê·œì¹™ê³¼ ì ˆì°¨ë¥¼ ë”°ë¥¸ë‹¤","ì¦‰í¥ì ì´ê³  ì°½ì˜ì ìœ¼ë¡œ ì‹œë„í•œë‹¤"],"axes":("J","P")},
        {"id":"Q12","text":"ë¡œë´‡ê³¼ì˜ ìƒí˜¸ì‘ìš© ê²°ê³¼ë¥¼ ì •ë¦¬í•  ë•Œ ì„ í˜¸í•˜ëŠ” ë°©ì‹ì€?",
         "choices":["ëª…í™•í•œ ìš”ì•½ê³¼ ê²°ë¡ ì„ ë„ì¶œí•œë‹¤","ë‹¤ì–‘í•œ ê´€ì ê³¼ ê°€ëŠ¥ì„±ì„ ì œì‹œí•œë‹¤"],"axes":("J","P")}
    ]
    
    # ì¥ì†Œë³„ íŠ¹í™” ì§ˆë¬¸
    location_specific_questions = {
        "ë³‘ì›": [
            {"id":"H1","text":"ë³‘ì›ì—ì„œ ë¡œë´‡ì´ í™˜ì ì •ë³´ë¥¼ í™•ì¸í•  ë•Œ ë‹¹ì‹ ì˜ ë°˜ì‘ì€?",
             "choices":["ì¦‰ì‹œ í•„ìš”í•œ ì •ë³´ë¥¼ ì œê³µí•œë‹¤","ë¨¼ì € ë¡œë´‡ì˜ ì‹ ë¢°ì„±ì„ í™•ì¸í•œë‹¤"],"axes":("E","I")},
            {"id":"H2","text":"ë¡œë´‡ì´ ì˜ë£Œì§„ê³¼ í•¨ê»˜ ìˆì„ ë•Œ ì„ í˜¸í•˜ëŠ” ìƒí˜¸ì‘ìš©ì€?",
             "choices":["ë¡œë´‡ê³¼ ì˜ë£Œì§„ì´ í•¨ê»˜ ì„¤ëª…í•œë‹¤","ë¡œë´‡ì€ ë³´ì¡° ì—­í• ë§Œ í•œë‹¤"],"axes":("E","I")},
            {"id":"H3","text":"ë¡œë´‡ì´ ì¹˜ë£Œ ê³¼ì •ì„ ì•ˆë‚´í•  ë•Œ ì„ í˜¸í•˜ëŠ” ë°©ì‹ì€?",
             "choices":["êµ¬ì²´ì ì¸ ì¹˜ë£Œ ë‹¨ê³„ì™€ ì˜ˆìƒ ì‹œê°„ì„ ì•Œë ¤ì¤€ë‹¤","ì „ì²´ì ì¸ ì¹˜ë£Œ ëª©í‘œì™€ ë°©í–¥ì„±ì„ ì„¤ëª…í•œë‹¤"],"axes":("S","N")},
            {"id":"H4","text":"ë¡œë´‡ì´ í™˜ìì˜ ìƒíƒœë¥¼ ëª¨ë‹ˆí„°ë§í•  ë•Œ ì¤‘ì‹œí•˜ëŠ” ê²ƒì€?",
             "choices":["ì •í™•í•œ ìˆ˜ì¹˜ì™€ ê°ê´€ì  ë°ì´í„°","í™˜ìì˜ í¸ì•ˆí•¨ê³¼ ì£¼ê´€ì  ëŠë‚Œ"],"axes":("T","F")},
            {"id":"H5","text":"ë¡œë´‡ì´ ì‘ê¸‰ ìƒí™©ì„ ê°ì§€í–ˆì„ ë•Œ ë‹¹ì‹ ì˜ ë°˜ì‘ì€?",
             "choices":["ì¦‰ì‹œ ì˜ë£Œì§„ì—ê²Œ ì—°ë½í•˜ê³  ëŒ€ì‘í•œë‹¤","ìƒí™©ì„ íŒŒì•…í•œ í›„ ì‹ ì¤‘í•˜ê²Œ ëŒ€ì‘í•œë‹¤"],"axes":("J","P")}
        ],
        "ë„ì„œê´€": [
            {"id":"L1","text":"ë„ì„œê´€ì—ì„œ ë¡œë´‡ì´ ë„ì„œ ê²€ìƒ‰ì„ ë„ì™€ì¤„ ë•Œ ì„ í˜¸í•˜ëŠ” ë°©ì‹ì€?",
             "choices":["êµ¬ì²´ì ì¸ í‚¤ì›Œë“œì™€ ì¡°ê±´ì„ ì…ë ¥í•œë‹¤","ì¼ë°˜ì ì¸ ì£¼ì œë‚˜ ê´€ì‹¬ì‚¬ë¥¼ ë§í•œë‹¤"],"axes":("S","N")},
            {"id":"L2","text":"ë¡œë´‡ì´ ë…ì„œ ì¶”ì²œì„ í•  ë•Œ ì¤‘ì‹œí•˜ëŠ” ê²ƒì€?",
             "choices":["ì¸ê¸°ë„ì™€ í‰ì  ê°™ì€ ê°ê´€ì  ì§€í‘œ","ê°œì¸ì˜ ì·¨í–¥ê³¼ ê°ì •ì  ì—°ê²°"],"axes":("T","F")},
            {"id":"L3","text":"ë„ì„œê´€ì—ì„œ ë¡œë´‡ê³¼ í•¨ê»˜ ê³µë¶€í•  ë•Œ ì„ í˜¸í•˜ëŠ” í™˜ê²½ì€?",
             "choices":["ì¡°ìš©í•˜ê³  ì§‘ì¤‘í•  ìˆ˜ ìˆëŠ” ê°œì¸ ê³µê°„","ë‹¤ë¥¸ ì‚¬ëŒë“¤ê³¼ í•¨ê»˜í•˜ëŠ” í•™ìŠµ ê³µê°„"],"axes":("E","I")},
            {"id":"L4","text":"ë¡œë´‡ì´ ë„ì„œ ëŒ€ì¶œ/ë°˜ë‚©ì„ ë„ì™€ì¤„ ë•Œ ë‹¹ì‹ ì˜ ìŠ¤íƒ€ì¼ì€?",
             "choices":["ë¯¸ë¦¬ ê³„íší•˜ê³  í•œ ë²ˆì— ì²˜ë¦¬í•œë‹¤","í•„ìš”í•  ë•Œë§ˆë‹¤ ê°œë³„ì ìœ¼ë¡œ ì²˜ë¦¬í•œë‹¤"],"axes":("J","P")},
            {"id":"L5","text":"ë¡œë´‡ì´ ë„ì„œê´€ ì´ìš© ê·œì¹™ì„ ì•ˆë‚´í•  ë•Œ ì„ í˜¸í•˜ëŠ” ë°©ì‹ì€?",
             "choices":["ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ ê·œì¹™ì„ ì œì‹œí•œë‹¤","ì „ì²´ì ì¸ ì´ìš© ë¬¸í™”ì™€ ë¶„ìœ„ê¸°ë¥¼ ì„¤ëª…í•œë‹¤"],"axes":("S","N")}
        ],
        "ì‡¼í•‘ëª°": [
            {"id":"M1","text":"ì‡¼í•‘ëª°ì—ì„œ ë¡œë´‡ì´ ìƒí’ˆì„ ì¶”ì²œí•  ë•Œ ì„ í˜¸í•˜ëŠ” ë°©ì‹ì€?",
             "choices":["êµ¬ì²´ì ì¸ ìƒí’ˆ ì •ë³´ì™€ ê°€ê²©ì„ ì œê³µí•œë‹¤","ì „ì²´ì ì¸ ìŠ¤íƒ€ì¼ê³¼ íŠ¸ë Œë“œë¥¼ ì œì•ˆí•œë‹¤"],"axes":("S","N")},
            {"id":"M2","text":"ë¡œë´‡ì´ í• ì¸ ì •ë³´ë¥¼ ì•Œë ¤ì¤„ ë•Œ ì¤‘ì‹œí•˜ëŠ” ê²ƒì€?",
             "choices":["ì •í™•í•œ í• ì¸ìœ¨ê³¼ ì ˆì•½ ê¸ˆì•¡","íŠ¹ë³„í•œ ê¸°íšŒì™€ ì¦ê±°ìš´ ê²½í—˜"],"axes":("T","F")},
            {"id":"M3","text":"ì‡¼í•‘ëª°ì—ì„œ ë¡œë´‡ê³¼ í•¨ê»˜ ì‡¼í•‘í•  ë•Œ ì„ í˜¸í•˜ëŠ” ë°©ì‹ì€?",
             "choices":["ë¯¸ë¦¬ ëª©ë¡ì„ ë§Œë“¤ê³  ê³„íšì ìœ¼ë¡œ ì‡¼í•‘í•œë‹¤","ì¦‰í¥ì ìœ¼ë¡œ ë°œê²¬í•œ ìƒí’ˆì„ êµ¬ë§¤í•œë‹¤"],"axes":("J","P")},
            {"id":"M4","text":"ë¡œë´‡ì´ ë§¤ì¥ ìœ„ì¹˜ë¥¼ ì•ˆë‚´í•  ë•Œ ì„ í˜¸í•˜ëŠ” ì„¤ëª…ì€?",
             "choices":["êµ¬ì²´ì ì¸ ì¸µìˆ˜ì™€ ìœ„ì¹˜ ë²ˆí˜¸ë¥¼ ì•Œë ¤ì¤€ë‹¤","ì „ì²´ì ì¸ ë§¤ì¥ êµ¬ì¡°ì™€ ë¶„ìœ„ê¸°ë¥¼ ì„¤ëª…í•œë‹¤"],"axes":("S","N")},
            {"id":"M5","text":"ë¡œë´‡ì´ ê³ ê° ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•  ë•Œ ë‹¹ì‹ ì˜ ë°˜ì‘ì€?",
             "choices":["ì¦‰ì‹œ í•„ìš”í•œ ì„œë¹„ìŠ¤ë¥¼ ìš”ì²­í•œë‹¤","ë¨¼ì € ë¡œë´‡ì˜ ì„œë¹„ìŠ¤ ë²”ìœ„ë¥¼ í™•ì¸í•œë‹¤"],"axes":("E","I")}
        ],
        "í•™êµ": [
            {"id":"S1","text":"í•™êµì—ì„œ ë¡œë´‡ì´ ìˆ˜ì—…ì„ ë³´ì¡°í•  ë•Œ ì„ í˜¸í•˜ëŠ” ë°©ì‹ì€?",
             "choices":["êµ¬ì²´ì ì¸ í•™ìŠµ ëª©í‘œì™€ ë‹¨ê³„ë¥¼ ì œì‹œí•œë‹¤","ì „ì²´ì ì¸ í•™ìŠµ íë¦„ê³¼ ë§¥ë½ì„ ì„¤ëª…í•œë‹¤"],"axes":("S","N")},
            {"id":"S2","text":"ë¡œë´‡ì´ í•™ìƒë“¤ì˜ ì§ˆë¬¸ì— ë‹µí•  ë•Œ ì¤‘ì‹œí•˜ëŠ” ê²ƒì€?",
             "choices":["ì •í™•í•˜ê³  ê°ê´€ì ì¸ ì •ë³´ ì œê³µ","í•™ìƒì˜ ì´í•´ë„ì™€ ê°ì •ì  ìƒíƒœ ê³ ë ¤"],"axes":("T","F")},
            {"id":"S3","text":"ë¡œë´‡ê³¼ í•¨ê»˜ ê·¸ë£¹ í™œë™ì„ í•  ë•Œ ì„ í˜¸í•˜ëŠ” ì—­í• ì€?",
             "choices":["í™œë°œí•˜ê²Œ ì˜ê²¬ì„ ì œì‹œí•˜ê³  ì°¸ì—¬í•œë‹¤","ì¡°ìš©íˆ ê´€ì°°í•˜ê³  í•„ìš”í•  ë•Œë§Œ ì°¸ì—¬í•œë‹¤"],"axes":("E","I")},
            {"id":"S4","text":"ë¡œë´‡ì´ ê³¼ì œë¥¼ ê´€ë¦¬í•  ë•Œ ì„ í˜¸í•˜ëŠ” ë°©ì‹ì€?",
             "choices":["ëª…í™•í•œ ë§ˆê°ì¼ê³¼ ì²´í¬ë¦¬ìŠ¤íŠ¸ë¥¼ ì œê³µí•œë‹¤","ìœ ì—°í•œ ì¼ì •ê³¼ ì°½ì˜ì  ì ‘ê·¼ì„ ê¶Œì¥í•œë‹¤"],"axes":("J","P")},
            {"id":"S5","text":"ë¡œë´‡ì´ í•™êµ ìƒí™œì„ ì•ˆë‚´í•  ë•Œ ë‹¹ì‹ ì˜ ë°˜ì‘ì€?",
             "choices":["ì¦‰ì‹œ í•„ìš”í•œ ì •ë³´ë¥¼ ìš”ì²­í•œë‹¤","ì „ì²´ì ì¸ í•™êµ ë¬¸í™”ë¥¼ ë¨¼ì € ì´í•´í•œë‹¤"],"axes":("E","I")}
        ],
        "ê³µí•­": [
            {"id":"A1","text":"ê³µí•­ì—ì„œ ë¡œë´‡ì´ ìˆ˜í•˜ë¬¼ì„ ë„ì™€ì¤„ ë•Œ ì„ í˜¸í•˜ëŠ” ë°©ì‹ì€?",
             "choices":["êµ¬ì²´ì ì¸ ë¬´ê²Œì™€ í¬ê¸° ì œí•œì„ í™•ì¸í•œë‹¤","ì „ì²´ì ì¸ ìˆ˜í•˜ë¬¼ ì •ì±…ì„ ì´í•´í•œë‹¤"],"axes":("S","N")},
            {"id":"A2","text":"ë¡œë´‡ì´ ë³´ì•ˆ ê²€ì‚¬ë¥¼ ì•ˆë‚´í•  ë•Œ ì¤‘ì‹œí•˜ëŠ” ê²ƒì€?",
             "choices":["ì •í™•í•œ ì ˆì°¨ì™€ ê·œì • ì¤€ìˆ˜","í¸ì•ˆí•˜ê³  ìŠ¤íŠ¸ë ˆìŠ¤ ì—†ëŠ” ê²½í—˜"],"axes":("T","F")},
            {"id":"A3","text":"ë¡œë´‡ì´ í•­ê³µí¸ ì •ë³´ë¥¼ ì œê³µí•  ë•Œ ì„ í˜¸í•˜ëŠ” ì„¤ëª…ì€?",
             "choices":["êµ¬ì²´ì ì¸ ì‹œê°„ê³¼ ê²Œì´íŠ¸ ì •ë³´ë¥¼ ì œê³µí•œë‹¤","ì „ì²´ì ì¸ ì—¬í–‰ ì¼ì •ê³¼ ëŒ€ì•ˆì„ ì œì‹œí•œë‹¤"],"axes":("S","N")},
            {"id":"A4","text":"ë¡œë´‡ê³¼ í•¨ê»˜ ê³µí•­ì„ ì´ìš©í•  ë•Œ ë‹¹ì‹ ì˜ ìŠ¤íƒ€ì¼ì€?",
             "choices":["ë¯¸ë¦¬ ê³„íší•˜ê³  ì‹œê°„ì— ë§ì¶° ì´ë™í•œë‹¤","ìƒí™©ì— ë”°ë¼ ìœ ì—°í•˜ê²Œ ëŒ€ì‘í•œë‹¤"],"axes":("J","P")},
            {"id":"A5","text":"ë¡œë´‡ì´ ê¸´ê¸‰ ìƒí™©ì„ ì•ˆë‚´í•  ë•Œ ë‹¹ì‹ ì˜ ë°˜ì‘ì€?",
             "choices":["ì¦‰ì‹œ ì§€ì‹œì‚¬í•­ì„ ë”°ë¥´ê³  ëŒ€ì‘í•œë‹¤","ìƒí™©ì„ íŒŒì•…í•œ í›„ ì‹ ì¤‘í•˜ê²Œ íŒë‹¨í•œë‹¤"],"axes":("J","P")}
        ]
    }
    
    # ì„ íƒëœ ì¥ì†Œì˜ íŠ¹í™” ì§ˆë¬¸ ì¶”ê°€
    if location in location_specific_questions:
        return base_questions + location_specific_questions[location]
    else:
        return base_questions

def load_tie_questions(location="ì¼ë°˜"):
    """ì¥ì†Œë³„ íŠ¹í™”ëœ íƒ€ì´ë¸Œë ˆì´ì»¤ ì§ˆë¬¸ ë¡œë“œ"""
    base_tie_questions = {
        "EI": {"axes":("E","I"), "text":"ë¡œë´‡ê³¼ í•¨ê»˜í•˜ëŠ” í™œë™ì—ì„œ ì„ í˜¸í•˜ëŠ” í™˜ê²½ì€?", "choices":["ì‚¬ëŒë“¤ê³¼ í•¨ê»˜í•˜ëŠ” ë¶„ìœ„ê¸°","ì¡°ìš©í•˜ê³  ì§‘ì¤‘í•  ìˆ˜ ìˆëŠ” ê³µê°„"]},
        "SN": {"axes":("S","N"), "text":"ë¡œë´‡ì˜ ë¯¸ë˜ ê¸°ëŠ¥ì— ëŒ€í•œ ê´€ì‹¬ì€?", "choices":["í˜„ì¬ ì‹¤ìš©ì ì¸ ê¸°ëŠ¥ì— ì§‘ì¤‘","ë¯¸ë˜ì˜ í˜ì‹ ì  ê°€ëŠ¥ì„±ì— ê´€ì‹¬"]},
        "TF": {"axes":("T","F"), "text":"ë¡œë´‡ê³¼ì˜ ê´€ê³„ì—ì„œ ì¤‘ì‹œí•˜ëŠ” ê²ƒì€?", "choices":["íš¨ìœ¨ì„±ê³¼ ì„±ê³¼","ê°ì •ì  ì—°ê²°ê³¼ ì´í•´"]},
        "JP": {"axes":("J","P"), "text":"ë¡œë´‡ê³¼ì˜ ëª©í‘œ ë‹¬ì„±ì—ì„œ ì„ í˜¸í•˜ëŠ” ë°©ì‹ì€?", "choices":["ê³„íšì ì´ê³  ì²´ê³„ì ì¸ ì ‘ê·¼","ìœ ì—°í•˜ê³  ì ì‘ì ì¸ ë°©ë²•"]}
    }
    
    # ì¥ì†Œë³„ íŠ¹í™” íƒ€ì´ë¸Œë ˆì´ì»¤ ì§ˆë¬¸
    location_tie_questions = {
        "ë³‘ì›": {
            "EI": {"axes":("E","I"), "text":"ë³‘ì›ì—ì„œ ë¡œë´‡ê³¼ ìƒí˜¸ì‘ìš©í•  ë•Œ ì„ í˜¸í•˜ëŠ” ë°©ì‹ì€?", "choices":["ë‹¤ë¥¸ í™˜ìë“¤ê³¼ í•¨ê»˜ ì •ë³´ë¥¼ ê³µìœ í•œë‹¤","ê°œì¸ì ìœ¼ë¡œ ì¡°ìš©íˆ ìƒë‹´í•œë‹¤"]},
            "SN": {"axes":("S","N"), "text":"ì˜ë£Œ ë¡œë´‡ì˜ ì •ë³´ ì œê³µì—ì„œ ì¤‘ì‹œí•˜ëŠ” ê²ƒì€?", "choices":["êµ¬ì²´ì ì¸ ê²€ì‚¬ ê²°ê³¼ì™€ ìˆ˜ì¹˜","ì „ì²´ì ì¸ ê±´ê°• ìƒíƒœì™€ ì˜ˆí›„"]},
            "TF": {"axes":("T","F"), "text":"ë¡œë´‡ì´ ì˜ë£Œ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•  ë•Œ ì¤‘ì‹œí•˜ëŠ” ê²ƒì€?", "choices":["ì •í™•í•œ ì§„ë‹¨ê³¼ ì¹˜ë£Œ íš¨ê³¼","í™˜ìì˜ í¸ì•ˆí•¨ê³¼ ì‹¬ë¦¬ì  ì•ˆì •"]},
            "JP": {"axes":("J","P"), "text":"ë¡œë´‡ê³¼ì˜ ì¹˜ë£Œ ê³„íšì—ì„œ ì„ í˜¸í•˜ëŠ” ë°©ì‹ì€?", "choices":["ëª…í™•í•œ ì¹˜ë£Œ ë‹¨ê³„ì™€ ì¼ì •","ìƒí™©ì— ë”°ë¥¸ ìœ ì—°í•œ ì¡°ì •"]}
        },
        "ë„ì„œê´€": {
            "EI": {"axes":("E","I"), "text":"ë„ì„œê´€ì—ì„œ ë¡œë´‡ê³¼ í•¨ê»˜í•  ë•Œ ì„ í˜¸í•˜ëŠ” í™˜ê²½ì€?", "choices":["ë‹¤ë¥¸ ì´ìš©ìë“¤ê³¼ í•¨ê»˜í•˜ëŠ” ê³µê°„","ê°œì¸ì ìœ¼ë¡œ ì§‘ì¤‘í•  ìˆ˜ ìˆëŠ” ê³µê°„"]},
            "SN": {"axes":("S","N"), "text":"ë¡œë´‡ì˜ ë„ì„œ ì¶”ì²œì—ì„œ ì¤‘ì‹œí•˜ëŠ” ê²ƒì€?", "choices":["êµ¬ì²´ì ì¸ ì¥ë¥´ì™€ ì €ì ì •ë³´","ì „ì²´ì ì¸ ë…ì„œ ê²½í—˜ê³¼ ê°ë™"]},
            "TF": {"axes":("T","F"), "text":"ë¡œë´‡ì´ í•™ìŠµì„ ë„ì™€ì¤„ ë•Œ ì¤‘ì‹œí•˜ëŠ” ê²ƒì€?", "choices":["ì •í™•í•œ ì •ë³´ì™€ ê°ê´€ì  ì‚¬ì‹¤","ê°œì¸ì˜ ê´€ì‹¬ê³¼ ê°ì •ì  ì—°ê²°"]},
            "JP": {"axes":("J","P"), "text":"ë¡œë´‡ê³¼ì˜ í•™ìŠµ ê³„íšì—ì„œ ì„ í˜¸í•˜ëŠ” ë°©ì‹ì€?", "choices":["ì²´ê³„ì ì¸ í•™ìŠµ ì¼ì •ê³¼ ëª©í‘œ","ììœ ë¡œìš´ íƒêµ¬ì™€ ë°œê²¬"]}
        },
        "ì‡¼í•‘ëª°": {
            "EI": {"axes":("E","I"), "text":"ì‡¼í•‘ëª°ì—ì„œ ë¡œë´‡ê³¼ ìƒí˜¸ì‘ìš©í•  ë•Œ ì„ í˜¸í•˜ëŠ” ë°©ì‹ì€?", "choices":["ë‹¤ë¥¸ ì‡¼í•‘ê°ë“¤ê³¼ í•¨ê»˜ ì •ë³´ë¥¼ ê³µìœ í•œë‹¤","ê°œì¸ì ìœ¼ë¡œ ì¡°ìš©íˆ ìƒë‹´í•œë‹¤"]},
            "SN": {"axes":("S","N"), "text":"ë¡œë´‡ì˜ ìƒí’ˆ ì¶”ì²œì—ì„œ ì¤‘ì‹œí•˜ëŠ” ê²ƒì€?", "choices":["êµ¬ì²´ì ì¸ ìƒí’ˆ ì •ë³´ì™€ ê°€ê²©","ì „ì²´ì ì¸ ìŠ¤íƒ€ì¼ê³¼ íŠ¸ë Œë“œ"]},
            "TF": {"axes":("T","F"), "text":"ë¡œë´‡ì´ ì‡¼í•‘ì„ ë„ì™€ì¤„ ë•Œ ì¤‘ì‹œí•˜ëŠ” ê²ƒì€?", "choices":["íš¨ìœ¨ì ì¸ êµ¬ë§¤ì™€ ì ˆì•½","ì¦ê±°ìš´ ì‡¼í•‘ ê²½í—˜ê³¼ ë§Œì¡±"]},
            "JP": {"axes":("J","P"), "text":"ë¡œë´‡ê³¼ì˜ ì‡¼í•‘ ê³„íšì—ì„œ ì„ í˜¸í•˜ëŠ” ë°©ì‹ì€?", "choices":["ë¯¸ë¦¬ ê³„íší•˜ê³  ëª©ì ì ìœ¼ë¡œ ì‡¼í•‘í•œë‹¤","ì¦‰í¥ì ìœ¼ë¡œ ë°œê²¬í•œ ìƒí’ˆì„ êµ¬ë§¤í•œë‹¤"]}
        },
        "í•™êµ": {
            "EI": {"axes":("E","I"), "text":"í•™êµì—ì„œ ë¡œë´‡ê³¼ í•¨ê»˜í•  ë•Œ ì„ í˜¸í•˜ëŠ” í•™ìŠµ í™˜ê²½ì€?", "choices":["ë‹¤ë¥¸ í•™ìƒë“¤ê³¼ í•¨ê»˜í•˜ëŠ” ê·¸ë£¹ í™œë™","ê°œì¸ì ìœ¼ë¡œ ì§‘ì¤‘í•  ìˆ˜ ìˆëŠ” í™˜ê²½"]},
            "SN": {"axes":("S","N"), "text":"ë¡œë´‡ì˜ í•™ìŠµ ì•ˆë‚´ì—ì„œ ì¤‘ì‹œí•˜ëŠ” ê²ƒì€?", "choices":["êµ¬ì²´ì ì¸ í•™ìŠµ ëª©í‘œì™€ ë‹¨ê³„","ì „ì²´ì ì¸ í•™ìŠµ íë¦„ê³¼ ë§¥ë½"]},
            "TF": {"axes":("T","F"), "text":"ë¡œë´‡ì´ êµìœ¡ì„ ë„ì™€ì¤„ ë•Œ ì¤‘ì‹œí•˜ëŠ” ê²ƒì€?", "choices":["ì •í™•í•œ ì§€ì‹ê³¼ ê°ê´€ì  í‰ê°€","í•™ìƒì˜ ê´€ì‹¬ê³¼ ê°ì •ì  ì„±ì¥"]},
            "JP": {"axes":("J","P"), "text":"ë¡œë´‡ê³¼ì˜ í•™ìŠµ ê³„íšì—ì„œ ì„ í˜¸í•˜ëŠ” ë°©ì‹ì€?", "choices":["ì²´ê³„ì ì¸ í•™ìŠµ ì¼ì •ê³¼ í‰ê°€","ììœ ë¡œìš´ íƒêµ¬ì™€ ì°½ì˜ì  í™œë™"]}
        },
        "ê³µí•­": {
            "EI": {"axes":("E","I"), "text":"ê³µí•­ì—ì„œ ë¡œë´‡ê³¼ ìƒí˜¸ì‘ìš©í•  ë•Œ ì„ í˜¸í•˜ëŠ” ë°©ì‹ì€?", "choices":["ë‹¤ë¥¸ ì—¬í–‰ê°ë“¤ê³¼ í•¨ê»˜ ì •ë³´ë¥¼ ê³µìœ í•œë‹¤","ê°œì¸ì ìœ¼ë¡œ ì¡°ìš©íˆ ìƒë‹´í•œë‹¤"]},
            "SN": {"axes":("S","N"), "text":"ë¡œë´‡ì˜ ì—¬í–‰ ì•ˆë‚´ì—ì„œ ì¤‘ì‹œí•˜ëŠ” ê²ƒì€?", "choices":["êµ¬ì²´ì ì¸ ì‹œê°„ê³¼ ì ˆì°¨ ì •ë³´","ì „ì²´ì ì¸ ì—¬í–‰ ê²½í—˜ê³¼ í¸ì˜"]},
            "TF": {"axes":("T","F"), "text":"ë¡œë´‡ì´ ì—¬í–‰ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•  ë•Œ ì¤‘ì‹œí•˜ëŠ” ê²ƒì€?", "choices":["ì •í™•í•œ ì •ë³´ì™€ íš¨ìœ¨ì ì¸ ì„œë¹„ìŠ¤","í¸ì•ˆí•˜ê³  ìŠ¤íŠ¸ë ˆìŠ¤ ì—†ëŠ” ê²½í—˜"]},
            "JP": {"axes":("J","P"), "text":"ë¡œë´‡ê³¼ì˜ ì—¬í–‰ ê³„íšì—ì„œ ì„ í˜¸í•˜ëŠ” ë°©ì‹ì€?", "choices":["ë¯¸ë¦¬ ê³„íší•˜ê³  ì‹œê°„ì— ë§ì¶° ì§„í–‰í•œë‹¤","ìƒí™©ì— ë”°ë¼ ìœ ì—°í•˜ê²Œ ëŒ€ì‘í•œë‹¤"]}
        }
    }
    
    # ì„ íƒëœ ì¥ì†Œì˜ íŠ¹í™” íƒ€ì´ë¸Œë ˆì´ì»¤ ì§ˆë¬¸ ì‚¬ìš©
    if location in location_tie_questions:
        return location_tie_questions[location]
    else:
        return base_tie_questions

def compute_scores(responses):
    """ì ìˆ˜ ê³„ì‚°"""
    questions = load_questions(st.session_state.selected_location)
    scores = {axis: 0 for axis in ['E','I','S','N','T','F','J','P']}
    for q in questions:
        choice = responses.get(q['id'])
        if choice is None:
            return None
        pos, neg = q['axes']
        scores[pos if choice == q['choices'][0] else neg] += 1
    return scores

def resolve_ties(scores):
    """ë™ì  í•´ê²°"""
    tie_questions = load_tie_questions(st.session_state.selected_location)
    for axis, cfg in tie_questions.items():
        a, b = cfg['axes']
        if scores[a] == scores[b]:
            tie_choices = ["- ì„ íƒí•˜ì„¸ìš” -"] + list(cfg['choices'])
            choice = st.radio(cfg["text"], tie_choices, index=0, key=f"tie_{axis}")
            if choice == "- ì„ íƒí•˜ì„¸ìš” -":
                st.warning("ì¶”ê°€ ì„¤ë¬¸ ë¬¸í•­ ì‘ë‹µì„ ì„ íƒí•´ì•¼ ì§„ë‹¨ì´ ì™„ì„±ë©ë‹ˆë‹¤.")
                st.stop()
            scores[a if choice == cfg['choices'][1] else b] += 1
    return scores

def predict_type(scores):
    """MBTI ìœ í˜• ì˜ˆì¸¡"""
    return ''.join([
        'E' if scores['E'] >= scores['I'] else 'I',
        'S' if scores['S'] >= scores['N'] else 'N',
        'T' if scores['T'] >= scores['F'] else 'F',
        'J' if scores['J'] >= scores['P'] else 'P'
    ])

# ì‹œê°í™” í•¨ìˆ˜ë“¤
def create_score_chart(scores):
    """ì ìˆ˜ ë¶„í¬ ì°¨íŠ¸ ìƒì„±"""
    score_data = {
        'ì¶•': ['E', 'I', 'S', 'N', 'T', 'F', 'J', 'P'],
        'ì ìˆ˜': [scores['E'], scores['I'], scores['S'], scores['N'], 
                scores['T'], scores['F'], scores['J'], scores['P']]
    }
    score_df = pd.DataFrame(score_data)
    
    fig = px.bar(score_df, x='ì¶•', y='ì ìˆ˜', 
                title="MBTI ì¶•ë³„ ì ìˆ˜",
                color='ì¶•',
                color_discrete_map=MBTI_COLORS)
    fig.update_layout(height=300)
    return fig

def create_trend_chart(df, chart_type="line"):
    """íŠ¸ë Œë“œ ì°¨íŠ¸ ìƒì„±"""
    if df.empty:
        # ë¹ˆ ë°ì´í„°ì¼ ë•Œ ì•ˆë‚´ ë©”ì‹œì§€ê°€ í¬í•¨ëœ ì°¨íŠ¸ ìƒì„±
        fig = go.Figure()
        fig.add_annotation(
            text="ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.<br>ë¨¼ì € ì§„ë‹¨ì„ ì™„ë£Œí•´ì£¼ì„¸ìš”.",
            xref="paper", yref="paper",
            x=0.5, y=0.5, showarrow=False,
            font=dict(size=16, color="gray")
        )
        fig.update_layout(
            title="ğŸ“Š ê¸°ê°„ë³„ MBTI íŠ¸ë Œë“œ",
            height=400,
            xaxis=dict(showgrid=False, showticklabels=False),
            yaxis=dict(showgrid=False, showticklabels=False)
        )
        return fig
    
    # ë‚ ì§œ í˜•ì‹ ê°œì„ 
    df['date'] = pd.to_datetime(df['date'])
    daily_mbti = df.groupby(['date', 'mbti']).size().reset_index(name='count')
    
    # ë‚ ì§œë¥¼ ë” ì½ê¸° ì‰½ê²Œ í¬ë§·íŒ…
    daily_mbti['date_formatted'] = daily_mbti['date'].dt.strftime('%Yë…„ %mì›” %dì¼')
    
    if chart_type == "ë¼ì¸":
        fig = px.line(daily_mbti, x='date', y='count', color='mbti',
                    title="ğŸ“Š ê¸°ê°„ë³„ MBTI íŠ¸ë Œë“œ", color_discrete_map=MBTI_COLORS,
                    hover_data=['date_formatted'])
    elif chart_type == "ë°”":
        fig = px.bar(daily_mbti, x='date', y='count', color='mbti',
                   title="ğŸ“Š ê¸°ê°„ë³„ MBTI ë¶„í¬", color_discrete_map=MBTI_COLORS,
                   hover_data=['date_formatted'])
    else:
        fig = px.area(daily_mbti, x='date', y='count', color='mbti',
                    title="ğŸ“Š ê¸°ê°„ë³„ MBTI ëˆ„ì  ë¶„í¬", color_discrete_map=MBTI_COLORS,
                    hover_data=['date_formatted'])
    
    # ë ˆì´ì•„ì›ƒ ê°œì„ 
    fig.update_layout(
        height=500,
        showlegend=True,
        xaxis_title="ë‚ ì§œ",
        yaxis_title="ì§„ë‹¨ ìˆ˜",
        xaxis=dict(
            tickformat='%mì›” %dì¼',
            tickmode='auto',
            nticks=min(10, len(daily_mbti['date'].unique()))
        ),
        yaxis=dict(
            tickmode='linear',
            dtick=1
        ),
        hovermode='x unified',
        legend=dict(
            orientation="h",
            yanchor="bottom",
            y=1.02,
            xanchor="right",
            x=1
        ),
        margin=dict(l=50, r=50, t=80, b=80)
    )
    
    # í˜¸ë²„ í…œí”Œë¦¿ ê°œì„ 
    fig.update_traces(
        hovertemplate="<b>%{customdata[0]}</b><br>" +
                     "MBTI: %{fullData.name}<br>" +
                     "ì§„ë‹¨ ìˆ˜: %{y}<extra></extra>"
    )
    
    return fig

def create_correlation_heatmap(df):
    """ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ ìƒì„±"""
    mbti_dummies = pd.get_dummies(df['mbti'])
    corr = mbti_dummies.corr()
    fig = px.imshow(corr, title="MBTI ìœ í˜• ê°„ ìƒê´€í–‰ë ¬", 
                   aspect="auto", color_continuous_scale="RdBu")
    return fig, corr

# ë¶„ì„ ê²°ê³¼ ìë™ í•´ì„ í•¨ìˆ˜ë“¤
def analyze_heatmap_patterns(group_df, group_col):
    """íˆíŠ¸ë§µ íŒ¨í„´ ìë™ ë¶„ì„ ë° í•´ì„"""
    interpretations = []
    
    try:
        # ê° ê·¸ë£¹ë³„ ìµœê³ /ìµœì € MBTI ì°¾ê¸°
        for group in group_df.index:
            group_data = group_df.loc[group]
            max_mbti = group_data.idxmax()
            max_value = group_data.max()
            min_mbti = group_data.idxmin()
            min_value = group_data.min()
            
            # ë¹„ìœ¨ ê³„ì‚°
            total = group_data.sum()
            max_ratio = (max_value / total * 100) if total > 0 else 0
            
            if max_ratio > 30:  # 30% ì´ìƒì´ë©´ ì£¼ëª©í•  ë§Œí•œ íŒ¨í„´
                mbti_desc = get_mbti_description(max_mbti)
                interpretations.append(f"**{group}**: {max_mbti}({max_ratio:.1f}%)ê°€ ìš°ì„¸ â†’ {mbti_desc}")
        
        # ì „ì²´ì ì¸ íŒ¨í„´ ë¶„ì„
        if group_col == "gender":
            male_data = group_df.loc["ë‚¨"] if "ë‚¨" in group_df.index else None
            female_data = group_df.loc["ì—¬"] if "ì—¬" in group_df.index else None
            
            if male_data is not None and female_data is not None:
                # T/F ì¶• ë¹„êµ
                male_t = male_data[[col for col in male_data.index if 'T' in col]].sum()
                male_f = male_data[[col for col in male_data.index if 'F' in col]].sum()
                female_t = female_data[[col for col in female_data.index if 'T' in col]].sum()
                female_f = female_data[[col for col in female_data.index if 'F' in col]].sum()
                
                male_t_ratio = male_t / (male_t + male_f) * 100 if (male_t + male_f) > 0 else 0
                female_f_ratio = female_f / (female_t + female_f) * 100 if (female_t + female_f) > 0 else 0
                
                if male_t_ratio > 55:
                    interpretations.append(f"**ì„±ë³„ ì°¨ì´**: ë‚¨ì„±ì€ T(ì‚¬ê³ í˜•) {male_t_ratio:.1f}% â†’ ë…¼ë¦¬ì  ì ‘ê·¼ ì„ í˜¸")
                if female_f_ratio > 55:
                    interpretations.append(f"**ì„±ë³„ ì°¨ì´**: ì—¬ì„±ì€ F(ê°ì •í˜•) {female_f_ratio:.1f}% â†’ ê°ì •ì  ë°°ë ¤ ì¤‘ì‹œ")
        
        elif group_col == "age_group":
            # ì—°ë ¹ëŒ€ë³„ íŠ¹ì„± ë¶„ì„
            age_patterns = {}
            for age in group_df.index:
                age_data = group_df.loc[age]
                # E/I ì¶• ë¶„ì„
                e_types = age_data[[col for col in age_data.index if col.startswith('E')]].sum()
                i_types = age_data[[col for col in age_data.index if col.startswith('I')]].sum()
                e_ratio = e_types / (e_types + i_types) * 100 if (e_types + i_types) > 0 else 0
                
                if e_ratio > 60:
                    interpretations.append(f"**{age}**: ì™¸í–¥í˜•(E) {e_ratio:.1f}% â†’ í™œë°œí•œ ë¡œë´‡ ìƒí˜¸ì‘ìš© ì„ í˜¸")
                elif e_ratio < 40:
                    interpretations.append(f"**{age}**: ë‚´í–¥í˜•(I) {100-e_ratio:.1f}% â†’ ì‹ ì¤‘í•œ ë¡œë´‡ ìƒí˜¸ì‘ìš© ì„ í˜¸")
        
        elif group_col == "job":
            # ì§ì—…ë³„ íŠ¹ì„± ë¶„ì„
            for job in group_df.index:
                job_data = group_df.loc[job]
                # NT ì¡°í•© (ë¶„ì„ê°€í˜•) í™•ì¸
                nt_types = job_data[[col for col in job_data.index if 'NT' in col or (col.startswith('N') and 'T' in col) or (col.startswith('E') and 'NT' in col) or (col.startswith('I') and 'NT' in col)]].sum()
                total = job_data.sum()
                nt_ratio = nt_types / total * 100 if total > 0 else 0
                
                if nt_ratio > 40:
                    interpretations.append(f"**{job}**: NTì¡°í•© {nt_ratio:.1f}% â†’ ë…¼ë¦¬ì Â·ì²´ê³„ì  ë¡œë´‡ í™œìš© ì„ í˜¸")
    
    except Exception as e:
        interpretations.append(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    return interpretations

def get_mbti_description(mbti):
    """MBTI ìœ í˜•ë³„ ê°„ë‹¨í•œ ì„¤ëª…"""
    descriptions = {
        'ENFJ': 'ê³µê°ì  ë¦¬ë”ì‹­', 'ENTJ': 'ì „ëµì  ì‚¬ê³ ', 'ENTP': 'ì°½ì˜ì  í˜ì‹ ', 'ENFP': 'ì—´ì •ì  ì˜ê°',
        'ESFJ': 'í˜‘ë ¥ì  ì§€ì›', 'ESFP': 'ì¦‰í¥ì  ì¹œê·¼í•¨', 'ESTJ': 'ì²´ê³„ì  ê´€ë¦¬', 'ESTP': 'ì‹¤ìš©ì  ì ì‘',
        'INFJ': 'ì§ê´€ì  í†µì°°', 'INFP': 'ì´ìƒì£¼ì˜ì  ì°½ì˜', 'INTJ': 'ë…ì°½ì  ì „ëµ', 'INTP': 'ë…¼ë¦¬ì  ë¶„ì„',
        'ISFJ': 'ì‹ ì¤‘í•œ í—Œì‹ ', 'ISFP': 'ì˜ˆìˆ ì  ì‹¤ìš©', 'ISTJ': 'ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì²´ê³„', 'ISTP': 'ì‹¤ìš©ì  ë¶„ì„'
    }
    return descriptions.get(mbti, 'ë…íŠ¹í•œ ì„±í–¥')

def analyze_correlation_patterns(corr_matrix):
    """ìƒê´€ê´€ê³„ íŒ¨í„´ ìë™ ë¶„ì„ ë° í•´ì„"""
    interpretations = []
    
    try:
        # ê°•í•œ ì–‘ì˜ ìƒê´€ê´€ê³„ (r > 0.7) ì°¾ê¸°
        strong_positive = []
        strong_negative = []
        
        for i in range(len(corr_matrix.columns)):
            for j in range(i+1, len(corr_matrix.columns)):
                corr_val = corr_matrix.iloc[i, j]
                type1 = corr_matrix.columns[i]
                type2 = corr_matrix.columns[j]
                
                if corr_val > 0.7:
                    strong_positive.append((type1, type2, corr_val))
                elif corr_val < -0.7:
                    strong_negative.append((type1, type2, corr_val))
        
        # ê°•í•œ ì–‘ì˜ ìƒê´€ê´€ê³„ í•´ì„
        if strong_positive:
            interpretations.append("**ğŸ”— ê°•í•œ ì–‘ì˜ ìƒê´€ê´€ê³„ (í•¨ê»˜ ë‚˜íƒ€ë‚˜ëŠ” ê²½í–¥):**")
            for type1, type2, corr_val in strong_positive[:3]:  # ìƒìœ„ 3ê°œë§Œ
                reason = get_correlation_reason(type1, type2, "positive")
                interpretations.append(f"â€¢ {type1} â†” {type2} (r={corr_val:.2f}): {reason}")
        
        # ê°•í•œ ìŒì˜ ìƒê´€ê´€ê³„ í•´ì„
        if strong_negative:
            interpretations.append("**âŒ ê°•í•œ ìŒì˜ ìƒê´€ê´€ê³„ (ìƒí˜¸ ë°°íƒ€ì ):**")
            for type1, type2, corr_val in strong_negative[:3]:  # ìƒìœ„ 3ê°œë§Œ
                reason = get_correlation_reason(type1, type2, "negative")
                interpretations.append(f"â€¢ {type1} â†” {type2} (r={corr_val:.2f}): {reason}")
        
        # MBTI ì¶•ë³„ ìƒê´€ê´€ê³„ ë¶„ì„
        axes_analysis = analyze_mbti_axes_correlation(corr_matrix)
        if axes_analysis:
            interpretations.append("**ğŸ“Š MBTI ì¶•ë³„ íŒ¨í„´:**")
            interpretations.extend(axes_analysis)
    
    except Exception as e:
        interpretations.append(f"ìƒê´€ê´€ê³„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    return interpretations

def get_correlation_reason(type1, type2, correlation_type):
    """ìƒê´€ê´€ê³„ì˜ ì´ìœ  ì„¤ëª…"""
    if correlation_type == "positive":
        # ê³µí†µì  ì°¾ê¸°
        common_traits = []
        if type1[0] == type2[0]:  # E/I ê°™ìŒ
            common_traits.append("ê°™ì€ ì—ë„ˆì§€ ë°©í–¥")
        if type1[1] == type2[1]:  # S/N ê°™ìŒ
            common_traits.append("ê°™ì€ ì •ë³´ ì²˜ë¦¬")
        if type1[2] == type2[2]:  # T/F ê°™ìŒ
            common_traits.append("ê°™ì€ ì˜ì‚¬ê²°ì •")
        if type1[3] == type2[3]:  # J/P ê°™ìŒ
            common_traits.append("ê°™ì€ ìƒí™œ ì–‘ì‹")
        
        if common_traits:
            return f"{', '.join(common_traits)} ê³µìœ "
        else:
            return "ë³´ì™„ì  ê´€ê³„"
    
    else:  # negative
        # ì°¨ì´ì  ì°¾ê¸°
        differences = []
        if type1[0] != type2[0]:
            differences.append("E/I ëŒ€ë¦½")
        if type1[1] != type2[1]:
            differences.append("S/N ëŒ€ë¦½")
        if type1[2] != type2[2]:
            differences.append("T/F ëŒ€ë¦½")
        if type1[3] != type2[3]:
            differences.append("J/P ëŒ€ë¦½")
        
        return f"{', '.join(differences[:2])} ë“± ì •ë°˜ëŒ€ ì„±í–¥"

def analyze_mbti_axes_correlation(corr_matrix):
    """MBTI ì¶•ë³„ ìƒê´€ê´€ê³„ ë¶„ì„"""
    analyses = []
    
    try:
        # E/I ì¶• ë¶„ì„
        e_types = [col for col in corr_matrix.columns if col.startswith('E')]
        i_types = [col for col in corr_matrix.columns if col.startswith('I')]
        
        if e_types and i_types:
            # Eíƒ€ì…ë“¤ ê°„ì˜ í‰ê·  ìƒê´€ê´€ê³„
            e_correlations = []
            for i in range(len(e_types)):
                for j in range(i+1, len(e_types)):
                    if e_types[i] in corr_matrix.columns and e_types[j] in corr_matrix.columns:
                        e_correlations.append(corr_matrix.loc[e_types[i], e_types[j]])
            
            if e_correlations:
                avg_e_corr = np.mean(e_correlations)
                if avg_e_corr > 0.3:
                    analyses.append(f"â€¢ Eíƒ€ì…ë“¤ ê°„ ì–‘ì˜ ìƒê´€ê´€ê³„ (r={avg_e_corr:.2f}) â†’ ì™¸í–¥ì  ì„±í–¥ ê³µìœ ")
        
        # ë¹„ìŠ·í•œ ë°©ì‹ìœ¼ë¡œ ë‹¤ë¥¸ ì¶•ë“¤ë„ ë¶„ì„...
        
    except Exception as e:
        analyses.append(f"ì¶•ë³„ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
    
    return analyses

def analyze_time_patterns(df):
    """ì‹œê°„ëŒ€ë³„ ì§„ë‹¨ íŒ¨í„´ ë¶„ì„ ë° í•´ì„"""
    interpretations = []
    
    try:
        # ì‹œê°„ëŒ€ë³„ ë¶„ì„ì„ ìœ„í•´ timestampë¥¼ datetimeìœ¼ë¡œ ë³€í™˜
        df['datetime'] = pd.to_datetime(df['timestamp'])
        df['hour'] = df['datetime'].dt.hour
        df['weekday'] = df['datetime'].dt.day_name()
        df['is_weekend'] = df['datetime'].dt.weekday >= 5
        
        # ì‹œê°„ëŒ€ë³„ ë¶„í¬
        hourly_counts = df['hour'].value_counts().sort_index()
        peak_hour = hourly_counts.idxmax()
        peak_count = hourly_counts.max()
        low_hour = hourly_counts.idxmin()
        low_count = hourly_counts.min()
        
        total_diagnoses = len(df)
        peak_percentage = (peak_count / total_diagnoses) * 100
        
        interpretations.append("**â° ì‹œê°„ëŒ€ë³„ ì§„ë‹¨ íŒ¨í„´:**")
        interpretations.append(f"â€¢ **í”¼í¬ ì‹œê°„**: {peak_hour}ì‹œ ({peak_count}ê±´, {peak_percentage:.1f}%) â†’ {get_time_meaning(peak_hour)}")
        interpretations.append(f"â€¢ **ìµœì € ì‹œê°„**: {low_hour}ì‹œ ({low_count}ê±´) â†’ {get_time_meaning(low_hour)}")
        
        # ì£¼ì¤‘ vs ì£¼ë§ ë¶„ì„
        weekday_count = len(df[~df['is_weekend']])
        weekend_count = len(df[df['is_weekend']])
        
        if weekday_count > 0 and weekend_count > 0:
            weekday_ratio = weekday_count / total_diagnoses * 100
            weekend_ratio = weekend_count / total_diagnoses * 100
            
            interpretations.append("**ğŸ“… ì£¼ì¤‘ vs ì£¼ë§ íŒ¨í„´:**")
            interpretations.append(f"â€¢ **ì£¼ì¤‘**: {weekday_count}ê±´ ({weekday_ratio:.1f}%) â†’ ì—…ë¬´/í•™ì—… ì¤‘ ê´€ì‹¬")
            interpretations.append(f"â€¢ **ì£¼ë§**: {weekend_count}ê±´ ({weekend_ratio:.1f}%) â†’ ì—¬ê°€ ì‹œê°„ í™œìš©")
        
        # ì§„ë‹¨ ìˆ˜ì˜ ì˜ë¯¸ í•´ì„
        interpretations.append("**ğŸ“ˆ ì§„ë‹¨ ìˆ˜ê°€ ì˜ë¯¸í•˜ëŠ” ê²ƒ:**")
        if total_diagnoses > 50:
            interpretations.append("â€¢ **ë†’ì€ ê´€ì‹¬ë„**: ë¡œë´‡ ìƒí˜¸ì‘ìš©ì— ëŒ€í•œ ì‚¬ìš©ìë“¤ì˜ ë†’ì€ ê´€ì‹¬")
            interpretations.append("â€¢ **ë‹¤ì–‘í•œ ì‚¬ìš© íŒ¨í„´**: ë‹¤ì–‘í•œ ì‹œê°„ëŒ€ì™€ ìƒí™©ì—ì„œì˜ ë¡œë´‡ í™œìš©")
        elif total_diagnoses > 20:
            interpretations.append("â€¢ **ì¤‘ê°„ ê´€ì‹¬ë„**: ë¡œë´‡ ê¸°ìˆ ì— ëŒ€í•œ ì ì§„ì  ê´€ì‹¬ ì¦ê°€")
            interpretations.append("â€¢ **íŠ¹ì • ê·¸ë£¹ ì§‘ì¤‘**: íŠ¹ì • ì‚¬ìš©ì ê·¸ë£¹ì˜ ì§‘ì¤‘ì  í™œìš©")
        else:
            interpretations.append("â€¢ **ì´ˆê¸° ë‹¨ê³„**: ë¡œë´‡ ìƒí˜¸ì‘ìš© ì§„ë‹¨ì˜ ì´ˆê¸° ë„ì… ë‹¨ê³„")
            interpretations.append("â€¢ **ê°œì„  í•„ìš”**: ë” ë§ì€ ì‚¬ìš©ì ì°¸ì—¬ë¥¼ ìœ„í•œ í™ë³´ í•„ìš”")
        
        # íŠ¸ë Œë“œ ë¶„ì„
        if len(df) > 1:
            df_sorted = df.sort_values('datetime')
            first_date = df_sorted['datetime'].iloc[0]
            last_date = df_sorted['datetime'].iloc[-1]
            date_range = (last_date - first_date).days
            
            if date_range > 0:
                daily_avg = total_diagnoses / date_range
                interpretations.append(f"â€¢ **ì¼í‰ê·  ì§„ë‹¨**: {daily_avg:.1f}ê±´ â†’ {get_trend_meaning(daily_avg)}")
    
    except Exception as e:
        interpretations.append(f"ì‹œê°„ íŒ¨í„´ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    return interpretations

def get_time_meaning(hour):
    """ì‹œê°„ëŒ€ë³„ ì˜ë¯¸ í•´ì„"""
    if 6 <= hour <= 8:
        return "ì•„ì¹¨ ì¶œê·¼/ë“±êµ ì‹œê°„ëŒ€ì˜ ê´€ì‹¬"
    elif 9 <= hour <= 11:
        return "ì˜¤ì „ ì—…ë¬´ ì‹œê°„ ì¤‘ ì—¬ìœ "
    elif 12 <= hour <= 13:
        return "ì ì‹¬ì‹œê°„ í™œìš©"
    elif 14 <= hour <= 16:
        return "ì˜¤í›„ ì—¬ìœ  ì‹œê°„ í™œìš©"
    elif 17 <= hour <= 19:
        return "í‡´ê·¼/í•˜êµ í›„ ê´€ì‹¬"
    elif 20 <= hour <= 22:
        return "ì €ë… ì—¬ê°€ ì‹œê°„ í™œìš©"
    elif 23 <= hour or hour <= 2:
        return "ëŠ¦ì€ ì‹œê°„ ê°œì¸ì  ê´€ì‹¬"
    else:
        return "ìƒˆë²½ ì‹œê°„ëŒ€ (íŠ¹ì´ íŒ¨í„´)"

def get_trend_meaning(daily_avg):
    """ì¼í‰ê·  ì§„ë‹¨ ìˆ˜ì˜ ì˜ë¯¸"""
    if daily_avg > 5:
        return "ë§¤ìš° í™œë°œí•œ ì‚¬ìš©"
    elif daily_avg > 2:
        return "ê¾¸ì¤€í•œ ê´€ì‹¬ê³¼ í™œìš©"
    elif daily_avg > 1:
        return "ì ì§„ì  í™•ì‚°"
    else:
        return "ì´ˆê¸° ë„ì… ë‹¨ê³„"

def analyze_network_patterns(df):
    """ë„¤íŠ¸ì›Œí¬ ë¶„ì„ íŒ¨í„´ í•´ì„"""
    interpretations = []
    
    try:
        # MBTI ìœ í˜•ë³„ ë¹ˆë„ ê³„ì‚°
        mbti_counts = df['mbti'].value_counts()
        
        if len(mbti_counts) < 2:
            interpretations.append("ë„¤íŠ¸ì›Œí¬ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ë” ë§ì€ MBTI ìœ í˜• ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return interpretations
        
        # ê°€ì¥ ë§ì€ ìœ í˜•ê³¼ ì ì€ ìœ í˜•
        most_common = mbti_counts.index[0]
        most_count = mbti_counts.iloc[0]
        least_common = mbti_counts.index[-1]
        least_count = mbti_counts.iloc[-1]
        
        total = len(df)
        most_ratio = most_count / total * 100
        least_ratio = least_count / total * 100
        
        interpretations.append("**ğŸŒ MBTI ë„¤íŠ¸ì›Œí¬ ë¶„ì„:**")
        interpretations.append(f"â€¢ **ì¤‘ì‹¬ í—ˆë¸Œ ìœ í˜•**: {most_common} ({most_count}ê±´, {most_ratio:.1f}%) â†’ {get_network_role(most_common, 'hub')}")
        interpretations.append(f"â€¢ **í¬ì†Œ ìœ í˜•**: {least_common} ({least_count}ê±´, {least_ratio:.1f}%) â†’ {get_network_role(least_common, 'rare')}")
        
        # í´ëŸ¬ìŠ¤í„° ë¶„ì„
        clusters = analyze_mbti_clusters(mbti_counts)
        if clusters:
            interpretations.append("**ğŸ”— í´ëŸ¬ìŠ¤í„° í˜•ì„±:**")
            interpretations.extend(clusters)
        
        # ë„¤íŠ¸ì›Œí¬ ì˜ë¯¸ í•´ì„
        interpretations.append("**ğŸ“Š ë„¤íŠ¸ì›Œí¬ê°€ ì˜ë¯¸í•˜ëŠ” ê²ƒ:**")
        diversity_score = len(mbti_counts) / 16 * 100  # 16ê°œ ìœ í˜• ì¤‘ ëª‡ ê°œê°€ ë‚˜íƒ€ë‚¬ëŠ”ì§€
        
        if diversity_score > 75:
            interpretations.append("â€¢ **ë†’ì€ ë‹¤ì–‘ì„±**: ë‹¤ì–‘í•œ ì„±í–¥ì˜ ì‚¬ìš©ìë“¤ì´ ë¡œë´‡ì— ê´€ì‹¬")
            interpretations.append("â€¢ **í¬ìš©ì  ê¸°ìˆ **: ëª¨ë“  ì„±í–¥ì— ì í•©í•œ ë¡œë´‡ ìƒí˜¸ì‘ìš© ì„¤ê³„ í•„ìš”")
        elif diversity_score > 50:
            interpretations.append("â€¢ **ì¤‘ê°„ ë‹¤ì–‘ì„±**: íŠ¹ì • ì„±í–¥ ê·¸ë£¹ì˜ ì§‘ì¤‘ì  ê´€ì‹¬")
            interpretations.append("â€¢ **íƒ€ê²Ÿ ìµœì í™”**: ì£¼ìš” ì‚¬ìš©ì ê·¸ë£¹ì— ë§ì¶¤í˜• ê¸°ëŠ¥ ê°•í™”")
        else:
            interpretations.append("â€¢ **ì œí•œì  ë‹¤ì–‘ì„±**: íŠ¹ì • ì„±í–¥ì— í¸ì¤‘ëœ ì‚¬ìš© íŒ¨í„´")
            interpretations.append("â€¢ **í™•ì‚° í•„ìš”**: ë‹¤ì–‘í•œ ì„±í–¥ì˜ ì‚¬ìš©ì ìœ ì… ì „ëµ í•„ìš”")
    
    except Exception as e:
        interpretations.append(f"ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    return interpretations

def get_network_role(mbti, role_type):
    """ë„¤íŠ¸ì›Œí¬ì—ì„œì˜ MBTI ì—­í•  ì„¤ëª…"""
    if role_type == "hub":
        hub_descriptions = {
            'ENFJ': 'ë‹¤ë¥¸ ìœ í˜•ë“¤ê³¼ ì˜ ì–´ìš¸ë¦¬ëŠ” ì²œì—° ì¤‘ì¬ì',
            'ENFP': 'ì—´ì •ìœ¼ë¡œ ë‹¤ì–‘í•œ ì‚¬ëŒë“¤ì„ ì—°ê²°í•˜ëŠ” ì´‰ë§¤',
            'ESFJ': 'í˜‘ë ¥ì  ì„±í–¥ìœ¼ë¡œ ë„¤íŠ¸ì›Œí¬ ì•ˆì •ì„± ì œê³µ',
            'ESTJ': 'ì²´ê³„ì  ê´€ë¦¬ë¡œ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°í™”',
        }
        return hub_descriptions.get(mbti, 'ë„¤íŠ¸ì›Œí¬ì˜ ì¤‘ì‹¬ì  ì—­í• ')
    
    else:  # rare
        rare_descriptions = {
            'INTJ': 'ë…ë¦½ì  ì„±í–¥ìœ¼ë¡œ ì„ íƒì  ìƒí˜¸ì‘ìš©',
            'INTP': 'ë¶„ì„ì  íŠ¹ì„±ìœ¼ë¡œ ì‹ ì¤‘í•œ ì°¸ì—¬',
            'ISTP': 'ì‹¤ìš©ì  ì ‘ê·¼ìœ¼ë¡œ í•„ìš”ì‹œì—ë§Œ ì°¸ì—¬',
            'ISFP': 'ê°œì¸ì  ê°€ì¹˜ ì¤‘ì‹œë¡œ ì¡°ìš©í•œ ì°¸ì—¬',
        }
        return rare_descriptions.get(mbti, 'ë…íŠ¹í•œ ê´€ì ìœ¼ë¡œ ë„¤íŠ¸ì›Œí¬ì— íŠ¹ë³„í•¨ ì œê³µ')

def analyze_mbti_clusters(mbti_counts):
    """MBTI í´ëŸ¬ìŠ¤í„° ë¶„ì„"""
    clusters = []
    
    try:
        # ê¸°ì§ˆë³„ ê·¸ë£¹í•‘ (Keirsey Temperament)
        nt_types = [mbti for mbti in mbti_counts.index if 'NT' in mbti or (mbti[1] == 'N' and mbti[2] == 'T')]
        nf_types = [mbti for mbti in mbti_counts.index if 'NF' in mbti or (mbti[1] == 'N' and mbti[2] == 'F')]
        st_types = [mbti for mbti in mbti_counts.index if 'ST' in mbti or (mbti[1] == 'S' and mbti[2] == 'T')]
        sf_types = [mbti for mbti in mbti_counts.index if 'SF' in mbti or (mbti[1] == 'S' and mbti[2] == 'F')]
        
        if len(nt_types) >= 2:
            nt_total = sum(mbti_counts[mbti] for mbti in nt_types)
            clusters.append(f"  - **NT ê·¸ë£¹** ({len(nt_types)}ê°œ ìœ í˜•, {nt_total}ê±´): ë…¼ë¦¬ì Â·ì²´ê³„ì  ë¡œë´‡ í™œìš©")
        
        if len(nf_types) >= 2:
            nf_total = sum(mbti_counts[mbti] for mbti in nf_types)
            clusters.append(f"  - **NF ê·¸ë£¹** ({len(nf_types)}ê°œ ìœ í˜•, {nf_total}ê±´): ì°½ì˜ì Â·ê°ì •ì  ë¡œë´‡ ìƒí˜¸ì‘ìš©")
        
        if len(st_types) >= 2:
            st_total = sum(mbti_counts[mbti] for mbti in st_types)
            clusters.append(f"  - **ST ê·¸ë£¹** ({len(st_types)}ê°œ ìœ í˜•, {st_total}ê±´): ì‹¤ìš©ì Â·íš¨ìœ¨ì  ë¡œë´‡ ì‚¬ìš©")
        
        if len(sf_types) >= 2:
            sf_total = sum(mbti_counts[mbti] for mbti in sf_types)
            clusters.append(f"  - **SF ê·¸ë£¹** ({len(sf_types)}ê°œ ìœ í˜•, {sf_total}ê±´): í˜‘ë ¥ì Â·ë°°ë ¤ì  ë¡œë´‡ í™œìš©")
    
    except Exception as e:
        clusters.append(f"í´ëŸ¬ìŠ¤í„° ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
    
    return clusters

def analyze_mbti_changes(df):
    """MBTI ë³€í™” íŒ¨í„´ ìë™ ë¶„ì„ ë° í•´ì„"""
    interpretations = []
    
    try:
        if len(df) < 2:
            interpretations.append("MBTI ë³€í™” ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ìµœì†Œ 2ê°œì˜ ì§„ë‹¨ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return interpretations
        
        # ì‹œê°„ìˆœ ì •ë ¬
        df_sorted = df.sort_values('timestamp')
        changes = []
        
        # ë³€í™” íŒ¨í„´ ì¶”ì¶œ
        for i in range(1, len(df_sorted)):
            prev_mbti = df_sorted.iloc[i-1]['mbti']
            curr_mbti = df_sorted.iloc[i]['mbti']
            prev_time = pd.to_datetime(df_sorted.iloc[i-1]['timestamp'])
            curr_time = pd.to_datetime(df_sorted.iloc[i]['timestamp'])
            
            if prev_mbti != curr_mbti:
                time_diff = (curr_time - prev_time).days
                changes.append({
                    'from': prev_mbti,
                    'to': curr_mbti,
                    'days_between': time_diff,
                    'change_type': get_change_type(prev_mbti, curr_mbti)
                })
        
        if not changes:
            interpretations.append("**ğŸ”„ MBTI ë³€í™” ë¶„ì„:**")
            interpretations.append("â€¢ **ì•ˆì •ì  íŒ¨í„´**: ëª¨ë“  ì§„ë‹¨ì—ì„œ ë™ì¼í•œ MBTI ìœ í˜• ìœ ì§€")
            interpretations.append("â€¢ **ì¼ê´€ì„±**: ë¡œë´‡ ìƒí˜¸ì‘ìš© ì„ í˜¸ë„ê°€ ì¼ê´€ë˜ê²Œ ìœ ì§€ë¨")
            interpretations.append("â€¢ **ì‹ ë¢°ì„±**: ì§„ë‹¨ ê²°ê³¼ì˜ ë†’ì€ ì‹ ë¢°ì„±ì„ ë³´ì—¬ì¤Œ")
            return interpretations
        
        # ë³€í™” ë¶„ì„
        interpretations.append("**ğŸ”„ MBTI ë³€í™” ë¶„ì„:**")
        interpretations.append(f"â€¢ **ì´ ë³€í™” íšŸìˆ˜**: {len(changes)}ë²ˆ")
        
        # ë³€í™” ìœ í˜• ë¶„ì„
        change_types = {}
        for change in changes:
            change_type = change['change_type']
            if change_type not in change_types:
                change_types[change_type] = []
            change_types[change_type].append(change)
        
        # ê°€ì¥ ë§ì€ ë³€í™” ìœ í˜•
        if change_types:
            most_common_type = max(change_types.keys(), key=lambda x: len(change_types[x]))
            interpretations.append(f"â€¢ **ì£¼ìš” ë³€í™” íŒ¨í„´**: {most_common_type} ({len(change_types[most_common_type])}íšŒ)")
        
        # ë³€í™” ê°„ê²© ë¶„ì„
        time_intervals = [change['days_between'] for change in changes]
        avg_interval = np.mean(time_intervals)
        
        if avg_interval < 7:
            interpretations.append(f"â€¢ **ë³€í™” ì£¼ê¸°**: í‰ê·  {avg_interval:.1f}ì¼ â†’ ë¹ ë¥¸ ì ì‘ ë° íƒìƒ‰ ì„±í–¥")
        elif avg_interval < 30:
            interpretations.append(f"â€¢ **ë³€í™” ì£¼ê¸°**: í‰ê·  {avg_interval:.1f}ì¼ â†’ ì ì§„ì  ì„ í˜¸ë„ ë³€í™”")
        else:
            interpretations.append(f"â€¢ **ë³€í™” ì£¼ê¸°**: í‰ê·  {avg_interval:.1f}ì¼ â†’ ì‹ ì¤‘í•œ ë³€í™” íŒ¨í„´")
        
        # ë³€í™”ì˜ ì˜ë¯¸ í•´ì„
        interpretations.append("**ğŸ“Š ë³€í™”ê°€ ì˜ë¯¸í•˜ëŠ” ê²ƒ:**")
        
        if len(changes) > len(df) * 0.5:  # ë³€í™”ê°€ ë§ì€ ê²½ìš°
            interpretations.append("â€¢ **íƒìƒ‰ì  ì„±í–¥**: ë‹¤ì–‘í•œ ë¡œë´‡ ìƒí˜¸ì‘ìš© ë°©ì‹ì„ ì ê·¹ì ìœ¼ë¡œ íƒìƒ‰")
            interpretations.append("â€¢ **ì ì‘ë ¥**: ìƒí™©ì— ë”°ë¼ ìœ ì—°í•˜ê²Œ ìƒí˜¸ì‘ìš© ìŠ¤íƒ€ì¼ ì¡°ì •")
            interpretations.append("â€¢ **ì„±ì¥**: ë¡œë´‡ ì‚¬ìš© ê²½í—˜ì„ í†µí•œ ì„ í˜¸ë„ ë°œì „")
        else:
            interpretations.append("â€¢ **ì„ íƒì  ë³€í™”**: íŠ¹ì • ìƒí™©ì—ì„œë§Œ ìƒí˜¸ì‘ìš© ë°©ì‹ ë³€ê²½")
            interpretations.append("â€¢ **ì•ˆì •ì„±**: ê¸°ë³¸ì ì¸ ì„ í˜¸ë„ëŠ” ìœ ì§€í•˜ë©´ì„œ ë¶€ë¶„ì  ì¡°ì •")
            interpretations.append("â€¢ **í•™ìŠµ**: ê²½í—˜ì„ í†µí•œ ì ì§„ì  ìµœì í™”")
        
        # ì¶•ë³„ ë³€í™” ë¶„ì„
        axis_changes = analyze_axis_changes(changes)
        if axis_changes:
            interpretations.append("**ğŸ¯ ì¶•ë³„ ë³€í™” íŒ¨í„´:**")
            interpretations.extend(axis_changes)
    
    except Exception as e:
        interpretations.append(f"MBTI ë³€í™” ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    return interpretations

def get_change_type(from_mbti, to_mbti):
    """MBTI ë³€í™” ìœ í˜• ë¶„ë¥˜"""
    differences = []
    
    if from_mbti[0] != to_mbti[0]:  # E/I ë³€í™”
        differences.append("ì—ë„ˆì§€ ë°©í–¥")
    if from_mbti[1] != to_mbti[1]:  # S/N ë³€í™”
        differences.append("ì •ë³´ ì²˜ë¦¬")
    if from_mbti[2] != to_mbti[2]:  # T/F ë³€í™”
        differences.append("ì˜ì‚¬ê²°ì •")
    if from_mbti[3] != to_mbti[3]:  # J/P ë³€í™”
        differences.append("ìƒí™œ ì–‘ì‹")
    
    if len(differences) == 1:
        return f"{differences[0]} ë³€í™”"
    elif len(differences) == 2:
        return f"{', '.join(differences)} ë³€í™”"
    elif len(differences) == 3:
        return "ëŒ€í­ ë³€í™”"
    else:
        return "ì™„ì „ ë³€í™”"

def analyze_axis_changes(changes):
    """ì¶•ë³„ ë³€í™” íŒ¨í„´ ë¶„ì„"""
    analyses = []
    
    try:
        # ê° ì¶•ë³„ ë³€í™” íšŸìˆ˜ ê³„ì‚°
        axis_counts = {'E/I': 0, 'S/N': 0, 'T/F': 0, 'J/P': 0}
        
        for change in changes:
            from_mbti = change['from']
            to_mbti = change['to']
            
            if from_mbti[0] != to_mbti[0]:
                axis_counts['E/I'] += 1
            if from_mbti[1] != to_mbti[1]:
                axis_counts['S/N'] += 1
            if from_mbti[2] != to_mbti[2]:
                axis_counts['T/F'] += 1
            if from_mbti[3] != to_mbti[3]:
                axis_counts['J/P'] += 1
        
        # ê°€ì¥ ë§ì´ ë³€í™”í•œ ì¶•
        most_changed_axis = max(axis_counts.keys(), key=lambda x: axis_counts[x])
        most_changed_count = axis_counts[most_changed_axis]
        
        if most_changed_count > 0:
            axis_meanings = {
                'E/I': 'ì™¸í–¥ì„±/ë‚´í–¥ì„± - ë¡œë´‡ê³¼ì˜ ìƒí˜¸ì‘ìš© ë°©ì‹',
                'S/N': 'ê°ê°/ì§ê´€ - ì •ë³´ ì²˜ë¦¬ ë° í•™ìŠµ ë°©ì‹',
                'T/F': 'ì‚¬ê³ /ê°ì • - ë¡œë´‡ì— ëŒ€í•œ í‰ê°€ ê¸°ì¤€',
                'J/P': 'íŒë‹¨/ì¸ì‹ - ë¡œë´‡ í™œìš© ê³„íšì„±'
            }
            
            analyses.append(f"â€¢ **{most_changed_axis} ì¶•** ({most_changed_count}íšŒ): {axis_meanings[most_changed_axis]} ë³€í™”ê°€ ê°€ì¥ í™œë°œ")
            
            # ë³€í™”ê°€ ì—†ëŠ” ì¶•ë„ ì–¸ê¸‰
            stable_axes = [axis for axis, count in axis_counts.items() if count == 0]
            if stable_axes:
                analyses.append(f"â€¢ **ì•ˆì •ì  ì¶•**: {', '.join(stable_axes)} - ì¼ê´€ëœ ì„ í˜¸ë„ ìœ ì§€")
    
    except Exception as e:
        analyses.append(f"ì¶•ë³„ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
    
    return analyses

def analyze_statistical_significance(df, group_col):
    """í†µê³„ì  ìœ ì˜ì„± ë¶„ì„ ë° í•´ì„"""
    interpretations = []
    
    try:
        if len(df) < 10:  # ìµœì†Œ ìƒ˜í”Œ í¬ê¸°
            interpretations.append("í†µê³„ì  ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ìµœì†Œ 10ê°œì˜ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return interpretations
        
        # ê·¸ë£¹ë³„ MBTI ë¶„í¬ ë¶„ì„
        contingency_table = pd.crosstab(df[group_col], df['mbti'])
        
        if contingency_table.shape[0] < 2 or contingency_table.shape[1] < 2:
            interpretations.append("í†µê³„ì  ê²€ì •ì„ ìœ„í•´ì„œëŠ” ìµœì†Œ 2ê°œ ê·¸ë£¹ê³¼ 2ê°œ MBTI ìœ í˜•ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            return interpretations
        
        # ì¹´ì´ì œê³± ê²€ì •
        try:
            chi2, p_value, dof, expected = chi2_contingency(contingency_table)
            
            interpretations.append("**ğŸ“Š í†µê³„ì  ìœ ì˜ì„± ë¶„ì„:**")
            interpretations.append(f"â€¢ **ì¹´ì´ì œê³± í†µê³„ëŸ‰**: {chi2:.3f}")
            interpretations.append(f"â€¢ **p-ê°’**: {p_value:.3f}")
            
            if p_value < 0.001:
                interpretations.append("â€¢ **ê²°ê³¼**: ë§¤ìš° ê°•í•œ í†µê³„ì  ìœ ì˜ì„± (p < 0.001)")
                interpretations.append("â€¢ **í•´ì„**: ê·¸ë£¹ ê°„ MBTI ë¶„í¬ ì°¨ì´ê°€ ë§¤ìš° ëª…í™•í•¨")
            elif p_value < 0.01:
                interpretations.append("â€¢ **ê²°ê³¼**: ê°•í•œ í†µê³„ì  ìœ ì˜ì„± (p < 0.01)")
                interpretations.append("â€¢ **í•´ì„**: ê·¸ë£¹ ê°„ MBTI ë¶„í¬ì— ëª…í™•í•œ ì°¨ì´ ì¡´ì¬")
            elif p_value < 0.05:
                interpretations.append("â€¢ **ê²°ê³¼**: í†µê³„ì  ìœ ì˜ì„± ìˆìŒ (p < 0.05)")
                interpretations.append("â€¢ **í•´ì„**: ê·¸ë£¹ ê°„ MBTI ë¶„í¬ ì°¨ì´ê°€ í†µê³„ì ìœ¼ë¡œ ì˜ë¯¸ ìˆìŒ")
            else:
                interpretations.append("â€¢ **ê²°ê³¼**: í†µê³„ì  ìœ ì˜ì„± ì—†ìŒ (p â‰¥ 0.05)")
                interpretations.append("â€¢ **í•´ì„**: ê·¸ë£¹ ê°„ MBTI ë¶„í¬ ì°¨ì´ê°€ ìš°ì—°ì— ì˜í•œ ê²ƒì¼ ê°€ëŠ¥ì„±")
            
            # íš¨ê³¼ í¬ê¸° ë¶„ì„ (CramÃ©r's V)
            n = contingency_table.sum().sum()
            cramers_v = np.sqrt(chi2 / (n * (min(contingency_table.shape) - 1)))
            
            interpretations.append(f"â€¢ **íš¨ê³¼ í¬ê¸° (CramÃ©r's V)**: {cramers_v:.3f}")
            
            if cramers_v < 0.1:
                interpretations.append("â€¢ **íš¨ê³¼ í¬ê¸°**: ì‘ìŒ - ê·¸ë£¹ ê°„ ì°¨ì´ê°€ ë¯¸ë¯¸í•¨")
            elif cramers_v < 0.3:
                interpretations.append("â€¢ **íš¨ê³¼ í¬ê¸°**: ì¤‘ê°„ - ê·¸ë£¹ ê°„ ì°¨ì´ê°€ ì ë‹¹í•¨")
            elif cramers_v < 0.5:
                interpretations.append("â€¢ **íš¨ê³¼ í¬ê¸°**: í¼ - ê·¸ë£¹ ê°„ ì°¨ì´ê°€ ìƒë‹¹í•¨")
            else:
                interpretations.append("â€¢ **íš¨ê³¼ í¬ê¸°**: ë§¤ìš° í¼ - ê·¸ë£¹ ê°„ ì°¨ì´ê°€ ë§¤ìš° í¼")
        
        except Exception as e:
            interpretations.append(f"í†µê³„ ê²€ì • ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    except Exception as e:
        interpretations.append(f"í†µê³„ì  ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    return interpretations

def analyze_diversity_index(df):
    """ë‹¤ì–‘ì„± ì§€ìˆ˜ ë¶„ì„ ë° í•´ì„"""
    interpretations = []
    
    try:
        mbti_counts = df['mbti'].value_counts()
        total = len(df)
        
        # Shannon ë‹¤ì–‘ì„± ì§€ìˆ˜ ê³„ì‚°
        shannon_index = -sum((count/total) * np.log(count/total) for count in mbti_counts)
        max_shannon = np.log(16)  # 16ê°œ MBTI ìœ í˜•ì˜ ìµœëŒ€ ë‹¤ì–‘ì„±
        shannon_evenness = shannon_index / max_shannon
        
        # Simpson ë‹¤ì–‘ì„± ì§€ìˆ˜ ê³„ì‚°
        simpson_index = 1 - sum((count/total)**2 for count in mbti_counts)
        
        interpretations.append("**ğŸŒˆ ë‹¤ì–‘ì„± ë¶„ì„:**")
        interpretations.append(f"â€¢ **Shannon ë‹¤ì–‘ì„± ì§€ìˆ˜**: {shannon_index:.3f} (ìµœëŒ€: {max_shannon:.3f})")
        interpretations.append(f"â€¢ **ê· ë“±ì„± ì§€ìˆ˜**: {shannon_evenness:.3f}")
        interpretations.append(f"â€¢ **Simpson ë‹¤ì–‘ì„± ì§€ìˆ˜**: {simpson_index:.3f}")
        
        # í•´ì„
        if shannon_evenness > 0.8:
            interpretations.append("â€¢ **ë‹¤ì–‘ì„± ìˆ˜ì¤€**: ë§¤ìš° ë†’ìŒ - ëª¨ë“  MBTI ìœ í˜•ì´ ê³ ë¥´ê²Œ ë¶„í¬")
            interpretations.append("â€¢ **ì˜ë¯¸**: ë¡œë´‡ ìƒí˜¸ì‘ìš©ì— ëŒ€í•œ ë‹¤ì–‘í•œ ê´€ì ê³¼ ìš”êµ¬ì‚¬í•­ ì¡´ì¬")
        elif shannon_evenness > 0.6:
            interpretations.append("â€¢ **ë‹¤ì–‘ì„± ìˆ˜ì¤€**: ë†’ìŒ - ëŒ€ë¶€ë¶„ì˜ MBTI ìœ í˜•ì´ ì ì ˆíˆ ë¶„í¬")
            interpretations.append("â€¢ **ì˜ë¯¸**: ê· í˜• ì¡íŒ ì‚¬ìš©ì êµ¬ì„±ìœ¼ë¡œ í¬ê´„ì  ì„œë¹„ìŠ¤ ì„¤ê³„ ê°€ëŠ¥")
        elif shannon_evenness > 0.4:
            interpretations.append("â€¢ **ë‹¤ì–‘ì„± ìˆ˜ì¤€**: ì¤‘ê°„ - ì¼ë¶€ MBTI ìœ í˜•ì— ì§‘ì¤‘")
            interpretations.append("â€¢ **ì˜ë¯¸**: ì£¼ìš” ì‚¬ìš©ì ê·¸ë£¹ ì¤‘ì‹¬ì˜ ë§ì¶¤í˜• ì„œë¹„ìŠ¤ í•„ìš”")
        else:
            interpretations.append("â€¢ **ë‹¤ì–‘ì„± ìˆ˜ì¤€**: ë‚®ìŒ - íŠ¹ì • MBTI ìœ í˜•ì— í¸ì¤‘")
            interpretations.append("â€¢ **ì˜ë¯¸**: íŠ¹í™”ëœ ì„œë¹„ìŠ¤ ì œê³µ ë˜ëŠ” ë‹¤ì–‘ì„± í™•ëŒ€ ì „ëµ í•„ìš”")
        
        # ê°€ì¥ í¬ì†Œí•œ ìœ í˜•ê³¼ ê°€ì¥ ë§ì€ ìœ í˜•
        most_common = mbti_counts.index[0]
        least_common = mbti_counts.index[-1]
        dominance = mbti_counts.iloc[0] / total
        
        interpretations.append(f"â€¢ **ì§€ë°°ì  ìœ í˜•**: {most_common} ({dominance:.1%})")
        interpretations.append(f"â€¢ **í¬ì†Œ ìœ í˜•**: {least_common} ({mbti_counts.iloc[-1]/total:.1%})")
        
    except Exception as e:
        interpretations.append(f"ë‹¤ì–‘ì„± ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    return interpretations

# MBTI ê°€ì´ë“œ ë°ì´í„°
def load_guide_data(location="ì¼ë°˜"):
    """ì¥ì†Œë³„ íŠ¹í™”ëœ MBTI ê°€ì´ë“œ ë°ì´í„° ë¡œë“œ"""
    base_guide = {
        "ENFJ": {
            "description": "ë¦¬ë”ì‹­ê³¼ ê³µê° ëŠ¥ë ¥ì„ ê²¸ë¹„í•œ íƒ€ì…ì…ë‹ˆë‹¤. ë‹¤ë¥¸ ì‚¬ëŒì˜ ì„±ì¥ì„ ë•ê³ , íŒ€ì˜ ì¡°í™”ë¥¼ ì¤‘ì‹œí•©ë‹ˆë‹¤.",
            "hri_style": "ê³µê°ì  ë¦¬ë”ì‹­, ê²©ë ¤ì™€ ì„±ì¥ ì§€í–¥, íŒ€ì›Œí¬ ì¤‘ì‹œ",
            "examples": [
                "ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ ì–´ë–¤ ë„ì›€ì´ í•„ìš”í•˜ì‹ ì§€ í¸í•˜ê²Œ ë§ì”€í•´ ì£¼ì„¸ìš”.",
                "í•¨ê»˜ ì´ ë¬¸ì œë¥¼ í•´ê²°í•´ë³´ëŠ” ê±´ ì–´ë–¨ê¹Œìš”? ë‹¹ì‹ ì˜ ìƒê°ì´ ê¶ê¸ˆí•´ìš”.",
                "ì§„í–‰ ìƒí™©ì„ í™•ì¸í•´ë³´ë‹ˆ ì •ë§ ì˜ í•˜ê³  ê³„ì‹œë„¤ìš”! ë” ë°œì „í•  ìˆ˜ ìˆëŠ” ë¶€ë¶„ë„ ì œì•ˆë“œë¦´ê²Œìš”.",
                "í˜¹ì‹œ ì–´ë ¤ìš´ ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ ì£¼ì„¸ìš”. í•¨ê»˜ ì°¾ì•„ë³´ê² ìŠµë‹ˆë‹¤.",
                "íŒ€ ì „ì²´ê°€ ì„±ê³µí•  ìˆ˜ ìˆë„ë¡ ì œê°€ ì ê·¹ì ìœ¼ë¡œ ì§€ì›í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
            ]
        },
        "ENTJ": {
            "description": "ì „ëµì  ì‚¬ê³ ì™€ íš¨ìœ¨ì„±ì„ ì¤‘ì‹œí•˜ëŠ” íƒ€ì…ì…ë‹ˆë‹¤. ëª…í™•í•œ ëª©í‘œ ì„¤ì •ê³¼ ì²´ê³„ì ì¸ ì ‘ê·¼ì„ ì„ í˜¸í•©ë‹ˆë‹¤.",
            "hri_style": "ì „ëµì  ì‚¬ê³ , íš¨ìœ¨ì„± ì¤‘ì‹œ, ëª©í‘œ ì§€í–¥ì ",
            "examples": [
                "ëª©í‘œ ë‹¬ì„±ì„ ìœ„í•´ ì²´ê³„ì ìœ¼ë¡œ ì•ˆë‚´í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ë‹¨ê³„ë³„ë¡œ ì§„í–‰í•˜ì‹œì£ .",
                "í˜„ì¬ ìƒí™©ì„ ë¶„ì„í•œ ê²°ê³¼, ì´ ë°©ë²•ì´ ê°€ì¥ íš¨ìœ¨ì ì¼ ê²ƒ ê°™ìŠµë‹ˆë‹¤.",
                "ì‹œê°„ì„ ì ˆì•½í•˜ê¸° ìœ„í•´ í•µì‹¬ë§Œ ìš”ì•½í•´ì„œ ì•Œë ¤ë“œë¦´ê²Œìš”.",
                "ë‹¤ìŒ ë‹¨ê³„ë¡œ ë„˜ì–´ê°€ê¸° ì „ì— í˜„ì¬ ì§„í–‰ìƒí™©ì„ í™•ì¸í•´ë³´ì‹œê² ì–´ìš”?",
                "ìµœì ì˜ ê²°ê³¼ë¥¼ ìœ„í•´ ìš°ì„ ìˆœìœ„ë¥¼ ì •í•´ì„œ ì§„í–‰í•˜ëŠ” ê²ƒì´ ì¢‹ê² ìŠµë‹ˆë‹¤."
            ]
        },
        "ENTP": {
            "description": "ì°½ì˜ì ì´ê³  í˜ì‹ ì ì¸ ì‚¬ê³ ë¥¼ ê°€ì§„ íƒ€ì…ì…ë‹ˆë‹¤. ìƒˆë¡œìš´ ì•„ì´ë””ì–´ì™€ ë„ì „ì„ ì¦ê¹ë‹ˆë‹¤.",
            "hri_style": "í˜ì‹ ì  ì ‘ê·¼, ì°½ì˜ì  í•´ê²°ì±…, ë„ì „ ì§€í–¥ì ",
            "examples": [
                "ìƒˆë¡œìš´ ê´€ì ì—ì„œ ì´ ë¬¸ì œë¥¼ ë°”ë¼ë³´ëŠ” ê±´ ì–´ë–¨ê¹Œìš”?",
                "ê¸°ì¡´ ë°©ì‹ì„ ê°œì„ í•  ìˆ˜ ìˆëŠ” ì°½ì˜ì ì¸ ë°©ë²•ì„ ì œì•ˆí•´ ë“œë¦´ê²Œìš”.",
                "í¥ë¯¸ë¡œìš´ ìƒˆë¡œìš´ ì ‘ê·¼ë²•ì„ ì œì•ˆí•´ë“œë¦´ê²Œìš”. ì–´ë–»ê²Œ ìƒê°í•˜ì„¸ìš”?",
                "ì—¬ëŸ¬ ê°€ì§€ ì˜µì…˜ì´ ìˆëŠ”ë°, ì–´ë–¤ ê²ƒì´ ê°€ì¥ í¥ë¯¸ë¡œìš°ì‹ ê°€ìš”?",
                "í•¨ê»˜ ì‹¤í—˜í•´ë³´ë©´ì„œ ìƒˆë¡œìš´ í•´ê²°ì±…ì„ ì°¾ì•„ë³´ëŠ” ê±´ ì–´ë–¨ê¹Œìš”?"
            ]
        },
        "ENFP": {
            "description": "ì—´ì •ì ì´ê³  ì°½ì˜ì ì¸ íƒ€ì…ì…ë‹ˆë‹¤. ê°€ëŠ¥ì„±ê³¼ ìƒˆë¡œìš´ ê²½í—˜ì„ ì¶”êµ¬í•©ë‹ˆë‹¤.",
            "hri_style": "ì—´ì •ì  ì†Œí†µ, ì°½ì˜ì  ì˜ê°, ê°€ëŠ¥ì„± ì¶”êµ¬",
            "examples": [
                "ì •ë§ í¥ë¯¸ë¡œìš´ ì•„ì´ë””ì–´ë„¤ìš”! ë” ìì„¸íˆ ë“¤ì–´ë³´ê³  ì‹¶ì–´ìš”.",
                "ìƒˆë¡œìš´ ê°€ëŠ¥ì„±ì„ í•¨ê»˜ íƒìƒ‰í•´ë³´ëŠ” ê±´ ì–´ë–¨ê¹Œìš”?",
                "ë‹¹ì‹ ì˜ ì•„ì´ë””ì–´ê°€ ì •ë§ í¥ë¯¸ë¡­ë„¤ìš”! í•¨ê»˜ ë°œì „ì‹œì¼œë³´ëŠ” ê±´ ì–´ë–¨ê¹Œìš”?",
                "ì •ë§ ì˜í•˜ê³  ê³„ì‹œë„¤ìš”! ë” ë©‹ì§„ ê²°ê³¼ë¥¼ ë§Œë“¤ì–´ë³´ì‹œì£ .",
                "í•¨ê»˜ ì¦ê²ê²Œ ë°°ì›Œê°€ë©´ì„œ ìƒˆë¡œìš´ ê°€ëŠ¥ì„±ì„ ë°œê²¬í•´ë³´ì•„ìš”."
            ]
        },
        "ESFJ": {
            "description": "í˜‘ë ¥ì ì´ê³  ì‹¤ìš©ì ì¸ íƒ€ì…ì…ë‹ˆë‹¤. ë‹¤ë¥¸ ì‚¬ëŒì˜ í•„ìš”ë¥¼ ëŒë³´ê³  ì¡°í™”ë¥¼ ì¶”êµ¬í•©ë‹ˆë‹¤.",
            "hri_style": "í˜‘ë ¥ì  ì§€ì›, ì‹¤ìš©ì  ë„ì›€, ì¡°í™” ì¤‘ì‹œ",
            "examples": [
                "ì–´ë–¤ ë„ì›€ì´ í•„ìš”í•˜ì‹ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”. í•¨ê»˜ í•´ê²°í•´ë³´ê² ìŠµë‹ˆë‹¤.",
                "ëª¨ë‘ê°€ í¸ì•ˆí•˜ê²Œ ì´ìš©í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ë“œë¦´ê²Œìš”.",
                "ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“  í¸í•˜ê²Œ ë§ì”€í•´ ì£¼ì„¸ìš”. í•¨ê»˜ í•´ê²°í•´ë³´ê² ìŠµë‹ˆë‹¤.",
                "ê¶ê¸ˆí•œ ì ì´ë‚˜ ì–´ë ¤ìš´ ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë„ì™€ë“œë¦´ê²Œìš”.",
                "í•¨ê»˜ í˜‘ë ¥í•´ì„œ ì¢‹ì€ ê²°ê³¼ë¥¼ ë§Œë“¤ì–´ë³´ì‹œì£ ."
            ]
        },
        "ESFP": {
            "description": "ì¦‰í¥ì ì´ê³  ì¹œê·¼í•œ íƒ€ì…ì…ë‹ˆë‹¤. í˜„ì¬ì˜ ì¦ê±°ì›€ê³¼ ì‹¤ìš©ì  í•´ê²°ì±…ì„ ì¤‘ì‹œí•©ë‹ˆë‹¤.",
            "hri_style": "ì¦‰í¥ì  ìƒí˜¸ì‘ìš©, ì¹œê·¼í•œ ì†Œí†µ, ì‹¤ìš©ì  í•´ê²°",
            "examples": [
                "ì§€ê¸ˆ ë‹¹ì¥ ë„ì›€ì´ í•„ìš”í•˜ì‹œêµ°ìš”! ë°”ë¡œ í•´ê²°í•´ë“œë¦´ê²Œìš”.",
                "í¸í•˜ê²Œ ë§ì”€í•´ ì£¼ì„¸ìš”. í•¨ê»˜ ì¦ê²ê²Œ í•´ê²°í•´ë³´ì£ .",
                "ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì–´ë– ì„¸ìš”? ì¦ê±°ìš´ í•˜ë£¨ê°€ ë˜ë„ë¡ ë„ì™€ë“œë¦´ê²Œìš”!",
                "ì‹¤ìš©ì ì´ë©´ì„œë„ ì¬ë¯¸ìˆëŠ” ë°©ë²•ìœ¼ë¡œ ì§„í–‰í•´ë³´ëŠ” ê±´ ì–´ë–¨ê¹Œìš”?",
                "ì§€ê¸ˆ ì´ ìˆœê°„ì„ ìµœëŒ€í•œ í™œìš©í•´ì„œ ë©‹ì§„ ê²°ê³¼ë¥¼ ë§Œë“¤ì–´ë³´ì‹œì£ !"
            ]
        },
        "ESTJ": {
            "description": "ì²´ê³„ì ì´ê³  ì±…ì„ê° ìˆëŠ” íƒ€ì…ì…ë‹ˆë‹¤. ê·œì¹™ê³¼ íš¨ìœ¨ì„±ì„ ì¤‘ì‹œí•©ë‹ˆë‹¤.",
            "hri_style": "ì²´ê³„ì  ê´€ë¦¬, ì±…ì„ê° ìˆëŠ” ì•ˆë‚´, íš¨ìœ¨ì„± ì¤‘ì‹œ",
            "examples": [
                "ê·œì •ì— ë”°ë¼ ì²´ê³„ì ìœ¼ë¡œ ì•ˆë‚´í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
                "íš¨ìœ¨ì ìœ¼ë¡œ ì§„í–‰í•  ìˆ˜ ìˆë„ë¡ ë‹¨ê³„ë³„ë¡œ ë„ì™€ë“œë¦´ê²Œìš”.",
                "ì •í•´ì§„ ì ˆì°¨ë¥¼ ì¤€ìˆ˜í•˜ë©´ì„œ ìµœìƒì˜ ê²°ê³¼ë¥¼ ë³´ì¥í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
                "ì±…ì„ê°ì„ ê°€ì§€ê³  ì •í™•í•˜ê²Œ ì²˜ë¦¬í•´ë“œë¦´ í…Œë‹ˆ ì•ˆì‹¬í•˜ì„¸ìš”.",
                "ì²´ê³„ì ì¸ ê´€ë¦¬ë¥¼ í†µí•´ ëª¨ë“  ê²ƒì´ ì›í™œí•˜ê²Œ ì§„í–‰ë˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤."
            ]
        },
        "ESTP": {
            "description": "ì‹¤ìš©ì ì´ê³  ì ì‘ë ¥ì´ ë›°ì–´ë‚œ íƒ€ì…ì…ë‹ˆë‹¤. í˜„ì¬ ìƒí™©ì— ë§ëŠ” í•´ê²°ì±…ì„ ì°¾ìŠµë‹ˆë‹¤.",
            "hri_style": "ì‹¤ìš©ì  í•´ê²°, ì ì‘ì  ëŒ€ì‘, ì¦‰ì‹œ ì‹¤í–‰",
            "examples": [
                "í˜„ì¬ ìƒí™©ì— ë§ëŠ” ì‹¤ìš©ì ì¸ í•´ê²°ì±…ì„ ì œì•ˆí•´ ë“œë¦´ê²Œìš”.",
                "ë°”ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì•Œë ¤ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
                "ìƒí™©ì´ ë³€í•˜ë©´ ì¦‰ì‹œ ì ì‘í•´ì„œ ìƒˆë¡œìš´ ë°©ë²•ì„ ì°¾ì•„ë³´ê² ìŠµë‹ˆë‹¤.",
                "ì‹¤ì œë¡œ íš¨ê³¼ê°€ ìˆëŠ” ë°©ë²•ë“¤ë§Œ ê³¨ë¼ì„œ ì œì•ˆí•´ë“œë¦´ê²Œìš”.",
                "ì§€ê¸ˆ ë‹¹ì¥ í•„ìš”í•œ ê²ƒë¶€í„° í•´ê²°í•˜ê³  ë‚˜ë¨¸ì§€ëŠ” ì°¨ê·¼ì°¨ê·¼ ì§„í–‰í•´ë³´ì£ ."
            ]
        },
        "INFJ": {
            "description": "ì§ê´€ì ì´ê³  ì´ìƒì£¼ì˜ì ì¸ íƒ€ì…ì…ë‹ˆë‹¤. ê¹Šì€ í†µì°°ë ¥ê³¼ ì°½ì˜ì„±ì„ ê°€ì§‘ë‹ˆë‹¤.",
            "hri_style": "ì§ê´€ì  ì´í•´, ê¹Šì€ í†µì°°, ì°½ì˜ì  ì ‘ê·¼",
            "examples": [
                "ë” ê¹Šì´ ìˆëŠ” ì´í•´ë¥¼ ìœ„í•´ í•¨ê»˜ íƒìƒ‰í•´ë³´ëŠ” ê±´ ì–´ë–¨ê¹Œìš”?",
                "ë³¸ì§ˆì ì¸ ë¬¸ì œë¥¼ ì°¾ì•„ í•´ê²°í•´ë³´ê² ìŠµë‹ˆë‹¤.",
                "ë‹¹ì‹ ì˜ ë‚´ë©´ì˜ ëª©ì†Œë¦¬ì— ê·€ ê¸°ìš¸ì—¬ë³´ì„¸ìš”. ì œê°€ í•¨ê»˜ ë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤.",
                "ì§ê´€ì ìœ¼ë¡œ ëŠë¼ì‹œëŠ” ë¶€ë¶„ì´ ìˆë‹¤ë©´ ê·¸ê²ƒë„ ì¤‘ìš”í•œ ë‹¨ì„œê°€ ë  ìˆ˜ ìˆì–´ìš”.",
                "ì¥ê¸°ì ì¸ ê´€ì ì—ì„œ ì§„ì •ìœ¼ë¡œ ì˜ë¯¸ ìˆëŠ” í•´ê²°ì±…ì„ ì°¾ì•„ë³´ê² ìŠµë‹ˆë‹¤."
            ]
        },
        "INFP": {
            "description": "ì´ìƒì£¼ì˜ì ì´ê³  ì°½ì˜ì ì¸ íƒ€ì…ì…ë‹ˆë‹¤. ê°œì¸ì˜ ê°€ì¹˜ì™€ ì˜ë¯¸ë¥¼ ì¤‘ì‹œí•©ë‹ˆë‹¤.",
            "hri_style": "ì´ìƒì£¼ì˜ì  ì ‘ê·¼, ì°½ì˜ì  ì˜ê°, ê°œì¸ì  ê°€ì¹˜ ì¤‘ì‹œ",
            "examples": [
                "ë‹¹ì‹ ë§Œì˜ íŠ¹ë³„í•œ ê´€ì ì´ ê¶ê¸ˆí•´ìš”. í•¨ê»˜ ì´ì•¼ê¸°í•´ë³´ì£ .",
                "ì˜ë¯¸ ìˆëŠ” ê²½í—˜ì„ ë§Œë“¤ì–´ë³´ëŠ” ê±´ ì–´ë–¨ê¹Œìš”?",
                "ë‹¹ì‹ ì˜ ê°€ì¹˜ê´€ê³¼ ì¼ì¹˜í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì§„í–‰í•´ë³´ëŠ” ê²ƒì´ ì¤‘ìš”í•  ê²ƒ ê°™ì•„ìš”.",
                "ì°½ì˜ì ì¸ ì•„ì´ë””ì–´ë¥¼ ììœ ë¡­ê²Œ í‘œí˜„í•´ë³´ì„¸ìš”. ì œê°€ í•¨ê»˜ ë°œì „ì‹œì¼œë³´ê² ìŠµë‹ˆë‹¤.",
                "ì§„ì •ì„± ìˆëŠ” í•´ê²°ì±…ì„ ì°¾ê¸° ìœ„í•´ ë‹¹ì‹ ì˜ ë§ˆìŒì† ì´ì•¼ê¸°ë¥¼ ë“¤ë ¤ì£¼ì„¸ìš”."
            ]
        },
        "INTJ": {
            "description": "ì „ëµì ì´ê³  ë…ì°½ì ì¸ ì‚¬ê³ ë¥¼ ê°€ì§„ íƒ€ì…ì…ë‹ˆë‹¤. ì¥ê¸°ì  ë¹„ì „ê³¼ íš¨ìœ¨ì„±ì„ ì¶”êµ¬í•©ë‹ˆë‹¤.",
            "hri_style": "ì „ëµì  ê³„íš, ë…ì°½ì  í•´ê²°ì±…, ì¥ê¸°ì  ë¹„ì „",
            "examples": [
                "ì¥ê¸°ì ì¸ ê´€ì ì—ì„œ ìµœì ì˜ í•´ê²°ì±…ì„ ì œì•ˆí•´ ë“œë¦´ê²Œìš”.",
                "ì „ëµì ìœ¼ë¡œ ì ‘ê·¼í•˜ì—¬ íš¨ìœ¨ì ìœ¼ë¡œ í•´ê²°í•´ë³´ê² ìŠµë‹ˆë‹¤.",
                "ë³µì¡í•œ ì‹œìŠ¤í…œì„ ë¶„ì„í•´ì„œ í•µì‹¬ ê°œì„ ì ì„ ì°¾ì•„ë³´ê² ìŠµë‹ˆë‹¤.",
                "ë…ì°½ì ì¸ ì•„ì´ë””ì–´ë¡œ ê¸°ì¡´ ë°©ì‹ì„ í˜ì‹ í•´ë³´ëŠ” ê±´ ì–´ë–¨ê¹Œìš”?",
                "ë¯¸ë˜ë¥¼ ëŒ€ë¹„í•œ ì²´ê³„ì ì¸ ê³„íšì„ í•¨ê»˜ ì„¸ì›Œë³´ì‹œì£ ."
            ]
        },
        "INTP": {
            "description": "ë…¼ë¦¬ì ì´ê³  ë¶„ì„ì ì¸ íƒ€ì…ì…ë‹ˆë‹¤. ë³µì¡í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ê²ƒì„ ì¦ê¹ë‹ˆë‹¤.",
            "hri_style": "ë…¼ë¦¬ì  ë¶„ì„, ë³µì¡í•œ ë¬¸ì œ í•´ê²°, ì •í™•ì„± ì¤‘ì‹œ",
            "examples": [
                "ë…¼ë¦¬ì ìœ¼ë¡œ ë¶„ì„í•´ë³´ë‹ˆ ì´ëŸ° í•´ê²°ì±…ì´ ê°€ì¥ ì í•©í•  ê²ƒ ê°™ì•„ìš”.",
                "ë³µì¡í•œ ë¬¸ì œë¥¼ ë‹¨ê³„ë³„ë¡œ ë¶„ì„í•´ì„œ í•´ê²°í•´ë³´ê² ìŠµë‹ˆë‹¤.",
                "ë‹¤ì–‘í•œ ê°€ëŠ¥ì„±ì„ íƒêµ¬í•´ë³´ë©´ì„œ ìµœì ì˜ ë‹µì„ ì°¾ì•„ë³´ê² ìŠµë‹ˆë‹¤.",
                "ì´ë¡ ì  ë°°ê²½ì„ ë°”íƒ•ìœ¼ë¡œ ì²´ê³„ì ìœ¼ë¡œ ì ‘ê·¼í•´ë³´ì‹œì£ .",
                "ì •í™•í•œ ë°ì´í„°ì™€ ë…¼ë¦¬ì  ì¶”ë¡ ì„ í†µí•´ ê²€ì¦ëœ í•´ê²°ì±…ì„ ì œì‹œí•˜ê² ìŠµë‹ˆë‹¤."
            ]
        },
        "ISFJ": {
            "description": "ì‹ ì¤‘í•˜ê³  í—Œì‹ ì ì¸ íƒ€ì…ì…ë‹ˆë‹¤. ì‹¤ìš©ì ì´ê³  ì•ˆì •ì ì¸ í•´ê²°ì±…ì„ ì œê³µí•©ë‹ˆë‹¤.",
            "hri_style": "ì‹ ì¤‘í•œ ì§€ì›, ì‹¤ìš©ì  í•´ê²°, ì•ˆì •ì  ì„œë¹„ìŠ¤",
            "examples": [
                "ì‹ ì¤‘í•˜ê²Œ ê²€í† í•œ í›„ ì•ˆì „í•˜ê³  ì‹¤ìš©ì ì¸ ë°©ë²•ì„ ì œì•ˆí•´ ë“œë¦´ê²Œìš”.",
                "ì•ˆì •ì ìœ¼ë¡œ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆë„ë¡ ì²´ê³„ì ìœ¼ë¡œ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.",
                "ë‹¹ì‹ ì˜ í¸ì•ˆí•¨ì„ ìµœìš°ì„ ìœ¼ë¡œ ìƒê°í•˜ë©° ì„¸ì‹¬í•˜ê²Œ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
                "ê²€ì¦ëœ ë°©ë²•ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ì§„í–‰í•˜ì—¬ ê±±ì • ì—†ì´ ì´ìš©í•˜ì‹¤ ìˆ˜ ìˆë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.",
                "í•„ìš”í•˜ì‹  ê²ƒì´ ìˆìœ¼ë©´ ì–¸ì œë“  ë§ì”€í•´ ì£¼ì„¸ìš”. í—Œì‹ ì ìœ¼ë¡œ ì§€ì›í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
            ]
        },
        "ISFP": {
            "description": "ì˜ˆìˆ ì ì´ê³  ì‹¤ìš©ì ì¸ íƒ€ì…ì…ë‹ˆë‹¤. í˜„ì¬ì˜ ê²½í—˜ê³¼ ê°œì¸ì˜ ê°€ì¹˜ë¥¼ ì¤‘ì‹œí•©ë‹ˆë‹¤.",
            "hri_style": "ì˜ˆìˆ ì  ì ‘ê·¼, ì‹¤ìš©ì  í•´ê²°, ê°œì¸ì  ê²½í—˜ ì¤‘ì‹œ",
            "examples": [
                "ë‹¹ì‹ ë§Œì˜ íŠ¹ë³„í•œ ë°©ì‹ìœ¼ë¡œ í•´ê²°í•´ë³´ëŠ” ê±´ ì–´ë–¨ê¹Œìš”?",
                "í˜„ì¬ì˜ ê²½í—˜ì„ ìµœëŒ€í•œ í™œìš©í•´ì„œ ë„ì™€ë“œë¦´ê²Œìš”.",
                "ê°œì¸ì ì¸ ê°€ì¹˜ì™€ ê°ì •ì„ ì¡´ì¤‘í•˜ë©´ì„œ í•¨ê»˜ ì§„í–‰í•´ë³´ê² ìŠµë‹ˆë‹¤.",
                "ì˜ˆìˆ ì ì´ê³  ì°½ì˜ì ì¸ ì ‘ê·¼ìœ¼ë¡œ ì•„ë¦„ë‹¤ìš´ í•´ê²°ì±…ì„ ë§Œë“¤ì–´ë³´ì‹œì£ .",
                "ì§€ê¸ˆ ì´ ìˆœê°„ì˜ ëŠë‚Œê³¼ ì§ê°ì„ ì†Œì¤‘íˆ ì—¬ê¸°ë©° ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
            ]
        },
        "ISTJ": {
            "description": "ì‹ ë¢°í•  ìˆ˜ ìˆê³  ì²´ê³„ì ì¸ íƒ€ì…ì…ë‹ˆë‹¤. ê·œì¹™ê³¼ ì •í™•ì„±ì„ ì¤‘ì‹œí•©ë‹ˆë‹¤.",
            "hri_style": "ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì•ˆë‚´, ì²´ê³„ì  ê´€ë¦¬, ì •í™•ì„± ì¤‘ì‹œ",
            "examples": [
                "ê·œì •ì— ë”°ë¼ ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ë³´ë¥¼ ì œê³µí•´ ë“œë¦´ê²Œìš”.",
                "ì²´ê³„ì ìœ¼ë¡œ ì§„í–‰í•˜ì—¬ ì•ˆì „í•˜ê²Œ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
                "ê²€ì¦ëœ ì ˆì°¨ë¥¼ í†µí•´ í™•ì‹¤í•˜ê³  ì •í™•í•œ ê²°ê³¼ë¥¼ ë³´ì¥í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
                "ì „í†µì ì´ê³  ê²€ì¦ëœ ë°©ë²•ìœ¼ë¡œ ì•ˆì •ì ì¸ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ê² ìŠµë‹ˆë‹¤.",
                "ì„¸ë¶€ì‚¬í•­ê¹Œì§€ ê¼¼ê¼¼íˆ í™•ì¸í•˜ì—¬ ì™„ë²½í•œ ê²°ê³¼ë¥¼ ë§Œë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤."
            ]
        },
        "ISTP": {
            "description": "ì‹¤ìš©ì ì´ê³  ë¶„ì„ì ì¸ íƒ€ì…ì…ë‹ˆë‹¤. ë¬¸ì œ í•´ê²°ê³¼ íš¨ìœ¨ì„±ì„ ì¤‘ì‹œí•©ë‹ˆë‹¤.",
            "hri_style": "ì‹¤ìš©ì  í•´ê²°, ë¶„ì„ì  ì ‘ê·¼, íš¨ìœ¨ì„± ì¤‘ì‹œ",
            "examples": [
                "ë¬¸ì œë¥¼ ë¶„ì„í•´ì„œ ì‹¤ìš©ì ì¸ í•´ê²°ì±…ì„ ì œì•ˆí•´ ë“œë¦´ê²Œìš”.",
                "íš¨ìœ¨ì ìœ¼ë¡œ ì§„í–‰í•  ìˆ˜ ìˆë„ë¡ ë¶„ì„ì ìœ¼ë¡œ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
                "ê°„ë‹¨í•˜ê³  ì§ì ‘ì ì¸ ë°©ë²•ìœ¼ë¡œ ë¹ ë¥´ê²Œ í•´ê²°í•´ë³´ê² ìŠµë‹ˆë‹¤.",
                "í•„ìš”í•œ ê²ƒë§Œ ê³¨ë¼ì„œ íš¨ê³¼ì ìœ¼ë¡œ ì²˜ë¦¬í•´ë“œë¦´ê²Œìš”.",
                "ì‹¤ì œ ìƒí™©ì— ë§ëŠ” í˜„ì‹¤ì ì¸ í•´ê²°ì±…ì„ ì°¾ì•„ë³´ì‹œì£ ."
            ]
        }
    }
    
    # ì¥ì†Œë³„ íŠ¹í™” ê°€ì´ë“œ
    location_guides = {
        "ë³‘ì›": {
            "ENFJ": {"hri_style": "ê³µê°ì  ì˜ë£Œ ì„œë¹„ìŠ¤, í™˜ì ì¤‘ì‹¬ì  ì ‘ê·¼, ì¹˜ë£ŒíŒ€ í˜‘ë ¥"},
            "ENTJ": {"hri_style": "ì „ëµì  ì¹˜ë£Œ ê³„íš, íš¨ìœ¨ì  ì˜ë£Œ ê´€ë¦¬, ì²´ê³„ì  ì§„ë£Œ"},
            "INFJ": {"hri_style": "ì§ê´€ì  ì§„ë‹¨, ê¹Šì€ í™˜ì ì´í•´, ê°œì¸í™”ëœ ì¹˜ë£Œ"},
            "INTJ": {"hri_style": "ì „ëµì  ì˜ë£Œ ê³„íš, í˜ì‹ ì  ì¹˜ë£Œ ë°©ë²•, ì¥ê¸°ì  ê±´ê°• ê´€ë¦¬"}
        },
        "ë„ì„œê´€": {
            "ENFJ": {"hri_style": "ê³µê°ì  í•™ìŠµ ì§€ì›, ë…ì„œ ë¬¸í™” ì¡°ì„±, ì´ìš©ì ì„±ì¥ ë„ì›€"},
            "ENTJ": {"hri_style": "ì „ëµì  í•™ìŠµ ê³„íš, íš¨ìœ¨ì  ì •ë³´ ê´€ë¦¬, ì²´ê³„ì  êµìœ¡"},
            "INFJ": {"hri_style": "ì§ê´€ì  ë…ì„œ ì¶”ì²œ, ê¹Šì€ ì§€ì‹ íƒêµ¬, ê°œì¸í™”ëœ í•™ìŠµ"},
            "INTJ": {"hri_style": "ì „ëµì  í•™ìŠµ ì„¤ê³„, í˜ì‹ ì  êµìœ¡ ë°©ë²•, ì¥ê¸°ì  ì§€ì‹ êµ¬ì¶•"}
        },
        "ì‡¼í•‘ëª°": {
            "ENFJ": {"hri_style": "ê³µê°ì  ê³ ê° ì„œë¹„ìŠ¤, ì‡¼í•‘ ê²½í—˜ í–¥ìƒ, ê³ ê° ë§Œì¡± ì¤‘ì‹œ"},
            "ENTJ": {"hri_style": "ì „ëµì  ì‡¼í•‘ ì•ˆë‚´, íš¨ìœ¨ì  êµ¬ë§¤ ì§€ì›, ì²´ê³„ì  ì„œë¹„ìŠ¤"},
            "INFJ": {"hri_style": "ì§ê´€ì  ìƒí’ˆ ì¶”ì²œ, ê¹Šì€ ê³ ê° ì´í•´, ê°œì¸í™”ëœ ì„œë¹„ìŠ¤"},
            "INTJ": {"hri_style": "ì „ëµì  ì‡¼í•‘ ê³„íš, í˜ì‹ ì  ì„œë¹„ìŠ¤ ë°©ë²•, ì¥ê¸°ì  ê³ ê° ê´€ê³„"}
        },
        "í•™êµ": {
            "ENFJ": {"hri_style": "ê³µê°ì  êµìœ¡ ì§€ì›, í•™ìƒ ì„±ì¥ ë„ì›€, í•™ìŠµ í™˜ê²½ ì¡°ì„±"},
            "ENTJ": {"hri_style": "ì „ëµì  í•™ìŠµ ê³„íš, íš¨ìœ¨ì  êµìœ¡ ê´€ë¦¬, ì²´ê³„ì  í•™ìŠµ"},
            "INFJ": {"hri_style": "ì§ê´€ì  í•™ìŠµ ì•ˆë‚´, ê¹Šì€ í•™ìƒ ì´í•´, ê°œì¸í™”ëœ êµìœ¡"},
            "INTJ": {"hri_style": "ì „ëµì  êµìœ¡ ì„¤ê³„, í˜ì‹ ì  í•™ìŠµ ë°©ë²•, ì¥ê¸°ì  ì§€ì‹ êµ¬ì¶•"}
        },
        "ê³µí•­": {
            "ENFJ": {"hri_style": "ê³µê°ì  ì—¬í–‰ ì„œë¹„ìŠ¤, í¸ì•ˆí•œ ì—¬í–‰ ê²½í—˜, ì—¬í–‰ì ì•ˆë‚´"},
            "ENTJ": {"hri_style": "ì „ëµì  ì—¬í–‰ ê³„íš, íš¨ìœ¨ì  ì—¬í–‰ ê´€ë¦¬, ì²´ê³„ì  ì„œë¹„ìŠ¤"},
            "INFJ": {"hri_style": "ì§ê´€ì  ì—¬í–‰ ì•ˆë‚´, ê¹Šì€ ì—¬í–‰ì ì´í•´, ê°œì¸í™”ëœ ì„œë¹„ìŠ¤"},
            "INTJ": {"hri_style": "ì „ëµì  ì—¬í–‰ ì„¤ê³„, í˜ì‹ ì  ì„œë¹„ìŠ¤ ë°©ë²•, ì¥ê¸°ì  ì—¬í–‰ ê³„íš"}
        }
    }
    
    # ì„ íƒëœ ì¥ì†Œì˜ íŠ¹í™” ê°€ì´ë“œ ì ìš©
    if location in location_guides:
        for mbti_type in base_guide:
            if mbti_type in location_guides[location]:
                base_guide[mbti_type]["hri_style"] = location_guides[location][mbti_type]["hri_style"]
    
    return base_guide

def check_existing_diagnosis(user_id, robot_id):
    """ê°™ì€ ì‚¬ìš©ì-ë¡œë´‡ ì¡°í•©ì˜ ìµœê·¼ ì§„ë‹¨ í™•ì¸"""
    try:
        # ìµœê·¼ 24ì‹œê°„ ë‚´ ê°™ì€ ì‚¬ìš©ì-ë¡œë´‡ ì¡°í•©ì˜ ì§„ë‹¨ í™•ì¸
        yesterday = datetime.now(pytz.timezone("Asia/Seoul")) - timedelta(hours=24)
        res = supabase.table("responses").select("*").eq("user_id", user_id).eq("robot_id", robot_id).gte("timestamp", yesterday.isoformat()).execute()
        
        if res.data:
            return True, res.data[0]  # ìµœê·¼ ì§„ë‹¨ì´ ìˆìŒ
        return False, None
    except Exception as e:
        st.error(f"ì§„ë‹¨ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
        return False, None

def generate_diagnosis_id():
    """ê³ ìœ í•œ ì§„ë‹¨ ì„¸ì…˜ ID ìƒì„±"""
    return f"diagnosis_{int(time.time())}_{st.session_state.user_id}_{st.session_state.robot_id}"

def save_response_with_session(diagnosis_data):
    """ì§„ë‹¨ ì„¸ì…˜ IDë¥¼ í¬í•¨í•œ ì‘ë‹µ ë°ì´í„° ì €ì¥"""
    try:
        # ì…ë ¥ê°’ ì •ì œ
        diagnosis_data["user_id"] = sanitize_input(diagnosis_data["user_id"])
        diagnosis_data["robot_id"] = sanitize_input(diagnosis_data["robot_id"])
        
        # ìœ íš¨ì„± ê²€ì¦
        user_valid, user_msg = validate_user_id(diagnosis_data["user_id"])
        robot_valid, robot_msg = validate_robot_id(diagnosis_data["robot_id"])
        
        if not user_valid:
            st.error(f"ì‚¬ìš©ì ID ì˜¤ë¥˜: {user_msg}")
            return False
        
        if not robot_valid:
            st.error(f"ë¡œë´‡ ID ì˜¤ë¥˜: {robot_msg}")
            return False
        
        if not supabase:
            st.error("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ì—†ìŠµë‹ˆë‹¤.")
            return False
            
        # diagnosis_session_idê°€ ìˆìœ¼ë©´ ì¤‘ë³µ í™•ì¸, ì—†ìœ¼ë©´ ê¸°ë³¸ ì €ì¥
        if "diagnosis_session_id" in diagnosis_data:
            try:
                # ì¤‘ë³µ ì§„ë‹¨ ì„¸ì…˜ í™•ì¸
                existing = supabase.table("responses").select("diagnosis_session_id").eq("diagnosis_session_id", diagnosis_data["diagnosis_session_id"]).execute()
                if existing.data:
                    st.warning("ì´ë¯¸ ì €ì¥ëœ ì§„ë‹¨ ì„¸ì…˜ì…ë‹ˆë‹¤.")
                    return False
            except Exception as e:
                # diagnosis_session_id ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš° ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰
                st.info("ì§„ë‹¨ ì„¸ì…˜ ID ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        
        # ê¸°ë³¸ ì €ì¥ ë°ì´í„° êµ¬ì„±
        save_data = {
            "user_id": diagnosis_data["user_id"],
            "gender": diagnosis_data["gender"],
            "age_group": diagnosis_data["age_group"],
            "job": diagnosis_data["job"],
            "robot_id": diagnosis_data["robot_id"],
            "responses": diagnosis_data["responses"],
            "mbti": diagnosis_data["mbti"],
            "scores": diagnosis_data["scores"],
            "timestamp": diagnosis_data["timestamp"]
        }
        
        # location ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì¶”ê°€
        try:
            # ë¨¼ì € location ì»¬ëŸ¼ í¬í•¨í•´ì„œ ì €ì¥ ì‹œë„
            save_data_with_location = save_data.copy()
            save_data_with_location["location"] = diagnosis_data.get("location", "ì¼ë°˜")
            supabase.table("responses").insert(save_data_with_location).execute()
            return True
        except Exception as location_error:
            # location ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš°, location ì—†ì´ ì €ì¥ ì‹œë„
            if "location" in str(location_error).lower():
                st.warning("ë°ì´í„°ë² ì´ìŠ¤ì— location ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. location ì •ë³´ ì—†ì´ ì €ì¥í•©ë‹ˆë‹¤.")
                supabase.table("responses").insert(save_data).execute()
                return True
            else:
                raise location_error
    except Exception as e:
        st.error(f"ì‘ë‹µ ì €ì¥ ì‹¤íŒ¨: {e}")
        return False

# ì´ˆê¸°í™”
init_session_state()
setup_styles()
guide_data = load_guide_data(st.session_state.get('selected_location', 'ì¼ë°˜'))

# ì‚¬ìš©ì ID í™•ì¸
if not st.session_state.user_id:
    st.markdown("# ğŸ¤– ë¡œë´‡ MBTI ì§„ë‹¨ ì‹œìŠ¤í…œ")
    st.markdown("### ì‚¬ìš©ì IDë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”")
    
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        user_id = st.text_input("ì‚¬ìš©ì ID", placeholder="ì˜ˆ: ê¹€ì² ìˆ˜, ì—°êµ¬ì‹¤A, íŒ€1", key="user_id_input")
        if st.button("ì‹œì‘í•˜ê¸°"):
            user_valid, user_msg = validate_user_id(user_id)
            if user_valid:
                st.session_state.user_id = user_id
                st.success(f"ì‚¬ìš©ì ID '{user_id}'ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
                st.rerun()
            else:
                st.error(f"ì‚¬ìš©ì ID ì˜¤ë¥˜: {user_msg}")
        
        with st.expander("ì‚¬ìš©ì ID ê°€ì´ë“œ"):
            st.info("""
            **ì‚¬ìš©ì ID ì˜ˆì‹œ:**
            - ê°œì¸: ê¹€ì² ìˆ˜, í™ê¸¸ë™, ì—°êµ¬ìA
            - íŒ€: ì—°êµ¬ì‹¤A, íŒ€1, ê·¸ë£¹B
            - í”„ë¡œì íŠ¸: í”„ë¡œì íŠ¸A, ì‹¤í—˜1, í…ŒìŠ¤íŠ¸íŒ€
            
            **ê·œì¹™:**
            - 2-20ì ì´ë‚´
            - ì˜ë¬¸, ìˆ«ì, í•œê¸€, ê³µë°±, ì–¸ë”ìŠ¤ì½”ì–´(_), í•˜ì´í”ˆ(-) ì‚¬ìš© ê°€ëŠ¥
            - íŠ¹ìˆ˜ë¬¸ì ì œí•œ (ë³´ì•ˆìƒ)
            """)
    st.stop()

# ë¡œê·¸ì¸ëœ ê²½ìš° USER_ID ì„¤ì •
USER_ID = st.session_state.user_id

# ì‚¬ì´ë“œë°” í‘œì‹œ
def show_sidebar():
    """ì‚¬ì´ë“œë°” í‘œì‹œ"""
    with st.sidebar:
        st.header("ğŸ‘¤ ì‚¬ìš©ì/ë¡œë´‡ ì •ë³´")
        show_user_input()
        show_user_profile()
        show_robot_management()
        show_admin_section()

def show_user_input():
    """ì‚¬ìš©ì ID ì…ë ¥"""
    st.subheader("ğŸ†” ì‚¬ìš©ì ID")
    
    # í˜„ì¬ ì‚¬ìš©ì ID í‘œì‹œ
    if st.session_state.user_id:
        st.success(f"í˜„ì¬ ì‚¬ìš©ì: **{st.session_state.user_id}**")
        if st.button("ì‚¬ìš©ì ë³€ê²½"):
            st.session_state.user_id = None
            st.rerun()
    else:
        user_id = st.text_input("ì‚¬ìš©ì ID ì…ë ¥", placeholder="ì˜ˆ: ê¹€ì² ìˆ˜, ì—°êµ¬ì‹¤A, íŒ€1")
        if st.button("í™•ì¸"):
            user_valid, user_msg = validate_user_id(user_id)
            if user_valid:
                st.session_state.user_id = user_id
                st.success(f"ì‚¬ìš©ì ID '{user_id}'ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
                st.rerun()
            else:
                st.error(f"ì‚¬ìš©ì ID ì˜¤ë¥˜: {user_msg}")

def show_user_profile():
    """ì‚¬ìš©ì í”„ë¡œí•„ í‘œì‹œ"""
    if not st.session_state.user_id:
        st.warning("ë¨¼ì € ì‚¬ìš©ì IDë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        return
    
    st.divider()
    st.subheader("ğŸ‘¤ ì‚¬ìš©ì í”„ë¡œí•„")
    
    profile = st.session_state.user_profile
    gender = st.selectbox("ì„±ë³„", ["ë‚¨", "ì—¬"], index=0 if profile["gender"]=="ë‚¨" else 1)
    age_group_list = ["10ëŒ€","20ëŒ€","30ëŒ€","40ëŒ€","50ëŒ€+"]
    age_group = st.selectbox("ì—°ë ¹ëŒ€", age_group_list, index=age_group_list.index(profile["age_group"]))
    job_list = ["í•™ìƒ","ì—°êµ¬ì›","êµìˆ˜","íšŒì‚¬ì›","ê¸°íƒ€"]
    job_sel_idx = job_list.index(profile["job"]) if profile["job"] in job_list else job_list.index("ê¸°íƒ€")
    job_sel = st.selectbox("ì§ì—…", job_list, index=job_sel_idx)
    
    if job_sel == "ê¸°íƒ€":
        job_detail = st.text_input("ì§ì—… ì§ì ‘ ì…ë ¥", value=profile["job"] if profile["job"] not in job_list else "")
        job_final = job_detail if job_detail.strip() else "ê¸°íƒ€"
    else:
        job_final = job_sel
    
    st.session_state.user_profile = {"gender": gender, "age_group": age_group, "job": job_final}

def show_robot_management():
    """ë¡œë´‡ ê´€ë¦¬ í‘œì‹œ"""
    if not st.session_state.user_id:
        st.warning("ë¨¼ì € ì‚¬ìš©ì IDë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        return
    
    st.divider()
    st.subheader("ğŸ¤– ë¡œë´‡ ID ê´€ë¦¬")
    
    # ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸
    db_status = "âŒ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜"
    try:
        # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        test_result = supabase.table("user_robots").select("id").limit(1).execute()
        db_status = "âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ë¨"
    except Exception as e:
        error_msg = str(e).lower()
        if "does not exist" in error_msg or "relation" in error_msg:
            db_status = "âš ï¸ user_robots í…Œì´ë¸”ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤"
            st.warning("""
            **user_robots í…Œì´ë¸”ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.**
            
            í•´ê²° ë°©ë²•:
            1. Supabase ëŒ€ì‹œë³´ë“œ â†’ SQL Editor
            2. `create_user_robots_table.sql` íŒŒì¼ì˜ ë‚´ìš©ì„ ì‹¤í–‰
            3. ë˜ëŠ” ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜
            """)
        elif "permission" in error_msg:
            db_status = "âŒ ê¶Œí•œ ì˜¤ë¥˜"
        elif "connection" in error_msg:
            db_status = "âŒ ì—°ê²° ì˜¤ë¥˜"
        else:
            db_status = f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜: {str(e)[:50]}"
    
    st.caption(f"ìƒíƒœ: {db_status}")
    
    # ë¡œë´‡ ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸° (ë¡œì»¬ + ë°ì´í„°ë² ì´ìŠ¤)
    robot_opts = list(st.session_state.robot_list)
    
    # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¡œë´‡ ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸° ì‹œë„
    try:
        db_robots = load_user_robots(st.session_state.user_id)
        for robot in db_robots:
            if robot not in robot_opts:
                robot_opts.append(robot)
    except Exception as e:
        st.info(f"ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¡œë´‡ ëª©ë¡ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)[:50]}")
    
    # ìƒˆ ë¡œë´‡ ë“±ë¡
    col1, col2 = st.columns([3, 1])
    with col1:
        new_robot = st.text_input("ìƒˆ ë¡œë´‡ ID ë“±ë¡", placeholder="ì˜ˆ: ë‚´ì‹ê¸°1, ë¡œë´‡A")
    with col2:
        if st.button("â• ë“±ë¡"):
            if not new_robot.strip():
                st.warning("ë¡œë´‡ IDë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                robot_valid, robot_msg = validate_robot_id(new_robot)
                if robot_valid:
                    if new_robot.strip() not in robot_opts:
                        if save_robot(st.session_state.user_id, new_robot.strip()):
                            robot_opts.insert(0, new_robot.strip())
                            st.session_state.robot_list = robot_opts
                            st.success(f"âœ… '{new_robot.strip()}' ë“±ë¡ ì™„ë£Œ!")
                            st.rerun()
                        else:
                            st.error("ë¡œë´‡ ë“±ë¡ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
                    else:
                        st.warning("ì´ë¯¸ ë“±ë¡ëœ ë¡œë´‡ì…ë‹ˆë‹¤.")
                else:
                    st.error(f"ë¡œë´‡ ID ì˜¤ë¥˜: {robot_msg}")
    
    # ë¡œë´‡ ì„ íƒ
    if robot_opts:
        robot_id = st.selectbox("ì§„ë‹¨ ëŒ€ìƒ ë¡œë´‡(ì„ íƒ)", robot_opts, 
                               index=0 if st.session_state.robot_id not in robot_opts else robot_opts.index(st.session_state.robot_id))
        st.session_state.robot_id = robot_id
        
        # ë¡œë´‡ ì‚­ì œ ê¸°ëŠ¥
        if len(robot_opts) > 1:  # ìµœì†Œ 1ê°œëŠ” ë‚¨ê²¨ë‘ê¸°
            st.subheader("ğŸ—‘ï¸ ë¡œë´‡ ì‚­ì œ")
            delete_robot = st.selectbox("ì‚­ì œí•  ë¡œë´‡ ì„ íƒ", robot_opts, key="delete_robot")
            if st.button("ğŸ—‘ï¸ ì‚­ì œ"):
                try:
                    # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì‚­ì œ ì‹œë„
                    if supabase:
                        supabase.table("user_robots").delete().eq("user_id", st.session_state.user_id).eq("robot_name", delete_robot).execute()
                    robot_opts.remove(delete_robot)
                    st.session_state.robot_list = robot_opts
                    if st.session_state.robot_id == delete_robot:
                        st.session_state.robot_id = robot_opts[0]
                    st.success(f"âœ… '{delete_robot}' ì‚­ì œ ì™„ë£Œ!")
                    st.rerun()
                except Exception as e:
                    st.error(f"ë¡œë´‡ ì‚­ì œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {str(e)[:50]}")
        else:
            st.info("ë¡œë´‡ì€ ìµœì†Œ 1ê°œ ì´ìƒ ìœ ì§€í•´ì•¼ í•©ë‹ˆë‹¤.")
    else:
        st.warning("ë“±ë¡ëœ ë¡œë´‡ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆ ë¡œë´‡ì„ ë“±ë¡í•´ì£¼ì„¸ìš”.")

def show_admin_section():
    """ê´€ë¦¬ì ì„¹ì…˜ í‘œì‹œ"""
    st.divider()
    st.subheader("ğŸ”§ ê´€ë¦¬ì ë¡œê·¸ì¸")
    
    if not st.session_state.admin_logged_in:
        admin_username = st.text_input("ê´€ë¦¬ì ID", key="admin_username_sidebar")
        admin_password = st.text_input("ë¹„ë°€ë²ˆí˜¸", type="password", key="admin_password_sidebar")
        
        col1, col2 = st.columns([1, 1])
        with col1:
            if st.button("ë¡œê·¸ì¸", key="admin_login_btn"):
                if check_admin_login(admin_username, admin_password):
                    st.session_state.admin_logged_in = True
                    st.success("ê´€ë¦¬ì ë¡œê·¸ì¸ ì„±ê³µ!")
                    st.rerun()
                else:
                    st.error("ì˜ëª»ëœ ë¡œê·¸ì¸ ì •ë³´ì…ë‹ˆë‹¤.")
        
        with col2:
            if st.button("ë¡œê·¸ì•„ì›ƒ", key="admin_logout_btn"):
                st.session_state.admin_logged_in = False
                st.rerun()
        
        st.caption("ê´€ë¦¬ì ê³„ì •: admin/admin123 ë˜ëŠ” manager/manager123")
    else:
        st.success("ê´€ë¦¬ì ë¡œê·¸ì¸ë¨")
        if st.button("ë¡œê·¸ì•„ì›ƒ", key="admin_logout_btn2"):
            st.session_state.admin_logged_in = False
            st.rerun()

# ë©”ì¸ ì½˜í…ì¸  í‘œì‹œ
def show_main_content():
    """ë©”ì¸ ì½˜í…ì¸  í‘œì‹œ"""
    if not st.session_state.user_id:
        st.warning("ì‚¬ì´ë“œë°”ì—ì„œ ì‚¬ìš©ì IDë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        return
    
    page = st.session_state.page
    
    if page == 1:
        show_diagnosis_page()
    elif page == 2:
        show_results_page()
    elif page == 3:
        show_analytics_page()

def show_diagnosis_page():
    """ì§„ë‹¨ í˜ì´ì§€ í‘œì‹œ"""
    st.header("1ï¸âƒ£ MBTI ê¸°ë°˜ HRI UX ì§„ë‹¨")
    
    # ì‚¬ìš©ì ì •ë³´ í‘œì‹œ
    st.info(f"ğŸ‘¤ **ì‚¬ìš©ì**: {st.session_state.user_id} | ğŸ¤– **ë¡œë´‡**: {st.session_state.robot_id}")
    
    # ì¤‘ë³µ ì§„ë‹¨ í™•ì¸
    has_recent_diagnosis, recent_diagnosis = check_existing_diagnosis(st.session_state.user_id, st.session_state.robot_id)
    
    if has_recent_diagnosis and not st.session_state.get('force_new_diagnosis', False):
        st.warning(f"âš ï¸ ìµœê·¼ 24ì‹œê°„ ë‚´ì— ì´ë¯¸ '{st.session_state.robot_id}'ì— ëŒ€í•œ ì§„ë‹¨ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        st.info(f"ë§ˆì§€ë§‰ ì§„ë‹¨ ì‹œê°„: {recent_diagnosis['timestamp']}")
        
        col1, col2 = st.columns(2)
        with col1:
            if st.button("ìƒˆë¡œìš´ ì§„ë‹¨ ì§„í–‰", type="primary"):
                # ìƒˆë¡œìš´ ì§„ë‹¨ ì„¸ì…˜ ì‹œì‘
                reset_diagnosis_session()
                st.session_state.current_diagnosis_id = generate_diagnosis_id()
                st.session_state.saved_result = False
                st.session_state.force_new_diagnosis = True  # ìƒˆ ì§„ë‹¨ ê°•ì œ í”Œë˜ê·¸
                st.rerun()
        with col2:
            if st.button("ê²°ê³¼ í˜ì´ì§€ë¡œ ì´ë™"):
                st.session_state.page = 2
                st.rerun()
        return
    
    # ì§„ë‹¨ ì¥ì†Œ ì„ íƒ
    st.subheader("ğŸ¢ ì§„ë‹¨ ì¥ì†Œ ì„ íƒ")
    location_options = ["ì¼ë°˜", "ë³‘ì›", "ë„ì„œê´€", "ì‡¼í•‘ëª°", "í•™êµ", "ê³µí•­"]
    selected_location = st.selectbox(
        "ì§„ë‹¨í•  ì¥ì†Œë¥¼ ì„ íƒí•˜ì„¸ìš”",
        location_options,
        index=0,
        help="ì¥ì†Œë³„ë¡œ ë‹¤ë¥¸ ì„¤ë¬¸ ë‚´ìš©ì´ ì œê³µë©ë‹ˆë‹¤"
    )
    
    # ì„ íƒëœ ì¥ì†Œë¥¼ ì„¸ì…˜ì— ì €ì¥
    st.session_state.selected_location = selected_location
    
    st.info(f"ğŸ“‹ {selected_location} í™˜ê²½ ì§„ë‹¨ (ì´ {len(load_questions(selected_location))}ê°œ ì§ˆë¬¸)")
    
    st.divider()
    
    consent = st.checkbox("ìµëª… ë°ì´í„° ë¶„ì„ í™œìš©ì— ë™ì˜í•©ë‹ˆë‹¤.", value=True)
    if not consent:
        st.warning("ì§„ë‹¨ ì‹œì‘ì—” ë™ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤!")
        st.stop()
    
    # ì„¤ë¬¸ í‘œì‹œ
    responses = show_survey()
    
    # ê²°ê³¼ ë³´ê¸° ë²„íŠ¼
    if st.button("ğŸ¯ ê²°ê³¼ ë³´ê¸°", type="primary", use_container_width=True):
        if len(responses) < len(load_questions(selected_location)):
            st.warning("ëª¨ë“  ë¬¸í•­ì— ë‹µë³€í•´ì£¼ì„¸ìš”!")
        else:
            # ìƒˆë¡œìš´ ì§„ë‹¨ ì„¸ì…˜ ì‹œì‘
            if not st.session_state.current_diagnosis_id:
                st.session_state.current_diagnosis_id = generate_diagnosis_id()
            
            st.session_state.page = 2
            st.rerun()

def show_survey():
    """ì„¤ë¬¸ í‘œì‹œ"""
    responses = {}
    current_questions = load_questions(st.session_state.selected_location)
    
    # 2ê°œì”© ì§ˆë¬¸ì„ í‘œì‹œ
    for i in range(0, len(current_questions), 2):
        col1, col2 = st.columns(2)
        
        # ì²« ë²ˆì§¸ ì§ˆë¬¸
        with col1:
            q1 = current_questions[i]
            st.write(f"**{i + 1}. {q1['text']}**")
            selected1 = st.radio(
                "ì„ íƒí•´ì£¼ì„¸ìš”:",
                q1['choices'],
                key=f"radio_{q1['id']}",
                label_visibility="collapsed"
            )
            if selected1:
                responses[q1['id']] = selected1
        
        # ë‘ ë²ˆì§¸ ì§ˆë¬¸ (ìˆëŠ” ê²½ìš°)
        with col2:
            if i + 1 < len(current_questions):
                q2 = current_questions[i + 1]
                st.write(f"**{i + 2}. {q2['text']}**")
                selected2 = st.radio(
                    "ì„ íƒí•´ì£¼ì„¸ìš”:",
                    q2['choices'],
                    key=f"radio_{q2['id']}",
                    label_visibility="collapsed"
                )
                if selected2:
                    responses[q2['id']] = selected2
        
        st.divider()
    
    st.session_state['responses'] = responses
    return responses

def show_results_page():
    """ê²°ê³¼ í˜ì´ì§€ í‘œì‹œ"""
    st.header(f"2ï¸âƒ£ [{st.session_state.robot_id}] ì§„ë‹¨ ê²°ê³¼Â·í”¼ë“œë°±")
    
    with st.spinner("ğŸ” MBTI ë¶„ì„ ì¤‘..."):
        time.sleep(1)
    
    responses = st.session_state.get('responses', {})
    profile = st.session_state.user_profile
    robot_id = st.session_state.robot_id
    user_id = st.session_state.user_id
    
    # ì‘ë‹µì´ ì—†ìœ¼ë©´ ì´ì „ ì§„ë‹¨ ë°ì´í„° í™•ì¸
    if not responses:
        has_recent_diagnosis, recent_diagnosis = check_existing_diagnosis(user_id, robot_id)
        if has_recent_diagnosis:
            st.info("ğŸ“‹ ì´ì „ ì§„ë‹¨ ê²°ê³¼ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.")
            mbti = recent_diagnosis['mbti']
            scores = recent_diagnosis['scores']
            
            # ì´ì „ ì§„ë‹¨ì˜ ì¥ì†Œ ì •ë³´ ë³µì›
            if 'location' in recent_diagnosis and recent_diagnosis['location']:
                st.session_state.selected_location = recent_diagnosis['location']
            else:
                # ê¸°ì¡´ ë°ì´í„°ì— location í•„ë“œê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ ì„¤ì •
                st.session_state.selected_location = 'ì¼ë°˜'
            
            display_results(mbti, scores, robot_id)
            return
        else:
            st.warning("ì§„ë‹¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì§„ë‹¨ì„ ì™„ë£Œí•´ì£¼ì„¸ìš”.")
            st.session_state.page = 1
            st.rerun()
            return
    
    scores = compute_scores(responses)
    
    if scores is None:
        st.warning("ëª¨ë“  ë¬¸í•­ì— ë‹µí•´ì£¼ì„¸ìš”.")
    else:
        scores = resolve_ties(scores)
        mbti = predict_type(scores)
        
        # ì¤‘ë³µ ì €ì¥ ë°©ì§€: í˜„ì¬ ì§„ë‹¨ ì„¸ì…˜ì—ì„œë§Œ ì €ì¥
        if not st.session_state.get('saved_result', False) and st.session_state.current_diagnosis_id:
            profile_with_location = profile.copy()
            profile_with_location['location'] = st.session_state.get('selected_location', 'ì¼ë°˜')
            
            # ì§„ë‹¨ ì„¸ì…˜ IDë¥¼ í¬í•¨í•˜ì—¬ ì €ì¥
            diagnosis_data = {
                "user_id": user_id,
                "gender": profile["gender"],
                "age_group": profile["age_group"],
                "job": profile["job"],
                "robot_id": robot_id,
                "responses": responses,
                "mbti": mbti,
                "scores": scores,
                "location": st.session_state.get('selected_location', 'ì¼ë°˜'),  # ì¥ì†Œ ì •ë³´ ì¶”ê°€
                "timestamp": datetime.now(pytz.timezone("Asia/Seoul")).isoformat(),
                "diagnosis_session_id": st.session_state.current_diagnosis_id  # ì„¸ì…˜ ID ì¶”ê°€
            }
            
            # ì €ì¥ ì„±ê³µ ì‹œì—ë§Œ saved_resultë¥¼ Trueë¡œ ì„¤ì •
            if save_response_with_session(diagnosis_data):
                st.session_state['saved_result'] = True
                st.success("âœ… ì§„ë‹¨ ê²°ê³¼ê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
            else:
                st.error("âŒ ì§„ë‹¨ ê²°ê³¼ ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        
        display_results(mbti, scores, robot_id)

def display_results(mbti, scores, robot_id):
    """ê²°ê³¼ í‘œì‹œ"""
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.markdown(f"""
        ## ğŸ¯ ì§„ë‹¨ ê²°ê³¼
        
        ### ğŸ‘¤ ì‚¬ìš©ì: **{st.session_state.user_id}**
        ### ğŸ¤– ë¡œë´‡: **{robot_id}**
        ### ğŸ¢ ì¥ì†Œ: **{st.session_state.get('selected_location', 'ì¼ë°˜')}**
        ### ğŸ§  MBTI ìœ í˜•: **{mbti}**
        """)
        
        # MBTI ìœ í˜•ë³„ ìƒ‰ìƒ í‘œì‹œ (í¬ê¸° ì¡°ì •)
        mbti_color = MBTI_COLORS.get(mbti, '#CCCCCC')
        st.markdown(f"""
        <div style="background-color: {mbti_color}; padding: 15px; border-radius: 10px; text-align: center; color: white; margin: 10px 0;">
            <h3 style="margin: 0; font-size: 1.5rem;">ğŸ¨ {mbti}</h3>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.subheader("ğŸ“Š ì ìˆ˜ ë¶„í¬")
        fig = create_score_chart(scores)
        st.plotly_chart(fig, use_container_width=True)
    
    # MBTI ê°€ì´ë“œ
    guide = guide_data.get(mbti)
    if guide:
        st.subheader("ğŸ“– MBTI ìœ í˜• ê°€ì´ë“œ")
        
        col1, col2 = st.columns([1, 1])
        with col1:
            st.write("**ğŸ­ ì„±ê²© íŠ¹ì§•:**")
            st.write(guide['description'])
        
        with col2:
            st.write("**ğŸ¤– HRI ìƒí˜¸ì‘ìš© ìŠ¤íƒ€ì¼:**")
            st.info(guide.get('hri_style', 'HRI ìŠ¤íƒ€ì¼ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.'))
            with st.expander("ğŸ“‹ ì‹œë‚˜ë¦¬ì˜¤ ì˜ˆì‹œ"):
                for i, ex in enumerate(guide['examples'], 1):
                    st.write(f"{i}. {ex}")
    
    # ì¥ì†Œë³„ íŠ¹í™” ì¡°ì–¸
    location = st.session_state.get('selected_location', 'ì¼ë°˜')
    if location != 'ì¼ë°˜':
        st.subheader(f"ğŸ¢ {location} í™˜ê²½ íŠ¹í™” ì¡°ì–¸")
        location_advice = {
            "ë³‘ì›": f"**{mbti}** ìœ í˜•ìœ¼ë¡œì„œ ë³‘ì› í™˜ê²½ì—ì„œëŠ” {guide.get('hri_style', 'íš¨ìœ¨ì ì´ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì„œë¹„ìŠ¤')}ë¥¼ ì œê³µí•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. í™˜ìì˜ ì•ˆì „ê³¼ í¸ì•ˆí•¨ì„ ìµœìš°ì„ ìœ¼ë¡œ ê³ ë ¤í•˜ì„¸ìš”.",
            "ë„ì„œê´€": f"**{mbti}** ìœ í˜•ìœ¼ë¡œì„œ ë„ì„œê´€ í™˜ê²½ì—ì„œëŠ” {guide.get('hri_style', 'ì¡°ìš©í•˜ê³  ì§‘ì¤‘í•  ìˆ˜ ìˆëŠ” í•™ìŠµ ì§€ì›')}ì„ ì œê³µí•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. ì§€ì‹ íƒêµ¬ì™€ í•™ìŠµ í™˜ê²½ ì¡°ì„±ì— ì¤‘ì ì„ ë‘ì„¸ìš”.",
            "ì‡¼í•‘ëª°": f"**{mbti}** ìœ í˜•ìœ¼ë¡œì„œ ì‡¼í•‘ëª° í™˜ê²½ì—ì„œëŠ” {guide.get('hri_style', 'ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ê³ ê° ì„œë¹„ìŠ¤')}ë¥¼ ì œê³µí•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. ê³ ê°ì˜ ì‡¼í•‘ ê²½í—˜ í–¥ìƒì— ì§‘ì¤‘í•˜ì„¸ìš”.",
            "í•™êµ": f"**{mbti}** ìœ í˜•ìœ¼ë¡œì„œ í•™êµ í™˜ê²½ì—ì„œëŠ” {guide.get('hri_style', 'êµìœ¡ì ì´ê³  ì„±ì¥ ì§€í–¥ì ì¸ í•™ìŠµ ì§€ì›')}ì„ ì œê³µí•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. í•™ìƒì˜ í•™ìŠµê³¼ ì„±ì¥ì„ ë•ëŠ” ë° ì¤‘ì ì„ ë‘ì„¸ìš”.",
            "ê³µí•­": f"**{mbti}** ìœ í˜•ìœ¼ë¡œì„œ ê³µí•­ í™˜ê²½ì—ì„œëŠ” {guide.get('hri_style', 'íš¨ìœ¨ì ì´ê³  ì•ˆì „í•œ ì—¬í–‰ ì„œë¹„ìŠ¤')}ë¥¼ ì œê³µí•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. ì—¬í–‰ìì˜ í¸ì˜ì™€ ì•ˆì „ì„ ìµœìš°ì„ ìœ¼ë¡œ ê³ ë ¤í•˜ì„¸ìš”."
        }
        st.info(location_advice.get(location, ""))
    
    # ë‹¤ìš´ë¡œë“œ ë° ë‹¤ìŒ ë‹¨ê³„
    st.subheader("ğŸ’¾ ê²°ê³¼ ì €ì¥")
    col1, col2, col3 = st.columns(3)
    with col1:
        st.download_button("ğŸ“¥ ê²°ê³¼ CSV ë‹¤ìš´ë¡œë“œ", 
                         pd.DataFrame([{'ì‚¬ìš©ì': st.session_state.user_id, 'ë¡œë´‡': robot_id, 'MBTI': mbti, 'ë‚ ì§œ': datetime.now().strftime('%Y-%m-%d')}]).to_csv(index=False), 
                         f"{st.session_state.user_id}_{robot_id}_{mbti}.csv")
    with col2:
        if st.button("ğŸ“Š í†µê³„/íˆìŠ¤í† ë¦¬ ëŒ€ì‹œë³´ë“œ ì´ë™", type="primary", use_container_width=True):
            st.session_state.page = 3
            st.rerun()
    with col3:
        if st.button("ğŸ”„ ìƒˆë¡œìš´ ì§„ë‹¨ ì‹œì‘", use_container_width=True):
            reset_diagnosis_session()
            st.session_state.page = 1
            st.rerun()

def show_analytics_page():
    """ë¶„ì„ í˜ì´ì§€ í‘œì‹œ"""
    st.header("3ï¸âƒ£ ì „ì²´ í†µê³„ Â· ë¡œë´‡ ì´ë ¥ Â· ì§‘ë‹¨ë¶„ì„(í†µí•©)")
    
    # ë°ì´í„° ìƒˆë¡œê³ ì¹¨ ë²„íŠ¼ ì¶”ê°€
    col1, col2, col3 = st.columns([1, 1, 2])
    with col1:
        if st.button("ğŸ”„ ë°ì´í„° ìƒˆë¡œê³ ì¹¨"):
            st.rerun()
    with col2:
        st.info(f"í˜„ì¬ ì‹œê°„: {datetime.now().strftime('%H:%M:%S')}")
    
    df = load_responses()
    if df.empty:
        st.info("ì•„ì§ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        if st.button("ì§„ë‹¨ ì²«í™”ë©´ìœ¼ë¡œ ëŒì•„ê°€ê¸°"):
            st.session_state.page = 1
            st.rerun()
        return
    
    # ë°ì´í„° ë¡œë”© ìƒíƒœ í‘œì‹œ
    st.success(f"âœ… ì´ {len(df)}ê°œì˜ ì§„ë‹¨ ë°ì´í„°ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    
    # ë””ë²„ê¹… ì •ë³´ í‘œì‹œ
    with st.expander("ğŸ” ë°ì´í„° ìƒì„¸ ì •ë³´ (ë””ë²„ê¹…)"):
        st.write("**ìµœê·¼ 5ê°œ ì§„ë‹¨ ë°ì´í„°:**")
        recent_data = df.sort_values('timestamp', ascending=False).head()
        st.dataframe(recent_data[['user_id', 'robot_id', 'mbti', 'timestamp']])
        
        st.write("**ì‚¬ìš©ìë³„ ë¡œë´‡ ì¡°í•©:**")
        user_robot_combinations = df.groupby(['user_id', 'robot_id']).size().reset_index(name='ì§„ë‹¨_íšŸìˆ˜')
        st.dataframe(user_robot_combinations)
    
    # ë‚ ì§œ ì»¬ëŸ¼ ì¶”ê°€
    df['date'] = pd.to_datetime(df['timestamp']).dt.date
    df['datetime'] = pd.to_datetime(df['timestamp'])
    
    # ì¤‘ë³µ ì œê±° ì˜µì…˜ ì œê³µ
    with st.expander("ğŸ”§ ë°ì´í„° í•„í„°ë§ ì˜µì…˜"):
        remove_duplicates = st.checkbox(
            "ì¤‘ë³µ ì§„ë‹¨ ì œê±° (ê°™ì€ ì‚¬ìš©ì-ë¡œë´‡ ì¡°í•©ì—ì„œ ìµœì‹  ì§„ë‹¨ë§Œ ìœ ì§€)", 
            value=False,
            help="ì²´í¬í•˜ë©´ ê°™ì€ ì‚¬ìš©ìê°€ ê°™ì€ ë¡œë´‡ì— ëŒ€í•´ ì—¬ëŸ¬ ë²ˆ ì§„ë‹¨í•œ ê²½ìš° ê°€ì¥ ìµœê·¼ ê²ƒë§Œ í‘œì‹œí•©ë‹ˆë‹¤."
        )
        
        if remove_duplicates:
            df_cleaned = df.sort_values('timestamp').drop_duplicates(
                subset=['user_id', 'robot_id'], 
                keep='last'
            )
            st.info(f"ì¤‘ë³µ ì œê±° ì „: {len(df)}ê°œ â†’ ì¤‘ë³µ ì œê±° í›„: {len(df_cleaned)}ê°œ")
            df = df_cleaned
        else:
            st.info("ëª¨ë“  ì§„ë‹¨ ë°ì´í„°ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤ (ì¤‘ë³µ í¬í•¨)")
    
    # íƒ­ìœ¼ë¡œ êµ¬ë¶„
    tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
        "ğŸ“Š ì „ì²´ íŠ¸ë Œë“œ", "ğŸ“ˆ ì§‘ë‹¨ë³„ ë¶„ì„", "ğŸ¤– ë¡œë´‡ ì´ë ¥", 
        "ğŸ§  ê³ ê¸‰ ë¶„ì„", "ğŸ“‹ ë°ì´í„° ê´€ë¦¬", "ğŸ”§ ê´€ë¦¬ì ê´€ë¦¬"
    ])
    
    with tab1:
        show_trend_analysis(df)
    
    with tab2:
        show_group_analysis(df)
    
    with tab3:
        show_robot_history(df)
    
    with tab4:
        show_advanced_analysis(df)
    
    with tab5:
        show_data_management(df)
    
    with tab6:
        show_admin_data_management(df)
    
    if st.button("ì§„ë‹¨ ì²«í™”ë©´ìœ¼ë¡œ ëŒì•„ê°€ê¸°"):
        st.session_state.page = 1
        st.rerun()

def show_trend_analysis(df):
    """íŠ¸ë Œë“œ ë¶„ì„ í‘œì‹œ"""
    st.subheader("ğŸ“Š ê¸°ê°„ë³„ MBTI íŠ¸ë Œë“œ")
    min_date, max_date = df['date'].min(), df['date'].max()
    
    # chart_type ë³€ìˆ˜ë¥¼ ë¨¼ì € ì´ˆê¸°í™”
    chart_type = "ë¼ì¸"  # ê¸°ë³¸ê°’ ì„¤ì •
    
    if min_date == max_date:
        st.info(f"ë°ì´í„° ë‚ ì§œ: {min_date}")
        df_period = df[df['date']==min_date]
    else:
        col1, col2 = st.columns([3, 1])
        with col1:
            date_sel = st.slider("ì¡°íšŒ ê¸°ê°„", min_value=min_date, max_value=max_date, 
                               value=(min_date, max_date), format="YYYY-MM-DD")
        with col2:
            chart_type = st.selectbox("ì°¨íŠ¸ ìœ í˜•", ["ë¼ì¸", "ë°”", "ì˜ì—­"])
        
        df_period = df[(df['date']>=date_sel[0])&(df['date']<=date_sel[1])]
    
    if not df_period.empty:
        fig = create_trend_chart(df_period, chart_type)
        st.plotly_chart(fig, use_container_width=True)
        
        # ìš”ì•½ í†µê³„
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ì´ ì§„ë‹¨ ìˆ˜", len(df_period))
        with col2:
            st.metric("MBTI ìœ í˜• ìˆ˜", df_period['mbti'].nunique())
        with col3:
            st.metric("ê°€ì¥ ë§ì€ ìœ í˜•", df_period['mbti'].mode().iloc[0] if not df_period['mbti'].mode().empty else "N/A")
        
        # ì‹œê°„ëŒ€ë³„ íŒ¨í„´ ìë™ í•´ì„
        st.subheader("ğŸ” ì‹œê°„ëŒ€ë³„ íŒ¨í„´ ìë™ ë¶„ì„")
        time_interpretations = analyze_time_patterns(df_period)
        
        if time_interpretations:
            for interpretation in time_interpretations:
                st.write(interpretation)
        else:
            st.info("ì‹œê°„ íŒ¨í„´ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ë” ë§ì€ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

def show_group_analysis(df):
    """ì§‘ë‹¨ë³„ ë¶„ì„ í‘œì‹œ"""
    st.subheader("ğŸ“ˆ ì§‘ë‹¨ë³„ MBTI ë¶„í¬ ë¶„ì„")
    
    col1, col2 = st.columns([1, 2])
    with col1:
        group_col = st.selectbox("ë¶„í¬ ë¶„ì„ ê¸°ì¤€", ["gender", "age_group", "job", "robot_id"])
        chart_style = st.selectbox("ì°¨íŠ¸ ìŠ¤íƒ€ì¼", ["ë°” ì°¨íŠ¸", "íŒŒì´ ì°¨íŠ¸", "íˆíŠ¸ë§µ"])
    
    with col2:
        group_df = df.groupby([group_col, "mbti"]).size().unstack(fill_value=0)
        
        if chart_style == "ë°” ì°¨íŠ¸":
            fig = px.bar(group_df, title=f"{group_col}ë³„ MBTI ë¶„í¬", 
                       color_discrete_map=MBTI_COLORS)
            fig.update_layout(height=400, xaxis_title=group_col, yaxis_title="ì§„ë‹¨ ìˆ˜")
            st.plotly_chart(fig, use_container_width=True)
        
        elif chart_style == "íŒŒì´ ì°¨íŠ¸":
            # ê°œì„ ëœ íŒŒì´ì°¨íŠ¸ - ê°€ë…ì„±ê³¼ ì‹œê°ì  ì§‘ì¤‘ë„ í–¥ìƒ
            categories = group_df.index
            n_cats = len(categories)
            
            if n_cats <= 2:
                # 2ê°œ ì´í•˜ì¼ ë•ŒëŠ” ë‚˜ë€íˆ ë°°ì¹˜
                cols = n_cats
                rows = 1
            elif n_cats <= 4:
                # 4ê°œ ì´í•˜ì¼ ë•ŒëŠ” 2x2 ë°°ì¹˜
                cols = 2
                rows = 2
            else:
                # ê·¸ ì´ìƒì¼ ë•ŒëŠ” 3ì—´ë¡œ ë°°ì¹˜
                cols = 3
                rows = (n_cats + cols - 1) // cols
            
            fig = make_subplots(
                rows=rows, cols=cols, 
                specs=[[{"type": "pie"}] * cols for _ in range(rows)],
                subplot_titles=[f"ğŸ“Š {cat}" for cat in categories],
                vertical_spacing=0.1,
                horizontal_spacing=0.05
            )
            
            for i, cat in enumerate(categories):
                row = i // cols + 1
                col = i % cols + 1
                
                values = group_df.loc[cat].values
                labels = group_df.columns
                
                # 0ì´ ì•„ë‹Œ ê°’ë§Œ í‘œì‹œ (ê°€ë…ì„± í–¥ìƒ)
                non_zero_mask = values > 0
                filtered_values = values[non_zero_mask]
                filtered_labels = labels[non_zero_mask]
                
                if len(filtered_values) > 0:
                    fig.add_trace(
                        go.Pie(
                            labels=filtered_labels, 
                            values=filtered_values, 
                            name=cat,
                            marker=dict(
                                colors=[MBTI_COLORS.get(mbti, '#2196F3') for mbti in filtered_labels],
                                line=dict(color='white', width=2)
                            ),
                            textinfo='label+percent',
                            textfont=dict(size=12, color='white'),
                            hovertemplate='<b>%{label}</b><br>ê°œìˆ˜: %{value}<br>ë¹„ìœ¨: %{percent}<extra></extra>',
                            hole=0.3  # ë„ë„› ì°¨íŠ¸ë¡œ ë§Œë“¤ì–´ ì‹œê°ì  ì§‘ì¤‘ë„ í–¥ìƒ
                        ),
                        row=row, col=col
                    )
            
            fig.update_layout(
                height=max(400, 350 * rows), 
                title_text=f"ğŸ¯ {group_col}ë³„ MBTI ë¶„í¬ ë¶„ì„",
                title_font_size=20,
                title_x=0.5,
                showlegend=True,
                legend=dict(
                    orientation="h",
                    yanchor="bottom",
                    y=-0.1,
                    xanchor="center",
                    x=0.5,
                    font=dict(size=12)
                ),
                font=dict(family="Arial, sans-serif", size=12),
                plot_bgcolor='rgba(0,0,0,0)',
                paper_bgcolor='rgba(0,0,0,0)'
            )
            
            st.plotly_chart(fig, use_container_width=True)
        
        else:  # íˆíŠ¸ë§µ
            fig = px.imshow(group_df, title=f"{group_col}ë³„ MBTI íˆíŠ¸ë§µ",
                          aspect="auto", color_continuous_scale="Viridis")
            fig.update_layout(height=400)
            st.plotly_chart(fig, use_container_width=True)
            
            # íˆíŠ¸ë§µ ì„¤ëª… ì¶”ê°€
            st.info("""
            **ğŸ“Š íˆíŠ¸ë§µ í•´ì„ ê°€ì´ë“œ:**
            - **ìƒ‰ìƒ ê°•ë„**: ì§„í•œ ìƒ‰ì¼ìˆ˜ë¡ í•´ë‹¹ ê·¸ë£¹ì—ì„œ íŠ¹ì • MBTI ìœ í˜•ì´ ë§ì´ ë‚˜íƒ€ë‚¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤
            - **íŒ¨í„´ ë¶„ì„**: ê°€ë¡œì¤„(ê·¸ë£¹)ë³„ë¡œ ì–´ë–¤ MBTIê°€ ì§‘ì¤‘ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
            - **ë¹„êµ ë¶„ì„**: ê·¸ë£¹ ê°„ MBTI ë¶„í¬ì˜ ì°¨ì´ë¥¼ í•œëˆˆì— íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
            - **í™œìš©**: íŠ¹ì • ê·¸ë£¹ì˜ ì„ í˜¸ MBTI íŒ¨í„´ì„ íŒŒì•…í•˜ì—¬ ë§ì¶¤í˜• ë¡œë´‡ ìƒí˜¸ì‘ìš© ì„¤ê³„ì— í™œìš© ê°€ëŠ¥í•©ë‹ˆë‹¤
            """)
            
            # íˆíŠ¸ë§µ íŒ¨í„´ ìë™ í•´ì„
            st.subheader("ğŸ” íˆíŠ¸ë§µ íŒ¨í„´ ìë™ ë¶„ì„")
            heatmap_interpretations = analyze_heatmap_patterns(group_df, group_col)
            
            if heatmap_interpretations:
                for interpretation in heatmap_interpretations:
                    st.write(interpretation)
            else:
                st.info("ë¶„ì„í•  íŒ¨í„´ì´ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë” ë§ì€ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            
            # í†µê³„ì  ìœ ì˜ì„± ë¶„ì„ ì¶”ê°€
            st.subheader("ğŸ” í†µê³„ì  ìœ ì˜ì„± ë¶„ì„")
            statistical_interpretations = analyze_statistical_significance(df, group_col)
            
            if statistical_interpretations:
                for interpretation in statistical_interpretations:
                    st.write(interpretation)
            
            # ë‹¤ì–‘ì„± ë¶„ì„ ì¶”ê°€
            st.subheader("ğŸ” ë‹¤ì–‘ì„± ë¶„ì„")
            diversity_interpretations = analyze_diversity_index(df)
            
            if diversity_interpretations:
                for interpretation in diversity_interpretations:
                    st.write(interpretation)

def show_robot_history(df):
    """ë¡œë´‡ ì´ë ¥ í‘œì‹œ"""
    st.subheader(f"ğŸ¤– '{st.session_state.robot_id}'ì˜ MBTI ë³€í™” íˆìŠ¤í† ë¦¬")
    
    # í˜„ì¬ ì‚¬ìš©ìì˜ ë¡œë´‡ ë°ì´í„°ë§Œ í•„í„°ë§
    bot_records = df[(df['user_id']==st.session_state.user_id) & (df['robot_id']==st.session_state.robot_id)].sort_values("timestamp")
    
    if not bot_records.empty:
        # ë‚ ì§œ í˜•ì‹ ê°œì„ 
        bot_records['timestamp'] = pd.to_datetime(bot_records['timestamp'])
        bot_records['date_formatted'] = bot_records['timestamp'].dt.strftime('%Yë…„ %mì›” %dì¼')
        bot_records['time_formatted'] = bot_records['timestamp'].dt.strftime('%H:%M')
        
        # íƒ€ì„ë¼ì¸ ì°¨íŠ¸ ê°œì„ 
        fig = px.scatter(
            bot_records, 
            x='timestamp', 
            y='mbti', 
            title=f"ğŸ“ˆ '{st.session_state.robot_id}' MBTI ë³€í™” íƒ€ì„ë¼ì¸",
            color='mbti', 
            color_discrete_map=MBTI_COLORS,
            size=[25] * len(bot_records),
            hover_data=['date_formatted', 'time_formatted', 'gender', 'age_group', 'job']
        )
        
        # ë ˆì´ì•„ì›ƒ ê°œì„ 
        fig.update_layout(
            height=500,
            xaxis_title="ë‚ ì§œ",
            yaxis_title="MBTI ìœ í˜•",
            xaxis=dict(
                tickformat='%mì›” %dì¼',
                tickmode='auto',
                nticks=min(10, len(bot_records)),
                tickangle=45
            ),
            yaxis=dict(
                categoryorder='array',
                categoryarray=['ISTJ', 'ISFJ', 'INFJ', 'INTJ', 'ISTP', 'ISFP', 'INFP', 'INTP',
                             'ESTP', 'ESFP', 'ENFP', 'ENTP', 'ESTJ', 'ESFJ', 'ENFJ', 'ENTJ']
            ),
            hovermode='closest',
            legend=dict(
                orientation="h",
                yanchor="bottom",
                y=1.02,
                xanchor="right",
                x=1
            ),
            margin=dict(l=50, r=50, t=80, b=80)
        )
        
        # ë§ˆì»¤ ìŠ¤íƒ€ì¼ ê°œì„ 
        fig.update_traces(
            marker=dict(
                size=20,
                line=dict(width=2, color='white')
            ),
            hovertemplate="<b>%{customdata[0]}</b><br>" +
                         "ì‹œê°„: %{customdata[1]}<br>" +
                         "MBTI: %{y}<br>" +
                         "ì„±ë³„: %{customdata[2]}<br>" +
                         "ì—°ë ¹ëŒ€: %{customdata[3]}<br>" +
                         "ì§ì—…: %{customdata[4]}<extra></extra>"
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # ìƒì„¸ ì´ë ¥ í…Œì´ë¸”
        st.subheader("ğŸ“‹ ìƒì„¸ ì§„ë‹¨ ì´ë ¥")
        
        # í…Œì´ë¸” ë°ì´í„° ì¤€ë¹„
        history_df = bot_records[["date_formatted", "time_formatted", "mbti", "gender", "age_group", "job"]].copy()
        history_df.columns = ["ë‚ ì§œ", "ì‹œê°„", "MBTI", "ì„±ë³„", "ì—°ë ¹ëŒ€", "ì§ì—…"]
        
        # MBTI ìƒ‰ìƒ ì ìš©
        def color_mbti(val):
            color = MBTI_COLORS.get(val, '#CCCCCC')
            return f'background-color: {color}; color: white; font-weight: bold; text-align: center;'
        
        styled_df = history_df.style.map(color_mbti, subset=['MBTI'])
        st.dataframe(styled_df, use_container_width=True)
        
        # í†µê³„ ìš”ì•½
        st.subheader("ğŸ“Š ì§„ë‹¨ í†µê³„")
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("ì´ ì§„ë‹¨ ìˆ˜", len(bot_records))
        with col2:
            st.metric("MBTI ìœ í˜• ìˆ˜", bot_records['mbti'].nunique())
        with col3:
            most_common = bot_records['mbti'].mode().iloc[0] if not bot_records['mbti'].mode().empty else "N/A"
            st.metric("ê°€ì¥ ë§ì€ ìœ í˜•", most_common)
        with col4:
            days_span = (bot_records['timestamp'].max() - bot_records['timestamp'].min()).days + 1
            st.metric("ì§„ë‹¨ ê¸°ê°„", f"{days_span}ì¼")
        
        # MBTI ë³€í™” ë¶„ì„
        if len(bot_records) > 1:
            st.subheader("ğŸ”„ MBTI ë³€í™” ë¶„ì„")
            
            # ë³€í™” íŒ¨í„´ ë¶„ì„
            changes = []
            for i in range(1, len(bot_records)):
                prev_mbti = bot_records.iloc[i-1]['mbti']
                curr_mbti = bot_records.iloc[i]['mbti']
                if prev_mbti != curr_mbti:
                    changes.append({
                        'from': prev_mbti,
                        'to': curr_mbti,
                        'date': bot_records.iloc[i]['date_formatted']
                    })
            
            if changes:
                st.info(f"ì´ {len(changes)}ë²ˆì˜ MBTI ë³€í™”ê°€ ìˆì—ˆìŠµë‹ˆë‹¤.")
                
                # ë³€í™” ì°¨íŠ¸
                if len(changes) > 0:
                    change_df = pd.DataFrame(changes)
                    fig_changes = px.scatter(
                        change_df,
                        x='date',
                        y='from',
                        color='to',
                        title="MBTI ë³€í™” íŒ¨í„´",
                        color_discrete_map=MBTI_COLORS
                    )
                    fig_changes.update_layout(height=300)
                    st.plotly_chart(fig_changes, use_container_width=True)
                
                # MBTI ë³€í™” íŒ¨í„´ ìë™ í•´ì„
                st.subheader("ğŸ” MBTI ë³€í™” íŒ¨í„´ ìë™ ë¶„ì„")
                change_interpretations = analyze_mbti_changes(bot_records)
                
                if change_interpretations:
                    for interpretation in change_interpretations:
                        st.write(interpretation)
                else:
                    st.info("ë³€í™” íŒ¨í„´ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ë” ë§ì€ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            else:
                st.success("MBTI ìœ í˜•ì´ ì¼ê´€ë˜ê²Œ ìœ ì§€ë˜ê³  ìˆìŠµë‹ˆë‹¤.")
                
                # ì•ˆì •ì„±ì— ëŒ€í•œ í•´ì„
                st.subheader("ğŸ” ì•ˆì •ì„± ë¶„ì„")
                st.write("**ğŸ”„ MBTI ì•ˆì •ì„± ë¶„ì„:**")
                st.write("â€¢ **ì¼ê´€ëœ ì„ í˜¸ë„**: ëª¨ë“  ì§„ë‹¨ì—ì„œ ë™ì¼í•œ MBTI ìœ í˜• ìœ ì§€")
                st.write("â€¢ **ì‹ ë¢°ì„±**: ì§„ë‹¨ ê²°ê³¼ì˜ ë†’ì€ ì‹ ë¢°ì„±ê³¼ ì¼ê´€ì„±")
                st.write("â€¢ **ëª…í™•í•œ ì„±í–¥**: ë¡œë´‡ ìƒí˜¸ì‘ìš©ì— ëŒ€í•œ ëª…í™•í•˜ê³  ì•ˆì •ì ì¸ ì„ í˜¸ë„")
                st.write("â€¢ **ì˜ˆì¸¡ ê°€ëŠ¥ì„±**: í–¥í›„ ë¡œë´‡ ìƒí˜¸ì‘ìš© íŒ¨í„´ ì˜ˆì¸¡ ìš©ì´")
    else:
        st.info(f"ë¡œë´‡ '{st.session_state.robot_id}'ì˜ ì§„ë‹¨ ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤.")

def create_mbti_network(df):
    """MBTI ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ìƒì„±"""
    if len(df) < 2:
        return None, "ë„¤íŠ¸ì›Œí¬ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ìµœì†Œ 2ê°œì˜ ì§„ë‹¨ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤."
    
    # MBTI ìœ í˜• ê°„ ê´€ê³„ ë¶„ì„
    mbti_counts = df['mbti'].value_counts()
    
    # ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ìƒì„±
    G = nx.Graph()
    
    # ë…¸ë“œ ì¶”ê°€ (MBTI ìœ í˜•)
    for mbti in mbti_counts.index:
        G.add_node(mbti, size=mbti_counts[mbti], color=MBTI_COLORS.get(mbti, '#CCCCCC'))
    
    # ì—£ì§€ ì¶”ê°€ (ê³µí†µ íŠ¹ì„± ê¸°ë°˜)
    mbti_axes = {
        'E': ['ENFJ', 'ENTJ', 'ENTP', 'ENFP', 'ESFJ', 'ESFP', 'ESTJ', 'ESTP'],
        'I': ['INFJ', 'INTJ', 'INTP', 'INFP', 'ISFJ', 'ISFP', 'ISTJ', 'ISTP'],
        'S': ['ESFJ', 'ESFP', 'ESTJ', 'ESTP', 'ISFJ', 'ISFP', 'ISTJ', 'ISTP'],
        'N': ['ENFJ', 'ENTJ', 'ENTP', 'ENFP', 'INFJ', 'INTJ', 'INTP', 'INFP'],
        'T': ['ENTJ', 'ENTP', 'ESTJ', 'ESTP', 'INTJ', 'INTP', 'ISTJ', 'ISTP'],
        'F': ['ENFJ', 'ENFP', 'ESFJ', 'ESFP', 'INFJ', 'INFP', 'ISFJ', 'ISFP'],
        'J': ['ENFJ', 'ENTJ', 'ESFJ', 'ESTJ', 'INFJ', 'INTJ', 'ISFJ', 'ISTJ'],
        'P': ['ENTP', 'ENFP', 'ESFP', 'ESTP', 'INTP', 'INFP', 'ISFP', 'ISTP']
    }
    
    # ê³µí†µ íŠ¹ì„±ì„ ê°€ì§„ MBTI ìœ í˜•ë“¤ ê°„ì— ì—£ì§€ ì¶”ê°€
    for axis, mbti_types in mbti_axes.items():
        for i, mbti1 in enumerate(mbti_types):
            for mbti2 in mbti_types[i+1:]:
                if mbti1 in G.nodes and mbti2 in G.nodes:
                    if G.has_edge(mbti1, mbti2):
                        G[mbti1][mbti2]['weight'] += 1
                    else:
                        G.add_edge(mbti1, mbti2, weight=1, axis=axis)
    
    # ë„¤íŠ¸ì›Œí¬ ë ˆì´ì•„ì›ƒ ê³„ì‚°
    pos = nx.spring_layout(G, k=1, iterations=50)
    
    # Plotly ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ìƒì„±
    edge_trace = go.Scatter(
        x=[], y=[], line=dict(width=0.5, color='#888'), hoverinfo='none', mode='lines')
    
    for edge in G.edges():
        x0, y0 = pos[edge[0]]
        x1, y1 = pos[edge[1]]
        edge_trace['x'] += tuple([x0, x1, None])
        edge_trace['y'] += tuple([y0, y1, None])
    
    node_trace = go.Scatter(
        x=[], y=[], text=[], mode='markers', hoverinfo='text',
        marker=dict(size=[], color=[], line=dict(width=2)))
    
    for node in G.nodes():
        x, y = pos[node]
        node_trace['x'] += tuple([x])
        node_trace['y'] += tuple([y])
        node_trace['marker']['color'] += tuple([G.nodes[node]['color']])
        node_trace['marker']['size'] += tuple([G.nodes[node]['size'] * 5 + 10])
        node_trace['text'] += tuple([f"{node}<br>ì§„ë‹¨ ìˆ˜: {G.nodes[node]['size']}"])
    
    fig = go.Figure(data=[edge_trace, node_trace],
                   layout=go.Layout(
                       title='ğŸ§  MBTI ë„¤íŠ¸ì›Œí¬ ë¶„ì„',
                       showlegend=False,
                       hovermode='closest',
                       margin=dict(b=20,l=5,r=5,t=40),
                       xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                       yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))
                   )
    
    # ë„¤íŠ¸ì›Œí¬ í†µê³„ ê³„ì‚°
    stats = {
        'total_nodes': len(G.nodes),
        'total_edges': len(G.edges),
        'density': nx.density(G),
        'avg_clustering': nx.average_clustering(G),
        'connected_components': nx.number_connected_components(G)
    }
    
    return fig, stats

def show_advanced_analysis(df):
    """ê³ ê¸‰ ë¶„ì„ í‘œì‹œ"""
    st.subheader("ğŸ§  ê³ ê¸‰ ë¶„ì„")
    
    # í˜„ì¬ ì‚¬ìš©ìì˜ ë°ì´í„°ë§Œ í•„í„°ë§
    user_df = df[df['user_id'] == st.session_state.user_id]
    
    if user_df.empty:
        st.info("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì§„ë‹¨ì„ ì™„ë£Œí•´ì£¼ì„¸ìš”.")
        return
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("ğŸ“Š MBTI ìƒê´€ê´€ê³„ ë¶„ì„")
        if len(user_df) > 1:
            fig, corr = create_correlation_heatmap(user_df)
            st.plotly_chart(fig, use_container_width=True)
            
            # ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ ì„¤ëª… ì¶”ê°€
            st.info("""
            **ğŸ“Š ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ í•´ì„ ê°€ì´ë“œ:**
            - **ìƒ‰ìƒ ì˜ë¯¸**: ë¹¨ê°„ìƒ‰(ì–‘ì˜ ìƒê´€ê´€ê³„), íŒŒë€ìƒ‰(ìŒì˜ ìƒê´€ê´€ê³„), í°ìƒ‰(ìƒê´€ê´€ê³„ ì—†ìŒ)
            - **ëŒ€ê°ì„ **: ìê¸° ìì‹ ê³¼ì˜ ìƒê´€ê´€ê³„ë¡œ í•­ìƒ 1.0 (ì™„ì „í•œ ì–‘ì˜ ìƒê´€ê´€ê³„)
            - **íŒ¨í„´ ë¶„ì„**: íŠ¹ì • MBTI ìœ í˜•ë“¤ì´ í•¨ê»˜ ë‚˜íƒ€ë‚˜ëŠ” ê²½í–¥ì„ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
            - **í™œìš©**: ë¡œë´‡ ìƒí˜¸ì‘ìš©ì—ì„œ ìœ ì‚¬í•œ ì„±í–¥ì˜ MBTI ê·¸ë£¹ì„ ì‹ë³„í•˜ì—¬ íš¨ê³¼ì ì¸ ì„œë¹„ìŠ¤ ì„¤ê³„ ê°€ëŠ¥
            """)
            
            # ìƒê´€ê´€ê³„ íŒ¨í„´ ìë™ í•´ì„
            st.subheader("ğŸ” ìƒê´€ê´€ê³„ íŒ¨í„´ ìë™ ë¶„ì„")
            correlation_interpretations = analyze_correlation_patterns(corr)
            
            if correlation_interpretations:
                for interpretation in correlation_interpretations:
                    st.write(interpretation)
            else:
                st.info("ìƒê´€ê´€ê³„ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ë” ë‹¤ì–‘í•œ MBTI ìœ í˜•ì˜ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        else:
            st.info("ìƒê´€ê´€ê³„ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ìµœì†Œ 2ê°œì˜ ì§„ë‹¨ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    
    with col2:
        st.subheader("ğŸŒ MBTI ë„¤íŠ¸ì›Œí¬ ë¶„ì„")
        if len(user_df) > 1:
            fig, stats = create_mbti_network(user_df)
            if fig:
                st.plotly_chart(fig, use_container_width=True)
                
                # ë„¤íŠ¸ì›Œí¬ í†µê³„ í‘œì‹œ
                st.subheader("ğŸ“ˆ ë„¤íŠ¸ì›Œí¬ í†µê³„")
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("ë…¸ë“œ ìˆ˜", stats['total_nodes'])
                with col2:
                    st.metric("ì—°ê²° ìˆ˜", stats['total_edges'])
                with col3:
                    st.metric("ë°€ë„", f"{stats['density']:.3f}")
                
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("í‰ê·  í´ëŸ¬ìŠ¤í„°ë§", f"{stats['avg_clustering']:.3f}")
                with col2:
                    st.metric("ì—°ê²° ìš”ì†Œ", stats['connected_components'])
                
                # ë„¤íŠ¸ì›Œí¬ íŒ¨í„´ ìë™ í•´ì„
                st.subheader("ğŸ” ë„¤íŠ¸ì›Œí¬ íŒ¨í„´ ìë™ ë¶„ì„")
                network_interpretations = analyze_network_patterns(user_df)
                
                if network_interpretations:
                    for interpretation in network_interpretations:
                        st.write(interpretation)
                else:
                    st.info("ë„¤íŠ¸ì›Œí¬ íŒ¨í„´ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ë” ë‹¤ì–‘í•œ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            else:
                st.info(stats)  # statsê°€ ë¬¸ìì—´ì¸ ê²½ìš° (ì—ëŸ¬ ë©”ì‹œì§€)
        else:
            st.info("ë„¤íŠ¸ì›Œí¬ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ìµœì†Œ 2ê°œì˜ ì§„ë‹¨ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    
    # ì¶”ê°€ ë¶„ì„
    st.subheader("ğŸ” ì‹¬í™” ë¶„ì„")
    
    if len(user_df) > 1:
        # MBTI ë³€í™” íŒ¨í„´ ë¶„ì„
        st.write("**ğŸ”„ MBTI ë³€í™” íŒ¨í„´**")
        user_df_sorted = user_df.sort_values('timestamp')
        changes = []
        for i in range(1, len(user_df_sorted)):
            prev_mbti = user_df_sorted.iloc[i-1]['mbti']
            curr_mbti = user_df_sorted.iloc[i]['mbti']
            if prev_mbti != curr_mbti:
                changes.append({
                    'from': prev_mbti,
                    'to': curr_mbti,
                    'date': user_df_sorted.iloc[i]['timestamp']
                })
        
        if changes:
            st.info(f"ì´ {len(changes)}ë²ˆì˜ MBTI ë³€í™”ê°€ ìˆì—ˆìŠµë‹ˆë‹¤.")
            change_df = pd.DataFrame(changes)
            st.dataframe(change_df, use_container_width=True)
            
            # MBTI ë³€í™” íŒ¨í„´ ìë™ í•´ì„
            st.subheader("ğŸ” MBTI ë³€í™” íŒ¨í„´ ìë™ ë¶„ì„")
            change_interpretations = analyze_mbti_changes(user_df_sorted)
            
            if change_interpretations:
                for interpretation in change_interpretations:
                    st.write(interpretation)
            else:
                st.info("ë³€í™” íŒ¨í„´ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ë” ë§ì€ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        else:
            st.success("MBTI ìœ í˜•ì´ ì¼ê´€ë˜ê²Œ ìœ ì§€ë˜ê³  ìˆìŠµë‹ˆë‹¤.")
            
            # ì•ˆì •ì„±ì— ëŒ€í•œ í•´ì„
            st.write("**ğŸ” ì•ˆì •ì„± ë¶„ì„:**")
            st.write("â€¢ **ì¼ê´€ëœ ì„ í˜¸ë„**: ëª¨ë“  ì§„ë‹¨ì—ì„œ ë™ì¼í•œ MBTI ìœ í˜• ìœ ì§€")
            st.write("â€¢ **ì‹ ë¢°ì„±**: ì§„ë‹¨ ê²°ê³¼ì˜ ë†’ì€ ì‹ ë¢°ì„±ê³¼ ì¼ê´€ì„±")
            st.write("â€¢ **ëª…í™•í•œ ì„±í–¥**: ë¡œë´‡ ìƒí˜¸ì‘ìš©ì— ëŒ€í•œ ëª…í™•í•˜ê³  ì•ˆì •ì ì¸ ì„ í˜¸ë„")
        
        # ì‹œê°„ëŒ€ë³„ ë¶„ì„
        st.write("**â° ì‹œê°„ëŒ€ë³„ ë¶„ì„**")
        user_df['hour'] = pd.to_datetime(user_df['timestamp']).dt.hour
        hour_counts = user_df['hour'].value_counts().sort_index()
        
        fig_hour = px.bar(
            x=hour_counts.index,
            y=hour_counts.values,
            title="ì‹œê°„ëŒ€ë³„ ì§„ë‹¨ ë¶„í¬",
            labels={'x': 'ì‹œê°„', 'y': 'ì§„ë‹¨ ìˆ˜'}
        )
        st.plotly_chart(fig_hour, use_container_width=True)
        
        # ì‹œê°„ëŒ€ë³„ íŒ¨í„´ ìë™ í•´ì„
        st.subheader("ğŸ” ì‹œê°„ëŒ€ë³„ íŒ¨í„´ ìë™ ë¶„ì„")
        time_interpretations = analyze_time_patterns(user_df)
        
        if time_interpretations:
            for interpretation in time_interpretations:
                st.write(interpretation)
        else:
            st.info("ì‹œê°„ íŒ¨í„´ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ë” ë§ì€ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    else:
        st.info("ì‹¬í™” ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ìµœì†Œ 2ê°œì˜ ì§„ë‹¨ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

def show_data_management(df):
    """ë°ì´í„° ê´€ë¦¬ í‘œì‹œ"""
    st.subheader("ğŸ“‹ ë°ì´í„° ê´€ë¦¬")
    
    # í˜„ì¬ ì‚¬ìš©ìì˜ ëª¨ë“  ë¡œë´‡ MBTI ì´ë ¥
    with st.expander("ë‚´ ëª¨ë“  ë¡œë´‡ MBTI ì§„ë‹¨/ë³€í™” ì´ë ¥", expanded=True):
        my_records = df[df['user_id']==st.session_state.user_id].sort_values('timestamp')
        if not my_records.empty:
            st.dataframe(my_records[["timestamp", "robot_id", "mbti", "gender", "age_group", "job"]], 
                       use_container_width=True)
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ì´ ì§„ë‹¨ ìˆ˜", len(my_records))
            with col2:
                st.metric("ì‚¬ìš©í•œ ë¡œë´‡ ìˆ˜", my_records['robot_id'].nunique())
            with col3:
                st.metric("MBTI ìœ í˜• ìˆ˜", my_records['mbti'].nunique())
        else:
            st.info("ì•„ì§ ì§„ë‹¨ ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    # ë°ì´í„° ë‹¤ìš´ë¡œë“œ (í˜„ì¬ ì‚¬ìš©ì ë°ì´í„°ë§Œ)
    st.subheader("ğŸ“¥ ë°ì´í„° ë‹¤ìš´ë¡œë“œ")
    user_df = df[df['user_id'] == st.session_state.user_id]
    
    if not user_df.empty:
        col1, col2 = st.columns(2)
        with col1:
            st.download_button("ë‚´ ë°ì´í„° CSV", 
                             user_df.to_csv(index=False).encode("utf-8"), 
                             f"{st.session_state.user_id}_data.csv", "text/csv")
        with col2:
            st.download_button("ë‚´ ë°ì´í„° JSON", 
                             user_df.to_json(orient="records", force_ascii=False).encode("utf-8"), 
                             f"{st.session_state.user_id}_data.json", "application/json")
    else:
        st.info("ë‹¤ìš´ë¡œë“œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

def show_admin_data_management(df):
    """ê´€ë¦¬ì ì „ìš© ë°ì´í„° ê´€ë¦¬"""
    st.subheader("ğŸ”§ ê´€ë¦¬ì ë°ì´í„° ê´€ë¦¬")
    
    if not st.session_state.admin_logged_in:
        st.warning("ê´€ë¦¬ì ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        return
    
    # ë°ì´í„° í†µê³„
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("ì´ ì§„ë‹¨ ìˆ˜", len(df))
    with col2:
        st.metric("ê³ ìœ  ì‚¬ìš©ì ìˆ˜", df['user_id'].nunique())
    with col3:
        st.metric("ê³ ìœ  ë¡œë´‡ ìˆ˜", df['robot_id'].nunique())
    with col4:
        st.metric("MBTI ìœ í˜• ìˆ˜", df['mbti'].nunique())
    
    # ë°ì´í„° ê´€ë¦¬ íƒ­
    admin_tab1, admin_tab2, admin_tab3, admin_tab4 = st.tabs([
        "ğŸ“Š ì „ì²´ ë°ì´í„°", "ğŸ—‘ï¸ ì¤‘ë³µ ë°ì´í„° ì •ë¦¬", "ğŸ“¥ ë°ì´í„° ë‚´ë³´ë‚´ê¸°", "âš™ï¸ ì‹œìŠ¤í…œ ê´€ë¦¬"
    ])
    
    with admin_tab1:
        st.subheader("ğŸ“Š ì „ì²´ ì§„ë‹¨ ë°ì´í„°")
        
        # í•„í„°ë§ ì˜µì…˜
        col1, col2, col3 = st.columns(3)
        with col1:
            user_filter = st.text_input("ì‚¬ìš©ì ID í•„í„°", placeholder="íŠ¹ì • ì‚¬ìš©ì ê²€ìƒ‰")
        with col2:
            robot_filter = st.text_input("ë¡œë´‡ ID í•„í„°", placeholder="íŠ¹ì • ë¡œë´‡ ê²€ìƒ‰")
        with col3:
            mbti_filter = st.selectbox("MBTI ìœ í˜• í•„í„°", ["ì „ì²´"] + list(df['mbti'].unique()))
        
        # í•„í„°ë§ ì ìš©
        filtered_df = df.copy()
        if user_filter:
            filtered_df = filtered_df[filtered_df['user_id'].str.contains(user_filter, case=False, na=False)]
        if robot_filter:
            filtered_df = filtered_df[filtered_df['robot_id'].str.contains(robot_filter, case=False, na=False)]
        if mbti_filter != "ì „ì²´":
            filtered_df = filtered_df[filtered_df['mbti'] == mbti_filter]
        
        st.dataframe(filtered_df, use_container_width=True)
        st.info(f"í•„í„°ë§ëœ ê²°ê³¼: {len(filtered_df)}ê±´")
    
    with admin_tab2:
        st.subheader("ğŸ—‘ï¸ ì¤‘ë³µ ë°ì´í„° ì •ë¦¬")
        
        # ì¤‘ë³µ ì§„ë‹¨ í™•ì¸ - ë” ìƒì„¸í•œ ë¶„ì„
        duplicates_info = []
        duplicate_records = []
        
        for (user_id, robot_id), group in df.groupby(['user_id', 'robot_id']):
            if len(group) > 1:
                # ì¤‘ë³µëœ ê·¸ë£¹ì˜ ìƒì„¸ ì •ë³´
                group_sorted = group.sort_values('timestamp')
                duplicates_info.append({
                    'user_id': user_id,
                    'robot_id': robot_id,
                    'count': len(group),
                    'first_date': group_sorted.iloc[0]['timestamp'],
                    'last_date': group_sorted.iloc[-1]['timestamp'],
                    'mbti_changes': ' â†’ '.join(group_sorted['mbti'].tolist())
                })
                
                # ì¤‘ë³µ ë ˆì½”ë“œë“¤ ì €ì¥ (ì‚­ì œìš©)
                for idx, record in group_sorted.iterrows():
                    duplicate_records.append({
                        'index': idx,
                        'user_id': user_id,
                        'robot_id': robot_id,
                        'timestamp': record['timestamp'],
                        'mbti': record['mbti'],
                        'is_latest': idx == group_sorted.index[-1]  # ìµœì‹  ë°ì´í„° ì—¬ë¶€
                    })
        
        if duplicates_info:
            st.warning(f"ğŸ” ì¤‘ë³µ ì§„ë‹¨ ë°œê²¬: {len(duplicates_info)}ê°œ ì‚¬ìš©ì-ë¡œë´‡ ì¡°í•©")
            
            # ì¤‘ë³µ ë°ì´í„° ìš”ì•½ í‘œì‹œ
            duplicates_df = pd.DataFrame(duplicates_info)
            duplicates_df.columns = ['ì‚¬ìš©ì ID', 'ë¡œë´‡ ID', 'ì¤‘ë³µ ìˆ˜', 'ì²« ì§„ë‹¨ì¼', 'ë§ˆì§€ë§‰ ì§„ë‹¨ì¼', 'MBTI ë³€í™”']
            st.dataframe(duplicates_df, use_container_width=True)
            
            # ìƒì„¸ ì¤‘ë³µ ë ˆì½”ë“œ í‘œì‹œ
            st.subheader("ğŸ“‹ ìƒì„¸ ì¤‘ë³µ ë ˆì½”ë“œ")
            detailed_df = pd.DataFrame(duplicate_records)
            detailed_df['ìƒíƒœ'] = detailed_df['is_latest'].apply(lambda x: 'âœ… ìµœì‹ ' if x else 'ğŸ—‘ï¸ ì¤‘ë³µ')
            
            # í‘œì‹œìš© ë°ì´í„°í”„ë ˆì„ ì •ë¦¬
            display_df = detailed_df[['user_id', 'robot_id', 'timestamp', 'mbti', 'ìƒíƒœ']].copy()
            display_df.columns = ['ì‚¬ìš©ì ID', 'ë¡œë´‡ ID', 'ì§„ë‹¨ ì‹œê°„', 'MBTI', 'ìƒíƒœ']
            st.dataframe(display_df, use_container_width=True)
            
            # ì¤‘ë³µ ë°ì´í„° ê´€ë¦¬ ë²„íŠ¼ë“¤
            col1, col2, col3 = st.columns(3)
            
            with col1:
                if st.button("ğŸ—‘ï¸ ì¤‘ë³µ ë°ì´í„° ì •ë¦¬ (ìµœì‹ ë§Œ ìœ ì§€)", type="primary", use_container_width=True):
                    try:
                        # ì‹¤ì œ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¤‘ë³µ ì œê±°
                        if supabase:
                            deleted_count = 0
                            for record in duplicate_records:
                                if not record['is_latest']:  # ìµœì‹ ì´ ì•„ë‹Œ ë°ì´í„°ë§Œ ì‚­ì œ
                                    # ì‹¤ì œ ì‚­ì œ ë¡œì§ (ì£¼ì˜: ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ë” ì•ˆì „í•œ ë°©ë²• ì‚¬ìš©)
                                    try:
                                        supabase.table("responses").delete().eq("user_id", record['user_id']).eq("robot_id", record['robot_id']).eq("timestamp", record['timestamp']).execute()
                                        deleted_count += 1
                                    except Exception as e:
                                        st.error(f"ì‚­ì œ ì‹¤íŒ¨: {e}")
                            
                            if deleted_count > 0:
                                st.success(f"âœ… {deleted_count}ê°œì˜ ì¤‘ë³µ ë°ì´í„°ê°€ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤!")
                                st.balloons()
                                time.sleep(1)
                                st.rerun()
                            else:
                                st.warning("ì‚­ì œí•  ì¤‘ë³µ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        else:
                            st.warning("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                    except Exception as e:
                        st.error(f"ì¤‘ë³µ ë°ì´í„° ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            
            with col2:
                if st.button("â†©ï¸ ë˜ëŒë¦¬ê¸° (ë°±ì—…ì—ì„œ ë³µì›)", use_container_width=True):
                    st.info("ë˜ëŒë¦¬ê¸° ê¸°ëŠ¥ì€ ë°±ì—… íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                    st.info("ì‹œìŠ¤í…œ ê´€ë¦¬ íƒ­ì—ì„œ ë°±ì—…ì„ ë¨¼ì € ìƒì„±í•´ì£¼ì„¸ìš”.")
            
            with col3:
                if st.button("ğŸ“¥ ì¤‘ë³µ ë°ì´í„° ë‚´ë³´ë‚´ê¸°", use_container_width=True):
                    # ì¤‘ë³µ ë°ì´í„°ë§Œ CSVë¡œ ë‚´ë³´ë‚´ê¸°
                    duplicate_data_df = pd.DataFrame(duplicate_records)
                    csv_data = duplicate_data_df.to_csv(index=False).encode('utf-8')
                    
                    st.download_button(
                        "ğŸ“¥ ì¤‘ë³µ ë°ì´í„° CSV ë‹¤ìš´ë¡œë“œ",
                        csv_data,
                        f"duplicate_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                        "text/csv",
                        use_container_width=True
                    )
            
            # ê²½ê³  ë©”ì‹œì§€
            st.warning("âš ï¸ ì¤‘ë³µ ë°ì´í„° ì •ë¦¬ëŠ” ë˜ëŒë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‘ì—… ì „ ë°±ì—…ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
            
        else:
            st.success("âœ… ì¤‘ë³µ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            st.info("ëª¨ë“  ì‚¬ìš©ì-ë¡œë´‡ ì¡°í•©ì´ ê³ ìœ í•œ ì§„ë‹¨ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.")
    
    with admin_tab3:
        st.subheader("ğŸ“¥ ë°ì´í„° ë‚´ë³´ë‚´ê¸°")
        
        col1, col2 = st.columns(2)
        with col1:
            st.download_button("ì „ì²´ ë°ì´í„° CSV", 
                             df.to_csv(index=False).encode("utf-8"), 
                             "all_diagnosis_data.csv", "text/csv")
        with col2:
            st.download_button("ì „ì²´ ë°ì´í„° JSON", 
                             df.to_json(orient="records", force_ascii=False).encode("utf-8"), 
                             "all_diagnosis_data.json", "application/json")
        
        # í†µê³„ ë¦¬í¬íŠ¸ ìƒì„±
        st.subheader("ğŸ“ˆ í†µê³„ ë¦¬í¬íŠ¸")
        report_data = {
            "ì´ ì§„ë‹¨ ìˆ˜": len(df),
            "ê³ ìœ  ì‚¬ìš©ì ìˆ˜": df['user_id'].nunique(),
            "ê³ ìœ  ë¡œë´‡ ìˆ˜": df['robot_id'].nunique(),
            "ê°€ì¥ ë§ì€ MBTI": df['mbti'].mode().iloc[0] if not df['mbti'].mode().empty else "N/A",
            "í‰ê·  ì—°ë ¹ëŒ€": df['age_group'].mode().iloc[0] if not df['age_group'].mode().empty else "N/A",
            "ì„±ë³„ ë¶„í¬": df['gender'].value_counts().to_dict()
        }
        
        st.download_button("í†µê³„ ë¦¬í¬íŠ¸ JSON", 
                         str(report_data).encode("utf-8"), 
                         "diagnosis_report.json", "application/json")
    
    with admin_tab4:
        st.subheader("âš™ï¸ ì‹œìŠ¤í…œ ê´€ë¦¬")
        
        col1, col2 = st.columns(2)
        with col1:
            st.subheader("ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ")
            try:
                # ê°„ë‹¨í•œ ì—°ê²° í…ŒìŠ¤íŠ¸
                test_result = supabase.table("responses").select("id").limit(1).execute()
                st.success("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì •ìƒ")
                st.info(f"ì´ ë ˆì½”ë“œ ìˆ˜: {len(df)}")
                
                # í…Œì´ë¸”ë³„ ë°ì´í„° í˜„í™©
                try:
                    responses_count = len(supabase.table("responses").select("id").execute().data)
                    robots_count = len(supabase.table("user_robots").select("id").execute().data)
                    
                    st.metric("ì§„ë‹¨ ë°ì´í„°", f"{responses_count}ê±´")
                    st.metric("ë“±ë¡ëœ ë¡œë´‡", f"{robots_count}ê°œ")
                except:
                    st.info("ìƒì„¸ í†µê³„ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    
            except Exception as e:
                st.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜: {e}")
        
        with col2:
            st.subheader("â„¹ï¸ ì‹œìŠ¤í…œ ì •ë³´")
            st.info(f"í˜„ì¬ ì‚¬ìš©ì: {st.session_state.user_id}")
            st.info(f"ê´€ë¦¬ì ë¡œê·¸ì¸: {'ì˜ˆ' if st.session_state.admin_logged_in else 'ì•„ë‹ˆì˜¤'}")
            st.info(f"í˜„ì¬ í˜ì´ì§€: {st.session_state.page}")
            st.info(f"ì´ ë“±ë¡ ì‚¬ìš©ì: {len(st.session_state.registered_users)}ëª…")
        
        # ìœ„í—˜í•œ ì‘ì—… ì„¹ì…˜
        st.markdown("---")
        st.subheader("âš ï¸ ìœ„í—˜í•œ ì‘ì—…")
        st.warning("ì•„ë˜ ì‘ì—…ë“¤ì€ ë˜ëŒë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹ ì¤‘í•˜ê²Œ ì§„í–‰í•˜ì„¸ìš”.")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.markdown("#### ğŸ—‘ï¸ ì „ì²´ ë°ì´í„° ì‚­ì œ")
            st.error("ëª¨ë“  ì§„ë‹¨ ë°ì´í„°ì™€ ë¡œë´‡ ì •ë³´ê°€ ì‚­ì œë©ë‹ˆë‹¤.")
            
            # 2ë‹¨ê³„ í™•ì¸ ì‹œìŠ¤í…œ
            if 'delete_confirm_step' not in st.session_state:
                st.session_state.delete_confirm_step = 0
            
            if st.session_state.delete_confirm_step == 0:
                if st.button("ğŸ—‘ï¸ ì „ì²´ ë°ì´í„° ì‚­ì œ", type="secondary", use_container_width=True):
                    st.session_state.delete_confirm_step = 1
                    st.rerun()
            
            elif st.session_state.delete_confirm_step == 1:
                st.error("âš ï¸ ì •ë§ë¡œ ëª¨ë“  ë°ì´í„°ë¥¼ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ?")
                col_yes, col_no = st.columns(2)
                
                with col_yes:
                    if st.button("âœ… ì˜ˆ, ì‚­ì œí•©ë‹ˆë‹¤", type="primary", use_container_width=True):
                        st.session_state.delete_confirm_step = 2
                        st.rerun()
                
                with col_no:
                    if st.button("âŒ ì•„ë‹ˆì˜¤, ì·¨ì†Œ", use_container_width=True):
                        st.session_state.delete_confirm_step = 0
                        st.rerun()
            
            elif st.session_state.delete_confirm_step == 2:
                st.error("ğŸš¨ ìµœì¢… í™•ì¸: ì´ ì‘ì—…ì€ ë˜ëŒë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
                
                # í™•ì¸ í…ìŠ¤íŠ¸ ì…ë ¥
                confirm_text = st.text_input("'DELETE ALL DATA'ë¥¼ ì •í™•íˆ ì…ë ¥í•˜ì„¸ìš”:", key="delete_confirm_text")
                
                col_final_yes, col_final_no = st.columns(2)
                
                with col_final_yes:
                    if st.button("ğŸ”¥ ìµœì¢… ì‚­ì œ ì‹¤í–‰", type="primary", use_container_width=True):
                        if confirm_text == "DELETE ALL DATA":
                            try:
                                # ì „ì²´ ë°ì´í„° ì‚­ì œ ì‹¤í–‰
                                success, message = reset_all_data()
                                if success:
                                    st.success("âœ… ëª¨ë“  ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤!")
                                    st.balloons()
                                    st.session_state.delete_confirm_step = 0
                                    time.sleep(2)
                                    st.rerun()
                                else:
                                    st.error(f"âŒ ì‚­ì œ ì‹¤íŒ¨: {message}")
                            except Exception as e:
                                st.error(f"âŒ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                        else:
                            st.error("í™•ì¸ í…ìŠ¤íŠ¸ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                
                with col_final_no:
                    if st.button("âŒ ì·¨ì†Œ", use_container_width=True):
                        st.session_state.delete_confirm_step = 0
                        st.rerun()
        
        with col2:
            st.markdown("#### ğŸ”„ ë°ì´í„° ë°±ì—…")
            st.info("ì‚­ì œ ì „ ë°ì´í„°ë¥¼ ë°±ì—…í•˜ì„¸ìš”.")
            
            # CSV ë°±ì—… (ë” ì•ˆì „í•œ ì˜µì…˜)
            if st.button("ğŸ“Š CSV ë°±ì—… ë‹¤ìš´ë¡œë“œ", use_container_width=True):
                try:
                    # CSVëŠ” ë‚ ì§œ ì§ë ¬í™” ë¬¸ì œê°€ ì—†ìŒ
                    csv_data = df.to_csv(index=False).encode('utf-8')
                    
                    st.download_button(
                        "ğŸ“¥ CSV ë°±ì—… íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
                        csv_data,
                        f"mbti_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                        "text/csv",
                        use_container_width=True
                    )
                    st.success("âœ… CSV ë°±ì—… íŒŒì¼ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
                except Exception as e:
                    st.error(f"âŒ CSV ë°±ì—… ìƒì„± ì˜¤ë¥˜: {e}")
            
            if st.button("ğŸ’¾ ì „ì²´ ë°±ì—… ë‹¤ìš´ë¡œë“œ", use_container_width=True):
                try:
                    # ë°±ì—… ë°ì´í„° ìƒì„± - JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ë„ë¡ ë³€í™˜
                    df_backup = df.copy()
                    
                    # ë‚ ì§œ/ì‹œê°„ ì»¬ëŸ¼ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
                    for col in df_backup.columns:
                        if pd.api.types.is_datetime64_any_dtype(df_backup[col]):
                            df_backup[col] = df_backup[col].astype(str)
                        elif df_backup[col].dtype == 'object':
                            # ë‚ ì§œ ê°ì²´ê°€ ìˆì„ ìˆ˜ ìˆëŠ” object íƒ€ì… ì»¬ëŸ¼ ì²˜ë¦¬
                            df_backup[col] = df_backup[col].apply(
                                lambda x: x.isoformat() if hasattr(x, 'isoformat') else str(x) if x is not None else None
                            )
                    
                    backup_data = {
                        "responses": df_backup.to_dict('records'),
                        "backup_time": datetime.now().isoformat(),
                        "total_records": len(df_backup),
                        "columns": list(df_backup.columns),
                        "backup_version": "1.0"
                    }
                    
                    # JSON ì§ë ¬í™” ì‹œ ê¸°ë³¸ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
                    def json_serializer(obj):
                        """JSON ì§ë ¬í™”ë¥¼ ìœ„í•œ ì»¤ìŠ¤í…€ ì‹œë¦¬ì–¼ë¼ì´ì €"""
                        if hasattr(obj, 'isoformat'):
                            return obj.isoformat()
                        elif hasattr(obj, '__str__'):
                            return str(obj)
                        else:
                            return None
                    
                    backup_json = json.dumps(backup_data, ensure_ascii=False, indent=2, default=json_serializer)
                    
                    st.download_button(
                        "ğŸ“¥ ë°±ì—… íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
                        backup_json.encode('utf-8'),
                        f"mbti_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                        "application/json",
                        use_container_width=True
                    )
                    st.success("âœ… ë°±ì—… íŒŒì¼ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
                    st.info(f"ë°±ì—…ëœ ë ˆì½”ë“œ ìˆ˜: {len(df_backup)}ê±´")
                except Exception as e:
                    st.error(f"âŒ ë°±ì—… ìƒì„± ì˜¤ë¥˜: {e}")
                    st.info("CSV í˜•íƒœë¡œ ë°±ì—…ì„ ì‹œë„í•´ë³´ì„¸ìš”.")
        
        with col3:
            st.markdown("#### âš™ï¸ ì‹œìŠ¤í…œ ì„¤ì •")
            st.info("ì¶”ê°€ ì‹œìŠ¤í…œ ì„¤ì •ì´ í•„ìš”í•œ ê²½ìš° ì—¬ê¸°ì— ì¶”ê°€ë©ë‹ˆë‹¤.")
            
            # ì‹œìŠ¤í…œ ìƒíƒœ ì²´í¬
            if st.button("ğŸ”„ ì‹œìŠ¤í…œ ìƒíƒœ ìƒˆë¡œê³ ì¹¨", use_container_width=True):
                st.success("ì‹œìŠ¤í…œ ìƒíƒœê°€ ìƒˆë¡œê³ ì¹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
                st.rerun()

# ë©”ì¸ ì‹¤í–‰
if __name__ == "__main__":
    show_sidebar()
    show_main_content()

#ë¡œì»¬ ë°ì´í„° ì €ì¥ì†Œ ì´ˆê¸°í™”
def init_local_storage():
    """ë¡œì»¬ ë°ì´í„° ì €ì¥ì†Œ ì´ˆê¸°í™”"""
    if 'local_data' not in st.session_state:
        st.session_state.local_data = []
    if 'local_user_robots' not in st.session_state:
        st.session_state.local_user_robots = {}

# ë°ì´í„°ë² ì´ìŠ¤ í•¨ìˆ˜ë“¤
def save_to_database(user_id, robot_id, mbti_result, responses, location, user_profile):
    """ì§„ë‹¨ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ ë˜ëŠ” ë¡œì»¬ ì €ì¥ì†Œì— ì €ì¥"""
    # ì§„ë‹¨ ì„¸ì…˜ ID ìƒì„±
    diagnosis_session_id = f"{user_id}_{robot_id}_{int(time.time())}"
    
    if supabase:
        try:
            # ì‘ë‹µ ë°ì´í„° ì €ì¥
            for question_id, response_data in responses.items():
                data = {
                    'user_id': sanitize_input(user_id),
                    'robot_id': sanitize_input(robot_id),
                    'question_id': question_id,
                    'question_text': response_data['question'],
                    'response_score': response_data['score'],
                    'mbti_dimension': response_data['dimension'],
                    'mbti_result': mbti_result,
                    'location': location,
                    'gender': user_profile.get('gender', ''),
                    'age_group': user_profile.get('age_group', ''),
                    'job': user_profile.get('job', ''),
                    'diagnosis_session_id': diagnosis_session_id,
                    'created_at': datetime.now(pytz.UTC).isoformat()
                }
                
                result = supabase.table('responses').insert(data).execute()
                
            st.session_state.current_diagnosis_id = diagnosis_session_id
            return True, "ì§„ë‹¨ ê²°ê³¼ê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤."
            
        except Exception as e:
            return False, f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    else:
        # ë¡œì»¬ ì €ì¥ì†Œì— ì €ì¥
        try:
            init_local_storage()
            
            # ì‘ë‹µ ë°ì´í„°ë¥¼ ë¡œì»¬ì— ì €ì¥
            for question_id, response_data in responses.items():
                data = {
                    'user_id': sanitize_input(user_id),
                    'robot_id': sanitize_input(robot_id),
                    'question_id': question_id,
                    'question_text': response_data['question'],
                    'response_score': response_data['score'],
                    'mbti_dimension': response_data['dimension'],
                    'mbti_result': mbti_result,
                    'location': location,
                    'gender': user_profile.get('gender', ''),
                    'age_group': user_profile.get('age_group', ''),
                    'job': user_profile.get('job', ''),
                    'diagnosis_session_id': diagnosis_session_id,
                    'created_at': datetime.now(pytz.UTC).isoformat()
                }
                
                st.session_state.local_data.append(data)
            
            st.session_state.current_diagnosis_id = diagnosis_session_id
            return True, "ì§„ë‹¨ ê²°ê³¼ê°€ ë¡œì»¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. (ë°ì´í„°ë² ì´ìŠ¤ ë¯¸ì—°ê²°)"
            
        except Exception as e:
            return False, f"ë¡œì»¬ ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

def check_recent_diagnosis(user_id, robot_id, hours=24):
    """ìµœê·¼ ì§„ë‹¨ ì´ë ¥ í™•ì¸"""
    if supabase:
        try:
            cutoff_time = datetime.now(pytz.UTC) - timedelta(hours=hours)
            
            result = supabase.table('responses').select('*').eq('user_id', user_id).eq('robot_id', robot_id).gte('created_at', cutoff_time.isoformat()).execute()
            
            return len(result.data) > 0
            
        except Exception as e:
            st.error(f"ì§„ë‹¨ ì´ë ¥ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return False
    else:
        # ë¡œì»¬ ë°ì´í„°ì—ì„œ í™•ì¸
        try:
            init_local_storage()
            cutoff_time = datetime.now(pytz.UTC) - timedelta(hours=hours)
            
            for data in st.session_state.local_data:
                if (data['user_id'] == user_id and 
                    data['robot_id'] == robot_id and 
                    datetime.fromisoformat(data['created_at'].replace('Z', '+00:00')) > cutoff_time):
                    return True
            return False
            
        except Exception as e:
            st.error(f"ë¡œì»¬ ì§„ë‹¨ ì´ë ¥ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return False

def load_data_from_database():
    """ë°ì´í„°ë² ì´ìŠ¤ ë˜ëŠ” ë¡œì»¬ ì €ì¥ì†Œì—ì„œ ë°ì´í„° ë¡œë“œ"""
    if supabase:
        try:
            result = supabase.table('responses').select('*').execute()
            if result.data:
                df = pd.DataFrame(result.data)
                df['created_at'] = pd.to_datetime(df['created_at'])
                return df
            return pd.DataFrame()
        except Exception as e:
            st.error(f"ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return pd.DataFrame()
    else:
        # ë¡œì»¬ ë°ì´í„° ë¡œë“œ
        try:
            init_local_storage()
            if st.session_state.local_data:
                df = pd.DataFrame(st.session_state.local_data)
                df['created_at'] = pd.to_datetime(df['created_at'])
                return df
            return pd.DataFrame()
        except Exception as e:
            st.error(f"ë¡œì»¬ ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return pd.DataFrame()

def get_user_robots(user_id):
    """ì‚¬ìš©ìì˜ ë¡œë´‡ ëª©ë¡ ì¡°íšŒ"""
    if supabase:
        try:
            result = supabase.table('user_robots').select('robot_name').eq('user_id', user_id).execute()
            if result.data:
                return [robot['robot_name'] for robot in result.data]
            return []
        except Exception as e:
            st.error(f"ë¡œë´‡ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return []
    else:
        # ë¡œì»¬ì—ì„œ ë¡œë´‡ ëª©ë¡ ì¡°íšŒ
        try:
            init_local_storage()
            if user_id in st.session_state.local_user_robots:
                return st.session_state.local_user_robots[user_id]
            return []
        except Exception as e:
            st.error(f"ë¡œì»¬ ë¡œë´‡ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return []

def add_user_robot(user_id, robot_name, robot_description=""):
    """ì‚¬ìš©ì ë¡œë´‡ ì¶”ê°€"""
    if supabase:
        try:
            data = {
                'user_id': sanitize_input(user_id),
                'robot_name': sanitize_input(robot_name),
                'robot_description': sanitize_input(robot_description),
                'created_at': datetime.now(pytz.UTC).isoformat()
            }
            
            result = supabase.table('user_robots').insert(data).execute()
            return True, "ë¡œë´‡ì´ ì„±ê³µì ìœ¼ë¡œ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤."
            
        except Exception as e:
            return False, f"ë¡œë´‡ ë“±ë¡ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    else:
        # ë¡œì»¬ì— ë¡œë´‡ ì¶”ê°€
        try:
            init_local_storage()
            
            if user_id not in st.session_state.local_user_robots:
                st.session_state.local_user_robots[user_id] = []
            
            if robot_name not in st.session_state.local_user_robots[user_id]:
                st.session_state.local_user_robots[user_id].append(robot_name)
                return True, "ë¡œë´‡ì´ ë¡œì»¬ì— ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤. (ë°ì´í„°ë² ì´ìŠ¤ ë¯¸ì—°ê²°)"
            else:
                return False, "ì´ë¯¸ ë“±ë¡ëœ ë¡œë´‡ì…ë‹ˆë‹¤."
                
        except Exception as e:
            return False, f"ë¡œì»¬ ë¡œë´‡ ë“±ë¡ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"